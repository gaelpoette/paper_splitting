% Please use the review version when submitting papers for review.
% The option below provides the final form of your paper

%\documentclass[review,authoryear,jeh]{beg_32}      %  review version
\documentclass[article,authoryear,jmlmc]{beg_32}             %  final version
% Use option "equation" for numbering equation as section

%\count0=115
\usepackage[hang]{footmisc}
\usepackage{graphicx}%
%\usepackage{multirow}%
%\usepackage{enumitem}
%\usepackage[english]{babel} 
%\usepackage[utf8]{inputenc}

%\usepackage{pifont}
\usepackage{mathtools} %Paquet pour des équations et symboles mathématiques
\usepackage{amsfonts}
%\usepackage{unicode-math}
\usepackage{dsfont}
%
\usepackage{url}
\usepackage{tikz,xcolor}
%\usepackage{hyperref}
%\usepackage{booktabs}
\usepackage{algorithm2e}
%
\usepackage{tabularx}
\usepackage{diagbox}
\usepackage{pgfplots}
\pgfplotsset{compat=1.16} % or any recent version like 1.17, 1.16
%\pgfplotsset{compat=1.18} % or any recent version like 1.17, 1.16



\usepackage{amsmath,amssymb,amsfonts}%
\usepackage{amsthm}%
\usepackage{mathrsfs}%
%\usepackage[title]{appendix}%
\usepackage{xcolor}%
%\usepackage{textcomp}%
%\usepackage{manyfoot}%
%\usepackage{booktabs}%
%\usepackage{algorithm}%
%\usepackage{algorithmicx}%
%\usepackage{algpseudocode}%
%\usepackage{listings}%
%%%%

\setlength{\footnotemargin}{0in}
\frenchspacing
\fancypagestyle{plain}{%
  \fancyhf{}
  \fancyhead[R]{\small {\it \jname}, x(x):\thepage--\pageref{LastPage} (\myyear\today)}
  \fancyfoot[R]{\small\bf\thepage }
  \fancyfoot[L]{\fottitle}
  }
%\fancypage{\fbox}{}
\renewcommand{\dmyy}{??}
\renewcommand{\myyear}{????}
\renewcommand{\today}{}

%\newcommand{\newG}[1]{\textcolor{blue}{#1}}

\begin{document}

\volume{Volume x, Issue x, \myyear\today}
\title{Numerical splitting schemes as the cornerstone for mini-batch optimization: On the importance of well-balanced methods}
\titlehead{On the importance of well-balanced methods}
\authorhead{F.A. Author, S.B. Author, \& T. Author}
%For at least  authors with different addresses, use instead the following commands
\author[1,2]{{Bilel} {Bensaid}}\corremail{bilel.bensaid30@gmail.com}

\author[1,2]{{Ga\"el} { Po\"ette}}\corremail{gael.poette@cea.fr}

\author[2]{{Rodolphe} {Turpault}}\corremail{rodolphe.turpault@u-bordeaux.fr}

\address[1]{{CEA, CESTA, DAM}, {{15 av. des Sabli\`eres}, {Le Barp}, {33114}, {France}}}

\address[2]{{Institut de Math\'ematiques de Bordeaux}, {Universit\'e de Bordeaux}, {{CNRS}, {Bordeaux INP}, {France}}}


%\corrauthor[1]{First A. Author}
%\author[1]{S.B. Author, Jr.}
%\author[2]{Third Author}
%\corremail{f.author@affiliation.com}
%\corraddress{Business or Academic Affiliation 1, City, State, Zip Code}
%\address[1]{Business or Academic Affiliation 1, City, State, Zip Code}
%\address[2]{Business or Academic Affiliation 2, City, Province, Zip Code, Country Business or Academic Affiliation 2, City, Province, Zip Code, Country}
\dataO{\today}
%\dataO{}
\dataF{\today}
%\dataF{}

\abstract{
	Mini-batch optimizers are at the center of neural network training. 
        Most of them are also called stochastic optimizers due to their intensive use of samplings within their implementations. 
        %In practice, these optimizers need an extensive tuning of the hyperparameters (learning rate, batch size,...) to converge and exhibit expected behaviours. %give appropriate performances. 
        In theory, many mini-batch algorithms used in this setting rely on the interpolation condition for convergence. 
        This condition is closely related to the overparametrization of the network. 
        %In this first part,
        Here, we investigate the non-interpolating case and highlight some undesirable behaviors for the stationary distributions of %the most popular stochastic
        optimizers. 
        %Moreover, we exploit the notion of balanced splitting to explain such behavior.
        These behaviors are explained thanks to the interpretation of mini-batch optimizers as unbalanced operator splittings.
        \newG{Building on this observation, we propose a simple modification that balances the sub-flows. We show that the resulting mini-batch optimizers, if they converge, do so toward critical points of the full-batch cost function-without relying on the interpolation condition. The proposed balanced splitting not only enables the construction of a new family of mini-batch optimizers derived from classical ones, but also paves the way for designing novel full-batch optimizers.}
}

\keywords{Non-convex optimization, Mini-batch algorithms, Splitting schemes}

\maketitle

\section{Introduction}
\label{intro}

Deep Learning tasks 
\cite{image_recognition,language_recognition,plasma} 
imply the resolution of highly non-convex optimization problems \cite{DL_opti}:
\begin{equation*}
\displaystyle{\min_{\theta \in \Rb^N}} \R(\theta).
\end{equation*}
This objective function has the particularity to involve a sum-structure on all the data of the problem:
\begin{equation*}
	\R(\theta) = \displaystyle{\sum_{p=1}^{P} \loss(u(\theta,x_p), y_p)},
\end{equation*}
where $(x_p,y_p)_{1\leq p \leq P}$ are the data of the problems, $u(\theta,x)$ a parametrized model (for example a neural network) and $\loss$ a loss function measuring the distance between predictions $u(\theta,x)$ and real data. In practice, the memory complexity is proportional to the number of data $P$ and $P$ is large. Therefore, the memory cost exceeds the RAM storage. To overcome this difficulty, the data are split into $m$ batches of size $b$ (except the last batch of size $P-(m-1)b$):
\begin{equation}
	\R(\theta) = \displaystyle{\sum_{p=1}^{P} \loss(u(\theta,x_p), y_p)} = \sum_{i=1}^m \R_i(\theta),
	\label{pb_somme_fini}
\end{equation}
where each function $\R_i$ is computed on $b$ data and not $P \gg b$. Then at every iteration, the algorithm has only access to one function $\R_i$. This problem is a particular case of stochastic optimization:
\begin{equation}
	\R(\theta) = \mean \left[\R_{\zeta}(\theta)\right],
	\label{opti_sto}
\end{equation}  
where $\zeta$ is a probability law. Although we study mini-batch optimization for memory consumption issue, some authors are interested in this for other reasons: it is commonly
accepted that stochastic noise introduced by the batches helps escaping saddle points \cite{sgd_escape1,sgd_escape2,sgd_escape3,sgd_escape4} or achieving best generalization error
\cite{sgd_gen1,sgd_gen2,sgd_gen3,sgd_gen4}. Many optimizers have been suggested these recent years to find critical points for the finite sum problem \eqref{pb_somme_fini}. In the
following, we present the most important families of mini-batch optimizers.

\subsection{Stochastic gradient descent (SGD)}
~~\\
The most studied optimizer to minimize \eqref{pb_somme_fini} is the stochastic gradient descent (SGD), introduced in \cite{SGD_Robins}. SGD consists in applying a gradient descent to each function $\R_i$, where $i$ is uniformly sampled in $\{1,\dots,m\}$:
\begin{equation}
	\theta_{n+1} = \theta_n-\eta \nR_i(\theta_n).
	\label{SGD}
\end{equation}
The convergence of SGD was studied under several assumptions:
bounded variance \cite{sgd_general_diminishing_lr,Bertsekas_basis,sgd_dynamical_basis,bertsekas_theorem,SGD_upper_bound}, relaxed strong growth condition \cite{RG_mean,RG_almost_sure}, gradient confusion \cite{gradient_confusion}, sure-smoothness \cite{sure_smoothness_sgd} and expected smoothness \cite{ES_sgd,sgd_descent_condition,ES_sgd,sgd_global_KL}. In all these studies, learning rates need to decrease so that
$
	\sum_{n\geq 0} \eta_n = +\infty \text{ , } \sum_{n\geq 0} \eta_n^{2}<+\infty
$
to ensure convergence. In practice, SGD is used with constant learning rate. However, the authors of \cite{sgd_prec} showed that even under bounded variance assumption, SGD with any constant learning rate does not converge to a critical point. To expect such a convergence, assumptions like Maximal Strong Growth Condition \cite{MSG_strong_convex,MSG_IG} or Expected Strong Growth Condition \cite{ESG_upper_bound} are necessary. All these hypotheses imply the famous interpolation condition:
\begin{equation}
  \nR(\theta)=0 \implies \forall i\in\{1,\dots,m\}, \nR_i(\theta)=0.
	\label{interpolation}
\end{equation}
This assumption is very strong. For example, concerning neural networks, this assumption is related to the overparametrization ($N\gg P$) of the network \cite{ESG_upper_bound,ESG_IG}. 
In a sense, this work aims at helping answering the question: is it possible to build efficient small networks using SGD?
Accurate shallow networks certainly present interesting properties in a {scientific ML context \cite{FV_scheme,reentry,acc_newt,FRANCK,plasma,KluthRipoll,DESPRES2020109275}}. 
%Then it is not possible to build efficient small networks using SGD. 
Hence, one of the main concern of this work will be to overcome the interpolation condition \eqref{interpolation}. Note that stochastic inertial algorithms and Adam-like optimizers suffer from the same limitation \cite{sgd_prec,adam1,adam2,zou_rms,rms_not_bounded}.

\subsection{Random reshuffle gradient descent (RRGD)}
~~\\
In practice, the reshuffled versions are prefered to classical ones \cite{RR_use1,RR_use2,RG_mean}. For example, the version of SGD with random reshuffling is called Random
Reshuffle GD (RRGD) or SGD without replacement. It consists, at the beginning of every epoch, in sampling a random permutation and then to apply the $m$ updates \eqref{SGD} in the
order given by the permutation. Empirically, it seems that RRGD converges faster than SGD and gives better performances \cite{RR_use1,RR_use_superior}. Many works
\cite{RR_outperforms_quadratics,RR_use3,RR_outperforms_convex,RR_small_epochs} try to \newG{theoretically explain this superiority}, especially for well-conditionned convex functions.

\subsection{Variance reduction}  
~~\\
Although variance reduction optimizers are not as widely used in practice as the previous two families, they have been extensively studied by ML researchers this last decade, since they can converge with constant learning
rate. In broad outline, the aim is to conceive methods that ensure that the variance does vanish asymptotically:
\begin{equation*}
  \displaystyle{\sum_{i=1}^m}\left[\|\nR(\theta_n)/m-\nR_i(\theta_n)\|^2\right] {\underset{n\to \infty}{\longrightarrow}} 0.
\end{equation*}
Now, we will present some of these methods describing their principal features:
\begin{itemize}
	\item historically, one of the first method of this kind introduced in the strongly convex case is SAG: Stochastic Average Gradient. The goal is to combine the small computational cost of SGD with the linear convergence of GD on strongly convex functions. The iterations are of the form:
	\begin{equation*}
		\theta_{n+1}=\theta_n-\eta \sum_{i=1}^m y_i^n
	\end{equation*}
	where at each iteration, one batch $i_n$ is sampled and the variables $y_i^n$ are updated according to:
	\begin{equation*}
		y_i^n =
		\left\{
		\begin{array}{ll}
			\nR_i(\theta_n) \text{ if } i=i_n, \\
			y_i^{n-1} \text{ otherwise}.
		\end{array}
		\right.
	\end{equation*}
It is interesting to see that this algorithm is a random version of a former deterministic optimizer called IAG \cite{IAG_first}: Incremental Aggregated Gradient Descent. We will come back to this algorithm later. This optimizer needs to store the last $m$ gradients ($m$ is the number of batches). Then the memory cost involves the product $mN$ that may be large.
\item From now on, $i_n$ denotes a uniform sample in $\{1,\dots,m\}$ at iteration $n$. In order to reduce the memory cost, the authors of \cite{SVRG_first} suggest SVRG: Stochastic Variance Reduced Gradient which is the basis of many other methods. Let us imagine that we keep a weight $\tilde{\theta}$ every $\tilde{m}$ iterations. The SVRG update is:
\begin{equation*}
	\theta_{n+1} = \theta_n -\eta \left[\nR_{i_n}(\theta_n)-\nR_{i_n}(\tilde{\theta})+\nR(\tilde{\theta})\right].
\end{equation*}
    Let us note two important facts: the introduction of a new hyperparameter $\tilde{m}$ representing the frequency of full-batch gradient evaluation and the computation of such a
    gradient. This is typical of many variance reduction methods, but it is an issue in our case because we want to avoid evaluation of $\R$ or $\nR$ because of the storage
    requirement. Based on the same ideas, we could mention SAGA \cite{SAGA_first} or SNVRG \cite{SNVRG_first}. For bounded variance noise, this algorithm admits a complexity that
    may be better than GD for large $b$. 
\item There are also methods called recursive variance reduction gradients . The prototype of these methods is SARAH \cite{SARAH_first}. The idea is to obtain recursively an estimation $g_n$ of $\nR$ without storing previous gradients:
\begin{equation*}
	g_n = \nR_{i_n}(\theta_n)-\nR_{i_n}(\theta_{n-1})+g_{n-1}
\end{equation*}
where every $\tilde{m}$ iterations, the estimation $g_n$ is the full batch gradient $\nR$. In the same family we find GEOM-SARAH \cite{GEOM_SARAH}, ZERO-SARAH (no computation of $\nR$ but introduction of a new hyperparameter and similar storage as SAG) \cite{ZERO_SARAH}, STORM (add an inertial hyperparameter $\beta$ to tune) \cite{STORM_first}, SPIDER (variance reduction on normalized gradient) \cite{SPIDER_first}, SSRGD (mix between SARAH and perturbed GD in order to escape more efficiently to saddle points) \cite{SSRGD}, PAGE (alternating between SARAH and SGD with some probability) \cite{PAGE}, Katyusha (acceleration on strongly convex functions) \cite{Katyusha}. 
\item Let us now present variance reduction methods that do not need computation of $\nR$, tuning of a hyperparameter like $\tilde{m}$ or $\beta$, or the storage of $m$ gradients.
  The first method that fits into this setting is SCSG \cite{SCSG}. The update looks like the one for SARAH (we do not substract the evaluation of the last iterate but of the first
    of each epoch) by replacing the full-batch gradient by a stochastic gradient. Coupling variance reduction with random reshuffling, the authors of \cite{RR_SARAH} suggest very
    recently RR-SARAH: its memory cost involves $3N$ and not $mN$ and it does not need computation of the full-batch gradient $\nR$. To the best of our knowledge, it is the only
    optimizer that enjoys these properties. However, the analysis is limited to strongly convex functions under strong assumption on the noise. Besides, the numerical experiments
    are limited to the logistic regression (for only one initialization). Finally, the relation between the non-interpolating condition and the limiting points obtained under
    constant learning rate, central in the present paper, has, to our knowledge, never been studied. 
\end{itemize}

\subsection{Objectives \newG{and contributions}}
\label{obj}
~~\\
This recap of the mini-batch literature encourages us to require the six following properties in the conception of a mini-batch optimizer, suitable for any network (even for under-parametrized model/network):
\begin{enumerate}
	\item ideally, we want to \textbf{avoid assumptions on the batch distribution}. Firstly, the bounded variance hypothesis is not reasonable 
          since it is not even satisfied by a linear regression. Moreover, the major part of stochastic optimizers need interpolation condition (hence over-parametrized models) to be used with constant learning rate. To the best of our knowledge, the only optimizers that get rid of these conditions are IAG and DIAG \cite{DIAG}.
	\item It is essential that the optimizers can achieve an arbitrary precision (concerning the gradient) without requiring decreasing learning rates. This is only verified by variance reduction methods. 
	\item The algorithms can not use the full-batch gradient, at any stage, due to storage requirement. The ones that fit into this category are: SGD, RRGD, IG, IAG, SAG, ZERO-SARAH, STORM, SCSG et RR-SARAH.
	\item If possible, we do not want to store $m$ gradients. For example, the optimizers that do not need such a storage are:  SGD, RRGD, IG, SVRG, SNVRG, SPIDER, SARAH, SSRGD, PAGE, SCSG et RR-SARAH.
        \item We absolutely need a \textbf{stopping criterion} that ensures that the full-batch gradient is smaller than a fixed arbitrary precision (without resorting to a full
          batch evaluation for the sake of memory consumption). 
          We will see that the family of mini-batch optimizers we design in this paper all have this property. 
          All the aforementionned
          optimizers are stopped when a maximal number of epochs is achieved (to the best of our knowledge).
        \item Finally, the results of this paper will tend to show that one last ingredient for a user-friendly mini-batch optimizer would be an \textbf{adaptive learning rate method}. 
          %One of the main goal of this paper and its following second part is to avoid tuning hyperparameters.
          In an adaptive context, all the methods of the literature that do
          not necessitate a growth condition or a precise knowledge of the noise make the batch size vary with a risk to exceed the memory
          \cite{armijo_batch_size,armijo_variance_reduction,sto_TR_sampling}. Let us note that such an approach can generate strong slow down due to dynamic memory access. Let us also note that in practice the batch size is not meant to be a hyperparameter to tune since it is given by the quotient between the data size and the RAM size. The only optimizers that overcome this difficulty are the CMA algorithms. \\
          In this paper, we aim at showing that the five first points are a priority for the design of mini-batch optimizers in a non-interpolating context 
          and that adaptiveness, which is also very important in practice, comes second. 
          Note that this last point (adaptiveness) deserves its own paper and will be dealt with in further publications. %which is the purpose of part II.
\end{enumerate}
Most of the optimizers satisfy only one or two of the aforementioned aspects. IAG satisfies the first three criteria. RR-SARAH satisfies the requirements 2, 3 and 4 (for the
second, it is proved in the strongly convex case and it will be clear in the general case thanks to the splitting operators point of view presented here). 
Therefore the goal of this paper %and its following second part 
is to build a deterministic or stochastic (never mind) mini-batch optimizer that \textbf{satisfies the five first
criteria}. \textcolor{blue}{In this paper, we focus on these five first points and only consider constant learning rates}. The last point will be tackled in further publications, based on the work described in this paper. We think these are crucial steps in order to develop smaller machine learning models.\\ 

\newG{
The main contributions of this work are as follows:
\begin{itemize}
    \item  We introduce a new operator-splitting perspective that helps explain several optimization pathologies.
    \item  We develop a balanced update rule that aligns the fixed points of each sub-flow, eliminating the mismatch observed with standard methods. This framework provides, by
      construction, guarantees on the critical points obtained in a mini-batch setting. Some existing mini-batch optimizers can be considered particular cases of our
      framework. It also enables the design of new full-batch optimizers.
    \item 
We prove that, without relying on the interpolation condition, the mini-batch optimizers we construct - if they do converge - converge to a critical point of the full-batch loss function.
    \item   Carefully chosen one-dimensional toy problems show that the proposed method avoids spurious stationary distributions and converges to the true minima even in
      non-interpolatory, imbalanced scenarios, constant learning rates context.
\end{itemize}
}

\subsection{Organization of the paper}
~~\\
To reach such a construction, the paper is organized as follows:
\begin{itemize}
	\item in section \ref{section_stationary}, the stationary distribution of SGD and RRGD (with constant learning rate) is investigated empirically on analytic examples: we build new benchmarks (in one
          dimension) to discuss the negation of the interpolation assumption. Undesirable behaviors (which are, to the best of our knowledge, not documented in the literature) are
          identified: SGD and RRGD may recover the critical points of the $\R_i$ in the long run, and not always the ones of $\R$ or even recover artificial critical points (in the
          sense they are neither critical points of $\R$ nor of the $\R_i, i\in\{1,...,m\}$). 
        \item An interpretation of RRGD as a specific splitting of operators is suggested in section \ref{section_splitting_schemes}. This enables to relate some "strange"
          stationary distributions (and critical points) to an imbalance of the splitting. 
	\item The construction of a balanced splitting gives birth to a new family of optimizer in section \ref{new_splitting}. This family extends the method developed by Speth \cite{rebalanced_splitting} for balancing convection-diffusion. It is shown to overcome the aforementioned issue and satisfies the first five criteria.
\newG{This new family ensures, by construction, with minimal assumptions on the full batch optimizer it is based on, the convergence toward a critical point of the full batch, without resorting to the interpolation condition. }
	\item In section \ref{num_res}, the newly developed methods are tested on several benchmarks. 
\end{itemize}
The balanced strategy developed here serves as an excellent starting point for the adaptive extension (sixth criterion) which will be discussed in further publications. 
%Let us emphasize the fact that only the final (adaptive) algorithms are tested on classical ML tasks in part II. In this part, numerical experiments are mainly meant to explain the behavior of the methods described therein.

\section{Stationary distributions of SGD}
\label{section_stationary}

The aim of this section is to empirically investigate the stationary distributions of the neural network weights obtained with SGD and RRGD with constant learning rate. 
We expect their supports to be "centered" around the minimums of $\R$. 
\newG{As we will demonstrate, this property frequently fails to hold in non-interpolating regimes.}

\subsection{Beyond the interpolation condition}
\label{subsection_beyond_interpolation}

%\paragraph{Interpolating benchmarks}
%~~\\
%To evaluate the properties of the different optimizers (specifically their stability and speed), it is necessary to have some analytical benchmarks. In \cite{Bilel}, the authors build some toy neural networks in two dimensions with different properties. These examples are recalled in appendix \ref{annexe_polys} (their names are modified for pedagogical reasons) but we mention below some of their caracteristics:
%\begin{itemize}
%	\item the benchmark \polyTwo admits four global minimums of zero values and four non-degenerate saddle points. It is the most simple configuration presented in \cite{Bilel}.
%	\item The benchmark \polyThree presents one global minimum of zero value and three local minimums with different values. The saddle points are still non-degenerated but their values are differents.
%	\item The benchmark \polyFive is very stiff and possesses several mimimums with different orders of degeneracy. It also possesses degenerate saddle points. 
%\end{itemize}
%Despite their pertinence concerning the evaluation of full-batch optimizer, they are not sufficient for mini-batch optimization since they satisfy the interpolation condition. 

%\paragraph{Non-interpolating benchmarks}
~~\\
As highlighted in the introduction, the major part of the studies proving that constant step size SGD converges to critical point of $\R$ is based on the interpolation condition or
the overparametrization of the network. It is therefore necessary to suggest new benchmarks taking
into account all the possible setups in the non-interpolated case, in one dimension. Even though one dimensional examples are very simple, we will see that they are nonetheless sufficient to highlight some undesirable behaviors.
These examples are presented in detail in appendix \ref{annexe_non_interpolated}. 
We have tried to give them a name related to game theory, the reason will be explicited within the descriptions of the benchmarks. 
Succintly, we present their caracteristics below:
\begin{itemize}
	\item for benchmark \exOne, the number of batches is $m=2$. Each function $\R_i$ is strongly convex and the function $\R$ admits a \textbf{unique non-interpolating minimum}. The name
          expresses the fact that the general interest should prevail. 
          Being selfish (i.e. focusing on $\R_1$ or $\R_2$)  always leads to a penalized agent.
	\item In the case of \exTwo, $m=2$. Each function $\R_i$ is quadric (non-convex) and admits two minimums with a shared global minimum at $0.5$. Then the objective function
          $\R$ has an \textbf{interpolating local minimum} at $0.5$ and a \textbf{non-interpolating global minimum}. The name means that if the agents are self-centered, they are all individually satisfied (global minimum of $\R_i$) but it is not the best compromise (the global minimum of $\R_i$ becomes a local one for $\R$).
	\item Concerning \exThree, $m=2$. Each function $\R_i$ is a polynomial of degree 6 or 10. Each of the three minimums is \textbf{non-interpolating}. The local minimum in 0
          is the \textbf{intersection of the graphs of $\R_1$ and $\R_2$}. The \textbf{global minimum of $\R_1$ is very close to the one of $\R$}. Here, the general interest is not a big "sacrifice" for the first agent. There also exists a balanced choice ($\theta=0$) insofar as the two agents are just as much (un)satisfied. 
	\item For \exFour, $m=2$. Each function $\R_i$ is non-convex and the two minimums of $\R$ are \textbf{non-interpolating}. Here, the \textbf{global minimum of $\R$ is far from the ones of $\R_1$ and $\R_2$}. The general interest does not suit to the agent individually. 
	\item The benchmark \exFive concerns three strongly convex functions ($m=3$). The unique global minimum of $\R$ is non-interpolating. The situation is similar as \exOne but with three agents.
	\item For \exSix, $m=3$. The \textbf{local} minimum at $\theta=0.5$ is \textbf{interpolating}. There exists a point $\theta=-0.75$ which is the minimum of two functions among the three (\textbf{"semi-interpolation"}). Finally, the global minimum is non-interpolating. In this case, if two agents plot ($\theta=-0.75$), the last agent is badly done, but globally this plot leads to a choice close to the general interest. 
	\item \exSeven also possesses $m=3$ batches. Despite being non-convex, $\R$ admits an unique \textbf{non-interpolating global minimum}. The situation is similar as \exFour but with three agents.
	\item \exHeight is made up of $m=4$ batches. The function $\R$ admits a \textbf{non-interpolating local and global minimum}. The global minimums of $\R_1$ and $\R_3$ are close enough, which is not the case for the rest of the global minimums. Therefore, two of the agents may plot but this is detrimental to the general interest, contrary to \exSix.      
\end{itemize}

\subsection{Graph description}
~~\\
Here, we numerically compute the stationary distributions of SGD/RRGD by Monte Carlo simulations on \exOne and \exTwo. These simulations are performed until time $t_f=10$, large
enough in practice for the distributions to reach stationarity (visual verification). Initially, 10000 particles are uniformly sampled on $[-1,1]$ for each batch $i\in \{1,2\}$.
The position of the particles after the time $t_f$ allows to build the densities $u^1$ and $u^2$. Here $u^1$ denotes the stationary distribution with the following initial condition (the second coordinate refers to the batch number):
\begin{equation*}
	u^1_0(\theta) = \left(\frac{1}{4} \mathds{1}_{[-1,1]}(\theta),1\right).
\end{equation*}
The same for $u^2$ with the initial condition:
\begin{equation*}
	u^2_0(\theta) = \left(\frac{1}{4} \mathds{1}_{[-1,1]}(\theta),2\right).
\end{equation*}
Let us denote by $L_1$ and $L_2$ the Lipschitz constants of $\nR_1$ and $\nR_2$ on $[-1,1]$ respectively. Figure \ref{sgd_ex1} presents the results obtained by the applications of SGD on \exOne
for four different values of the learning rates: $\frac{1}{\max(L_1,L_2)}$ is the associated learning rate related to the stiffest batch whereas $\frac{1}{L_1+L_2}$ is an estimation of
$\frac{1}{L}$, that is to say the stiffness of $\R$ (where $L$ denotes the Lipschitz constant of $\nR$), since $L \leq L_1+L_2$. At each time, the third of these learning rates is also
considered in order to test a smaller value. %: the factor 3 could seem arbitrary for the moment but it will be explained in part II. 
The figures
\ref{sgd_ex2}, \ref{RRGD_ex1} and \ref{RRGD_ex2} correspond respectively to \exTwo with SGD, \exOne with RRGD and \exTwo with RRGD, for the same choices of learning rates ($L_1,L_2$ depends on the benchmark). 

\begin{figure}[!h]
	\centering
	\scalebox{0.60}{\input{chapitre5_img/sgd_ex1.pgf}}
	\caption{Stationary distributions of SGD for different learning rates on \exOne: a) $\eta=\frac{1}{\max(L_1,L_2)}$, b) $\eta=\frac{1}{3\max(L_1,L_2)}$, c) $\eta=\frac{1}{L_1+L_2}$, d) $\eta=\frac{1}{3(L_1+L_2)}$. The green straight line represents the position of the global minimum of $\R$.}
	\label{sgd_ex1}
\end{figure}

\subsection{Comments}
~~\\
On the graphs a) and c) of figure \ref{sgd_ex1} a Dirac comb appears with a lot of stationary states spaced between the minimum $\theta=0$ of $\R_1$ and $\theta=0.5$ of $\R_2$. In
other words, several stationary states have nothing to do with the minimums of the $\R_i$ or $\R$. 
\newG{By reducing the learning rate (see graphs b) and d)), the distribution becomes more Gaussian-like, but it remains noticeably off-centered from the global minimum.}
 For RRGD, the behavior is completely different, see figure \ref{RRGD_ex1}. When the learning rate decreases, the distribution gets closer to a Dirac centered at the global
 minimum. But for intermediary learning rates (graphs a) and b)) the second Dirac is located at the minimum of $\R_2$, that is to say $\theta=0.5$. It is true that the behavior of RRGD is less troubling than the one of SGD but it is still problematic because the convergence (existence of a stationary distribution) does not imply to find a minimum of $\R$. \\
Concerning \exTwo (figure \ref{sgd_ex2}), for any learning rate, the interpolating local minimum at $\theta=0.5$ is largely favored. By diminishing $\eta$ on the graphs b) and d), the proportion of global minimum increases slightly. 
\newG{Unfortunately, SGD appears to favor interpolating minima regardless of their quality, even when the learning rate is sufficiently small.}
Concerning RRGD (figure \ref{RRGD_ex2}), the global minimum is not \newG{overlooked} even for an intermediary value of the learning rate (see graph a)). On the graph d), when the learning rate is small enough, the global minimum is favored despite not being interpolating. We will see in what follows that this difference with SGD can be explained by operator splitting. 
Note that for SGD, even when decreasing the learning rate, the stationary distributions are not satisfactory: the gaussian-like distribution is not well-centered (\exOne) or interpolating
local minimum are largely favored.

In this section, we built new benchmarks to analyse the influence of the interpolation condition on the long run behavior of SGD and RRGD. It appears that they have trouble
catching non-interpolating critical points. Even worse, SGD creates artificial stationary states in the sense that they are not critical points of the individual functions $\R_i$
or of the sum function $\R$. Decreasing the learning rate appears to have little to no effect on the previous observations: this point will be confirmed in the next section. 

\section{SGD revisited as a splitting scheme and issues}
\label{section_splitting_schemes}

The goal of this section is to explain the previous behaviors of SGD and RRGD and to suggest numerical schemes which correct their flaws. 

\newG{In the literature, the behavior of SGD is typically analyzed by interpreting it as a discretization of a stochastic differential equation driven by Brownian motion}
 \cite{SDE_comparison,SDE_modified,SDE_edp,malladi_adam,hu2018diffusion,flat_minima_exponential,yang2020fast,sgd_implicit_regularisation,sgd_implicit_regularisation2}. 
\newG{In contrast, we adopt a different perspective: we interpret RRGD - specifically, its deterministic variant - as a particular discretization of the classical gradient flow}
$\theta'(t)=-\nR(\theta(t))$, using an operator splitting approach. Let us remember the principle on the following Cauchy problem:
\begin{equation}
	\left\{
	\begin{array}{ll}
		y'(t) = F(y(t)), \\
		y(0) = y_0,
	\end{array}
	\right.
	\label{Cauchy_problem}
\end{equation}
which admits the exact solution $\phi_t(y_0)$ at time $t$. Let us consider that $F$ is the sum of $m$ operators $F=\displaystyle{\sum_{i=1}^m}F_i$ in such a way that each system:
\begin{equation*}
	\left\{
	\begin{array}{ll}
		y'(t) = F_i(y(t)), \\
		y(0) = y_0,
	\end{array}
	\right.
\end{equation*}
can be solved exactly (to make it simpler)  with solution $\phi_{\eta}^i(y_0)$ at time $t=\eta$, for all $i\in \{1,\dots,m\}$. By combining these solutions, we get:
\begin{equation}
	\chi_{\eta} = \phi_{\eta}^m \circ \dots \phi_{\eta}^2 \circ \phi_{\eta}^1.
	\label{splitting_lie_trotter}
\end{equation} 
Then the integrator $\chi$ satisfies $\chi_{\eta}(y_0) = \phi_{\eta}(y_0) + \mathcal{O}(\eta^2)$. In other words, $\chi_{\eta}$ is an order one approximation of $\phi_{\eta}$, the
solution of \eqref{Cauchy_problem}. A splitting method then \newG{consists of}:
\begin{itemize}
	\item the choice of operators $F_i$: in the resolution of an ODE or a PDE, they often represent the different scales or stiffnesses of a physical phenomenon (for example convection-diffusion). In our case, this choice is imposed by the decomposition of the dataset into $m$ batches. 
          These batches may be strongly unbalanced. We do not make any assumptions on how well balanced or not they might be.
	\item Solve exactly or approximately the ODE $y'(t)=F_i(y(t))$.
	\item Put the different solutions together in order to build a solution for \eqref{Cauchy_problem}. 
\end{itemize}
As a basis for splitting schemes, see \cite{splitting_ode_review,splitting_ode_review2}.

\subsection{RRGD as a splitting}
~~\\
Let us remember that the deterministic version of RRGD is IG ({\it Incremental Gradient}) in the literature \cite{IG_proximal,despres2022neural}. The latter can be interpreted as a Lie-Trotter
splitting with an explicit Euler scheme consisting in applying an explicit Euler scheme to each ODE $\theta'(t)=-\nR_i(\theta(t))$ and then to put them together like in \eqref{splitting_lie_trotter}. \\
In the same way as for SGD and RRGD, we plot the stationary distributions of IG on \exOne and \exTwo for the same choices of learning rates, on the figures \ref{IG_ex1} and \ref{IG_ex2}. Since there is no stochasticity, we only represent $u$, the stationary distribution corresponding to the initial condition:
\begin{equation*}
	u_0(\theta) = \frac{1}{2} \mathds{1}_{[-1,1]}(\theta).
\end{equation*}
On figure \ref{IG_ex1}, if the learning rate is not small enough (graphs a) and b)), a Delta Dirac function centered at the minimum of $\R_1$ is obtained. On the graph d), $\eta$ is reduced enough for the Dirac to be centered close to the minimum of $\R$. On figure \ref{IG_ex2}, the proportion of global minimum prevails over the local one even with an intermediary learning rate (graph a)), which is not the case for RRGD where a reduction of $\eta$ was required. Globally, the asymptotic behavior (when $\eta \to 0$) of IG is the same as RRGD in the long run. As a result, the pre-eminence of RRGD over SGD is not related to a different random sampling but to the splitting.

The most natural idea to improve IG is then to apply a Strang splitting: in fact, this splitting generates an order two scheme while only having a slight overcost for two operators
(each individual scheme is of order one). This one is commonly written for $m=2$:
\begin{equation*}
	\chi_{\eta} = \phi_{\eta/2}^1 \circ \phi_{\eta}^2 \circ \phi_{\eta/2}^1,
\end{equation*}
where each $\phi_{\eta}^i$ is an explicit Euler scheme with step $\eta$ applied to $\theta'(t) = -\nR_i(\theta(t))$.
Figures \ref{Strang_ex1} and \ref{Strang_ex2} respectively represent the stationary distributions of this Strang splitting scheme for \exOne and \exTwo. From the results of figure
\ref{Strang_ex2}, we can see that increasing the accuracy of the scheme does not change much, at least for the values of $\eta$ used on \exTwo. On figure \ref{Strang_ex1}, we get closer to the steady state faster when $\eta \to 0$ on
\exOne. However, there still exists undesirable steady states in the sense that they do not correspond to a minimization of $\R$. 

\subsection{Unbalanced schemes}
~~\\
Now, the reasonable question is the following one: why are there steady states that have nothing to do with the ones of $y'(t)=F(y(t))$ when this ODE is solved by splitting? This phenomenon was already noticed in the ODE/PDE context \cite{rebalanced_splitting}: arbitrary small learning rates may be necessary to recover the steady states of an ODE, even when the solutions slowly vary. \\
Let us illustrate this phenomenon on an affine ODE where:
\begin{equation}
	F(y) = \left(Ay+a\right) + \left(By+b\right) \coloneqq F_1(y) + F_2(y),
	\label{EDO_linear}
\end{equation}
with $A,B \in \Rb^{N \times N}$ and $a,b \in \Rb^N$. The exact solution of \eqref{EDO_linear} is given by:
\begin{equation*}
	y(t) = y_{\infty} + e^{(A+B)t}\left[y(0)-y_{\infty}\right]
\end{equation*}
where the invariant state $y_{\infty}$ is the solution of the linear system:
\begin{equation*}
	(A+B)y_{\infty} = -(a+b).
\end{equation*}
We assume that the real parts of the eigenvalues of $A+B$ are negative in order for $y_{\infty}$ to be the steady state of \eqref{EDO_linear} and for all the operators at play to be invertible, to simplify the presentation. The invariant states of each vector field satisfy:
\begin{equation*}
	Ay_{\infty}^1 = -a \text{ et } By_{\infty}^2 = -b.
\end{equation*}
Now, let us apply an exact Lie-Trotter splitting by composing $e^{A\eta}$ with $e^{B\eta}$, that is to say, one step is given by the product $e^{B\eta}e^{A\eta}$. To integrate
directly $F$, we would have used the operator $e^{(A+B)\eta}$. The error for one step $\eta$ is then related to the non-commutativity of $e^{B\eta}e^{A\eta}$ and $e^{(A+B)\eta}$:
\begin{equation}
	e^{B\eta}e^{A\eta} = e^{(A+B)\eta} + \left[B,A\right]\eta^2 + \mathcal{O}(\eta^3),
	\label{Taylor_split}
\end{equation}
where $\left[B,A\right]=BA-AB$ is the commutator of $A$ and $B$.\\
Let us begin by the two cases where this splitting makes it possible to retrieve $y_{\infty}$:
\begin{itemize}
	\item if $A^{-1}a=B^{-1}b$  then $y_{\infty}$ is the steady state of each $F_i$ (interpolation).
	\item If $A$ and $B$ commute then for every $\eta>0$, $e^{(A+B)\eta}=e^{B\eta}e^{A\eta}$. Therefore the splitting is exact and the steady state $y_{\infty}$ is recovered asymptotically. 
\end{itemize}
In the general case, the Taylor development \eqref{Taylor_split} suggests that the local error introduced by a first order splitting is $\mathcal{O}(\eta^2)$ in the neighborhood of $y_{\infty}$ is:
\begin{equation*}
	\left[e^{(A+B)\eta}-e^{B\eta}e^{A\eta}\right]y_{\infty} \approx \frac{\eta^2}{2}\left[B,A\right]y_{\infty}.
\end{equation*}
The error above assumes that one starts from a point in the neighborhood of $y_{\infty}$ and integrates during one step. In fact, one starts from an arbitrary initial condition and
integrate during a time of the order of $1/\eta$, until the scheme's steady state denoted by $z_{\infty}$ is approached. Hence, the error $z_{\infty}-y_{\infty}$ is of order $\mathcal{O}(\eta)$ for a Lie-Trotter splitting and of $\mathcal{O}(\eta^2)$ for the Strang one. Concerning the system \eqref{EDO_linear}, the authors of \cite{rebalanced_splitting} explicitly compute the steady state for a Strang splitting:
\begin{equation*}
	z_{\infty} = (I-\alpha\beta\alpha)^{-1}\left[\alpha B^*b + (\alpha\beta+I)A^*a\right],
\end{equation*}
where:
\begin{equation*}
	\alpha = e^{A\eta/2} \text{ , } \beta = e^{B\eta},
\end{equation*}
and:
\begin{equation*}
	A^* = (\alpha-I)A^{-1} \text{ , } B^* = (\beta-I) B^{-1}.
\end{equation*}
What we have to retain from this expression is that the steady state depends on the learning rate $\eta$ ! In the same way, let us compute the steady states of IG on \exOne. By expliciting the recurrent equation of IG on \exOne, we get:
\begin{equation*}
	\theta_{n+1} = 4\eta + (1-8\eta)(1-2\eta)\theta_n.
\end{equation*} 
Then, it is possible to obtain the explicit expression of $\theta_n$:
\begin{equation*}
	\theta_n = (1-8\eta)^n(1-2\eta)^n\left(\theta_0-\dfrac{2}{5-8\eta}\right) + \dfrac{2}{5-8\eta}.
\end{equation*}
Therefore for $\eta \in \left]0,\frac{5}{8}\right[$:
\begin{equation*}
	\theta_{\infty} (\eta)= \dfrac{2}{5-8\eta}.
\end{equation*}
For instance, here are the two steady states corresponding to two choices of learning rates tested in figures \ref{IG_ex1}:
\begin{equation*}
	\left\{
	\begin{array}{ll}
		\theta_{\infty}\left(\dfrac{1}{\max(L_1,L_2)}\right) = \frac{1}{2}, \\
		\theta_{\infty}\left(\dfrac{1}{L_1+L_2}\right) = \frac{10}{21}\approx 0.476.
	\end{array}
	\right.
\end{equation*}
We recover the steady states of the simulations of figure \ref{IG_ex1} and notice that it is impossible to exactly obtain the minimum of $\R$ at $\theta=\frac{2}{5}$ if $\eta \neq 0$. \\
The splitting approach enables to suggest a plausible explanation to the artificial steady states that we have detected on SGD, RRGD, IG and Strang when $\eta$ does not tend to 0.
Such a phenomenon is called an imbalance of the splitting. Therefore, the artificial steady states (or the predilection towards the interpolating local minimum) are related to the
\textbf{non-commutativity} of the vector fields $F_1$ and $F_2$ (i.e. $\nR_1$ and $\nR_2$ for any mini-batch version of GD). 

\subsection{Construction of balanced schemes}
~~\\
Let us illustrate the construction of a balanced scheme as suggested in \cite{rebalanced_splitting} (adapted to order one) for two operators $F_1$ and $F_2$:
\begin{equation*}
	\text{(Unstationary) } y'(t) = F_1(y(t)) + F_2(y(t)),
\end{equation*}
\begin{equation*}
	\text{(Stationary) } 0 = F_1(y_{\infty}) + F_2(y_{\infty}).
\end{equation*}
The idea is to share a vector $c$ between the two operators $F_1$ and $F_2$ in order for the new operators $F_1^* = F_1+c$ and $F_2^*=F_2-c$ to admit $y_{\infty}$ as a steady state. At point $y=y_{\infty}$, the vector to add in order to guarantee the equilibrium should be $c_{\infty} = \frac{1}{2} \left[F_2(y_{\infty})-F_1(y_{\infty})\right]$:
\begin{equation*}
	F_1^*(y) =  F_1(y) + \frac{1}{2} \left[F_2(y_{\infty})-F_1(y_{\infty})\right] = 0 \text{ at } y=y_{\infty},
\end{equation*}
\begin{equation*}
	F_2^*(y) =  F_2(y) - \frac{1}{2} \left[F_2(y_{\infty})-F_1(y_{\infty})\right] = 0 \text{ at } y=y_{\infty}.
\end{equation*}
Since $y_{\infty}$ is not known, the balanced splitting methods consist in adding an equilibrium vector $c_n$ at every step in order for the steady state associated to $F$ to be an approximate steady state for $F_1^* \coloneqq F_1+c_n$ and $F_2^*\coloneqq F_2-c_n$. The idea often consists in taking this vector of the form $\frac{1}{2}\left[\tilde{F}_1-\tilde{F}_2\right]$ where $\tilde{F}_1$ and $\tilde{F}_2$ are approximations of $F_1$ and $F_2$ computed using previous evaluations. Following this idea, Speth suggests the following scheme \cite{rebalanced_splitting} (here adjusted to order one compared to the original article):
\begin{equation}
	\left\{
	\begin{array}{ll}
		y_{n+1/2} = y_n+\eta F_1(y_n)+\eta c_n, \\
		y_{n+1} = y_{n+1/2}+\eta F_2(y_{n+1/2})-\eta c_n, \\
		c_{n+1} = c_n+\frac{1}{\eta}\left[\frac{y_{n+1}+y_n}{2}-y_{n+1/2}\right].
		\label{splitting_speth}
	\end{array}
	\right.
\end{equation}
This scheme is well-balanced since if $(y_n)_{n\in \mathbb{N}}$ converges then:
\begin{equation*}
	\left\{
	\begin{array}{ll}
		F_1(y_{\infty})=-c_{\infty}, \\
		F_2(y_{\infty})=c_{\infty},
	\end{array}
	\right.
\end{equation*}
which gives $F(y_{\infty})=0$. The strength of a balanced scheme is the following: either it does not admit a stationary distribution, or it converges to the steady states of $F$.\\
Figures \ref{speth_ex1} and \ref{speth_ex2} represent the stationary distributions for the scheme \eqref{splitting_speth} on \exOne and \exTwo. First on \exOne, we perfectly retrieve the minimum for the four choices of learning rates contrary to SGD and to unbalanced splittings (RRGD, IG, Strang). On \exTwo, the non-interpolating global minimum is favored for the four learning rates. These first examples show the interest and the efficiency of balanced splittings. 

\begin{remark}
	To finish this section, let us cite some connected works:
	\begin{itemize}
		\item In \cite{splitting_sgd}, the authors interpret SGD (in fact RRGD) as a Lie-Trotter splitting. As an application, the same splitting is conserved by replacing the explicit Euler scheme by a Dormand\&Prince scheme (default integrator of the Scipy library). This process tested on linear and logistic regression problems improves the performances of SGD. However, there is no mention of the essential notion of equilibrium and of stationary distribution. 
		\item For the composite problems where $\R=f+g$ with $f$ a differentiable function and $g$ a non-smooth convex function, there exists a literature about splitting methods \cite{splitting_proximal}. At first glance, no link was developed (until recently) between these methods and splitting schemes for ODE/PDE. The recent article \cite{splitting_proximal_ode} reinterprets the forward-backward, Douglas-Rachford and ADMM methods as splitting schemes on specific ODE.   
	\end{itemize}
\end{remark}

\section{New splitting schemes}
\label{new_splitting}

Despite its benefits, the Speth splitting suffers from drawbacks in our context:
\begin{itemize}
        \item the method shall be able to take any number of batches into account.
	\item When $y_{n+1}$ is computed \eqref{splitting_speth}, the evaluation of $F_1$ used in the balancing $c_n$ does not exploit the most recent value, namely $F_1(y_n)$, but rather $F_1(y_{n-1/2})$. But in the ML context, it is possible to employ the most recent evaluation. Indeed, it is possible to evaluate the batch we want at each iteration since all the batches have more or less the same computational cost. 
\end{itemize}
For these main reasons, in the following paragraph, we introduce an adjustment of Speth's splitting, convenient in our ML context. 

\subsection{Adjustment of the Speth splitting}
\label{extendedSpeth}
~~\\
We therefore suggest the following scheme where the "balance" vector is slightly modified from one step to another:
\begin{equation}
	\left\{
	\begin{array}{ll}
		y_{n+1/2} = y_n + \eta^1 \left[F_1(y_n)+F_2(y_{n-1/2})\right],\\
		y_{n+1} = y_{n+1/2} + \eta^2 \left[F_1(y_n)+F_2(y_{n+1/2})\right].
	\end{array}
	\right.
	\label{RAG_2batch}
\end{equation}
Note that in the previous expression, we enable the possibility to use different learning rates per batch. 
This may be useful for instance if {\em a priori} knowledge of the batch stiffnesses are available. 
%want to be able to use different learning rates for every iteration (adaptive method in the end): $\eta^1$ and $\eta^2$. It is possible to keep the balance property in this context with some modifications, for example by replacing the division by $\eta$ by $\frac{2}{\eta^1+\eta^2}$. Nevertheless, exploiting the most recent evaluation (mentioned above), this issue is settled immediately.

Asymptotically, $F_1(y_{\infty})+F_2(y_{\infty})=0$ hence $F(y_{\infty})=0$.
Up to a factor $\frac{1}{2}$ on the learning rate, 
\eqref{RAG_2batch} can be written in the same form as the Speth scheme with:
\begin{equation*}
	\left\{
	\begin{array}{ll}
		c_n = F_2(y_{n-1/2})-F_1(y_n),\\
		c_{n+1/2} = F_2(y_{n+1/2})-F_1(y_n).
	\end{array}
	\right.
\end{equation*}
Both on \exOne and \exTwo, the stationary distributions of \eqref{RAG_2batch} are exactly the same as the ones for the Speth splitting, see figures \ref{IAG_ex1} and \ref{IAG_ex2}. This fact empirically confirms the balanced property of this scheme.

For $m$ operators, \eqref{RAG_2batch} can naturally be extended to the form:
\begin{equation}
	y_{n+i/m} = y_{n+(i-1)/m}+\eta^i g_n^i,
	\label{RAG_scheme}
\end{equation}
where:
\begin{equation}
	g_n^i = \sum_{j=1}^i F_j\left(y_{n+(j-1)/m}\right)+\sum_{j=i+1}^m F_j\left(y_{n-1+(j-1)/m}\right),
	\label{gni}
\end{equation}
for all $i \in \{1,\dots,m\}$ and $n\geq 1$. 
\newG{A pseudo-code for this algorithm can be found in \ref{pseudo_code}, algorithm \ref{algo_speth}.}

\subsection{Reduction of the memory cost}
\label{red_mem}
~~\\
Now, the issue is that the implementation of formula \eqref{RAG_scheme} requires the storage of $m$ gradients hence its memory cost involves the product $mN$. Therefore, the goal is to obtain a \textbf{close approximation of $g_n^i$ by reducing the memory cost}.
%The previous algorithm has been described as if all the batches were in memory. 
The main ingredient of the previous algorithm consists in using an aggregator which \newG{is {\em consistent if not equal} with the full batch gradient}.
The memory cost question hence resumes to being able to build a lighter but still consistent aggregator.
It seems natural that the main source of inaccuracy of the aggregator comes from the stiffest batch. 
With this in mind, we suggest the following modification of \eqref{RAG_scheme}, for $n\geq 1$:
\begin{multline}
	y_{n+i/m} = y_{n+(i-1)/m} \\
	+\eta^i
	\left\{
	\begin{array}{ll}
		%g_n+\displaystyle{\sum_{j=1}^i} \left[F_j\left(y_{n+(j-1)/m}\right)-F_j\left(y_{n+(j-2)/m}\right)\right] \text{ if } 1\leq i<m,\\
          g_n+\displaystyle{\sum_{j=1}^i} \left[F_j\left(y_{n+(j-1)/m}\right)-F_j\left(y_{{n-1+(i_{\text{pivot}}-1})/m}\right)\right] \text{ if } 1\leq i<m,\\
		g_{n+1} \text{ if } i=m,
	\end{array}
	\right.
	\label{RAGL_splitting}
\end{multline}
where:
\begin{equation*}
	g_n = \sum_{j=1}^m F_j\left(y_{n-1+(j-1)/m}\right).
\end{equation*}
The scheme is initialized at $n=0$ with:
\begin{equation*}
	y_{n+i/m} = y_0 + \eta^i \mathds{1}_{i=m} g_0 \text{ with } g_0 = F(y_0),
\end{equation*}
for all $i \in \{1,\dots,m\}$. 
%The full-batch gradient is \textbf{only computed at the first epoch} sequentially (sum). 
The scheme \eqref{RAGL_splitting} is well-balanced since $g_n$ asymptotically approximates $F$ while the perturbation (the sum until $i$) tends to 0 in the long run. \\
%The formula \eqref{RAGL_splitting} with $F_i=-\nR_i$ corresponds to the RR-SARAH \cite{RR_SARAH} update for $i\leq m-1$. 
{The substraction of $F_j$ evaluated at the point of epoch $n$, corresponding to the stiffest batch $F_j\left(y_{n-1+(i_{\text{pivot}}-1)/m}\right)$ enables to suppress exactly the
part of $g_n$ related to this batch. It is the most problematic part because it has the largest variations. It now only remains to choose a criterion in order to define the
stiffest batch. We suggest to introduce a pivot variable whose indice  $i_{\text{pivot}}$ is such that
$$i_{\text{pivot}} = \argmax_{j\in\{1,...,m\}} \|\nR_j(y_{n-1+(j-1)/m})\|.$$}
This formula can be implemented with a reasonable memory cost of $3N$ instead of $mM$.
\newG{A pseudo-code for this algorithm can be found in \ref{pseudo_code}, algorithm \ref{algo_red_mem}.}

\begin{remark}
  \newG{  Note that if discussions about memory consumption are relevant, the cost per batch, the additional operations needed to apply updates \eqref{RAG_scheme} and
  \eqref{RAGL_splitting} are not costly. This will be verified later on in the numerical section \ref{num_res}.} 
\end{remark}

\subsection{\newG{Theoretical property}}
\label{th_prop}

The previous construction allows stating the following theorem.
\begin{theorem}
\label{th}
  Consider the flow of a given (full batch) optimizer
$
\dot{y}(t) = F(y(t)).
$
Assume
\begin{enumerate}
\item  that the flow is such that 
$
F(y)=0 \Longrightarrow \nR(y)=0,
$
\item that $m$ operators $(F_i)_{i\in\{1,..,m\}}$
such that $F=\sum_{i=1}^m F_i \newG{+\mathcal{O}(\eta)}$ have been chosen, 
\item that the updates $(y_n)_{n\in\mathbb{N}}$ obtained {\em via} expressions \eqref{RAG_scheme} or \eqref{RAGL_splitting}  converges toward $y_\infty$,
\end{enumerate}
 then $y_\infty$ is a critical points of $F$. 
\end{theorem}
The proofs can be found in \ref{proof}.\\

Several remarks can be made regarding the theorem above:
first, if convergence occurs in the mini-batch setting, it is toward a critical point of the full-batch objective $F$, and this is achieved {\em without relying on the
interpolation condition}.\\
Second, the choice of the underlying optimizer used in the splitting is crucial. If {\em Condition 1} is not satisfied (i.e., the full-batch optimizer does not lead to a critical
point of $\R$), then its mini-batch variant cannot be expected to do so either.\\
Finally, {\em Condition 2}, which concerns the sub-operators in the splitting, is not particularly restrictive. Given a full-batch optimizer, multiple splitting schemes can be devised due to the non-uniqueness of the sum decomposition. Some of these schemes may be more practical or effective than others - see Section~\ref{adam} for an illustrative example.

\subsection{Stopping criterion}
~~\\
Something important which seems, to our knowledge, to not have been noticed in the literature is that 
  expression \eqref{gni} (or its equivalent in \eqref{RAGL_splitting}) is consistent with $F=\sum_{i=1}^m F_i$ in the sense that $\forall i\in\{1,...,m\}$ 
  \begin{equation}
    \label{stopping_criterion}
    g_n^i = F(y_n) +\mathcal{O}(\eta). 
  \end{equation}
  This typically means that $g_n^i$ can be used as such in order to define a stopping criterion for the optimizer.
  Take the example of GD: in this case, $F_i=-\nR_i$ and $g_n^i = \nR(y_n)+\mathcal{O}(\eta)$. This means that we can choose to stop our optimizer as soon as $\|g_{n+1}^m\|/P \leq
  \epsilon$, with $\epsilon$ prescribed by the user, and this without resorting to a full batch evaluation of $\nR$. This will be numerically verified in section \ref{num_res}. \\
\ \\

\subsection{Initialization of the algorithm}
~~\\
The initialization of the scheme ($n=0$) can be done in two ways:
\begin{itemize}
	\item either set for all $i\in \{1,\dots,m\}$:
	\begin{equation*}
		g_0^i = \sum_{j=1}^i F_j\left(y_{n+(j-1)/m}\right),
	\end{equation*}
	and the iterates update is the same as in the case $n\geq 1$.
	\item Or compute the full-batch vector field $F$ sequentially (sum) only at the first epoch:
	\begin{equation*}
		\left\{
		\begin{array}{ll}
			y_{n+i/m}=y_0 \text{ if } 1\leq i<m, \\
			y_{n+i/m} = y_{n+(i-1)/m}+\eta^i F(y_0) \text{ if } i=m.
		\end{array}
		\right.
	\end{equation*}
        In practice, this can be easily done by setting the learning rate to zero only at the first epoch for example. 
\end{itemize}


\subsection{Example: application to Momentum}
\label{mom}
~~\\
The simplest choice for $F_i$ is $F_i=-\nR_i$, which is equivalent to IAG (SAG without random permutation) if memory reduction is not taken into account. % and RR-SARAH (without random permutation) otherwise. 
	The splitting scheme we suggest in this paper can be applied to any differential equation $y'(t)=F(y(t))$. For example in the case of the Momentum/Heavy-Ball equation \cite{momentum_sutskever_RNN,Polyak,polyak_momentum_stability,continuous_general} a natural decomposition of the vector field is given by:
	\begin{equation*}
		F_i(\theta,v) = 
		\begin{pmatrix}
                  -\frac{v}{m} \\
                        -{\bbetone}\frac{v}{m}+\nR_i(\theta)
		\end{pmatrix},
	\end{equation*}
        where $m$ is the number of batches.\\ 
        We suggest testing the mini-batch optimizer obtained above with this choice. Figure \ref{mom_ex1_ex2} revisits the benchmarks \exOne and \exTwo but with S-Momentum (stochastic Momentum) with $\beta_1=0.9$. 
        The learning rate is chosen as $\eta=0.125$, which is larger than the most restrictive stability condition dictated by the Lipschitz constants for GD for these benchmarks:
        Momentum is indeed more stable than GD for example, at least on these benchmarks. 
        The distributions are displayed at two epochs: for epoch $n=20$, stationarity is not yet reached with S-Momentum whereas for epoch $n=2000$, it is.  
        %In such conditions, the distributions obtained with Momentum have not yet reached stationarity as testifies figure \ref{mom_ex1_ex2}.
        At epoch $n=20$, the (local and global) mimima seem to be within the supports of the distributions, for both benchmarks. This 
        is qualitatively better than the results obtained with SGD, see figure \ref{sgd_ex1}. 
        But at epoch $n=20$, the results are not yet converged.  
\begin{figure}[!h]
	\centering
        \rotatebox{90}{\quad \quad epoch=20}\scalebox{0.60}{\input{chapitre5_img/mom_exs.pgf}}
	\rotatebox{90}{\quad \quad epoch=2000}\scalebox{0.60}{\input{chapitre5_img/mom_long_exs.pgf}}
        \caption{Stationary distributions of S-Momentum for learning rate $\eta=0.125$ on \exOne: a) and \exTwo: b), for epoch number $20$ (stationarity is not reached) and epoch number $2000$ (stationarity is reached). The green straight line represents the position of the global minimum of $\R$.}
	\label{mom_ex1_ex2}
\end{figure}
  The last line of figure \ref{mom_ex1_ex2} displays the results at epoch $n=2000$, for which we verified that stationarity is reached:  S-Momentum captures respectively
  $x=0.40533333$ instead of the unique global minimum $0.4$ of \exOne, 
  and $x=-0.72457792$ instead of $0.71$, the global minimum of \exTwo. 
  In other words, the appearance of artificial minimums strongly related to the choice of the learning rate emphasized in the previous examples does also occur for other optimisers
  (other $F$) and is not only restricted to SGD or RRGD. 

  On the other hand, figure \ref{speth_mom_ex1_ex2} presents the results obtained with eS-Momentum, for {\em extended Speth Momentum} {(without memory reduction)}, in exactly the same conditions as the ones of figure \ref{mom_ex1_ex2}. 
\begin{figure}[!h]
	\centering
	\rotatebox{90}{\quad \quad epoch=20}\scalebox{0.60}{\input{chapitre5_img/speth_mom_exs.pgf}}
	\rotatebox{90}{\quad \quad epoch=2000}\scalebox{0.60}{\input{chapitre5_img/speth_mom_long_exs.pgf}}
        \caption{Stationary distributions of eS-Momentum for learning rate $\eta=0.125$ on \exOne: a) and \exTwo: b), for $n_{max}=20$. The green straight line represents the position of the global minimum of $\R$.}
	\label{speth_mom_ex1_ex2}
\end{figure}
For the eS-Momentum optimizer, stationarity is very close to being reached with only $20$ epochs: the supports of the obtained distributions are way tighter around the local and
global minimums of $\R$. A faster convergence rate is consequently numerically observed.  
  At epoch $n=2000$, for which stationarity is reached (in fact, the aformentionned stopping criterion \eqref{stopping_criterion} involving $g_n^i$ is activated at $n\approx 40 - 60 \ll 2000$ depending on the
  initialization), the eS-Momentum algorithm recovers the 
  position of the global and local minimums with a perfect accuracy, with a constant learning rate. 

\begin{table}[h!]
	\caption{Runtimes on the different examples for Momentum and eS-Momentum}
        \begin{tabular}{lll}
%         Runtimes (s.)  & \exOne                &  \exTwo               & \exThree              & \exFour               & \exFive               & \exSix                & \exFive                  & \exSix\\ 
          Runtimes (s.) for & (a)                &  (b)              \\ 
          S-Momentum        & $7.48 \times 10^{-1}$ & $9.02\times 10^{-1}$  \\ 
         eS-Momentum        & $9.76 \times 10^{-1}$ & $1.21\times 10^{-0}$\\ 
	\end{tabular}
	\label{ex1_example}
\end{table}

        Note that the application of our extension of Speth's splitting gives an update that is fundamentally different than STORM (variance reduction on Momentum). 
        %The balanced splitting point of view leads to some optimizers looking like SAG or RR-SARAH when combined to GD but builds 
        %very different mini-batch optimisers from the ones obtained by applying a variance reduction technique. 
        We insist on the fact that any $F$ can be chosen: a whole new family of mini-batch algorithms can be constructed based on \eqref{RAG_scheme} using the
        equation relative to RMSProp, Adam etc.
        Once again, the variance reduction versions related to Adam, see Adam$^+$ \cite{adam_variance1}, are fundamentally different from this approach.
        Note finally that additionnally to being independent of the choice of $F$, our methodology allows any choice of summable decomposition $F=\sum_{i=1}^m F_i$ for the
        splitting. 

\subsection{\newG{Example: application to Adam}}
\label{adam}
The flow for Adam is given by \cite{bianchi,Bilel,Bilel_thesis}
\begin{eqnarray}
  \label{Flow_adam}
  \dot{y}(t) = F(y(t),t) \Longleftrightarrow  \left\{\begin{array}{l}
    \dot{\theta}(t)= f(t) \frac{v(t)}{\sqrt{\epsilon_a+s(t)}},\\
    \dot{v}(t)= -\bar{\beta}_1 v(t)+\bar{\beta}_1\nR(\theta(t)),\\
    \dot{s}(t)= -\bar{\beta}_2 s(t)+\bar{\beta}_2\nR^{\otimes 2}(\theta(t)),\\
  \end{array}\right.
\end{eqnarray}
with $f(t)= \frac{\sqrt{1-e^{-\bar{\beta}_2 t}}}{1-e^{-\bar{\beta}_1 t}}$
    where $a^{\otimes 2}$  denotes the element-wise squaring operator on a vector $a$ and $\bar{\beta}_\alpha = \frac{\beta_\alpha}{\eta}$, see \cite{Bilel}.

First, Adam's flow verifies {\em condition 1} of theorem \ref{th} \cite{bianchi,Bilel}.
Let us build operators $(F_i)_{i\in\{1,..,m\}}$ fulfilling {\em condition 2}.
One possibility is to choose for batch $b\in\{1,..,m\}$:
\begin{eqnarray}
\label{F_j_adam_1}
F_b(\theta(t),v(t),s(t))=\left(\begin{array}{l}
  \frac{1}{m}f(t) \frac{v(t)}{\sqrt{\epsilon_a+s(t)}}\\
     -\bar{\beta}_1 \frac{v(t)}{m}+\bar{\beta}_1\nR_b(\theta(t))\\
     -\bar{\beta}_2 \frac{s(t)}{m}+\frac{1}{m}\bar{\beta}_2\nR^{\otimes 2}(\theta(t))\\
  \end{array}\right).
\end{eqnarray}
As such $\sum_{b=1}^m F_b=F$. But in the last line, the full batch gradient is necessary, due to operator $\otimes 2$. 
In a sense, this splitting, fulfilling the conditions of theorem \ref{th}, is not usable in practice (see point 3 in section \ref{obj}) as it will need a full-batch gradient evaluation. 

Another possibility would be, here with $m=2$ batches because it is enough for the sake of understanding, to choose: 
\begin{eqnarray}
\label{F_j_adam_2}
F_1(\theta(t),v(t),s(t))=\left(\begin{array}{l}
  \frac{1}{2}f(t) \frac{v(t)}{\sqrt{\epsilon_a+s(t)}}\\
     -\bar{\beta}_1 \frac{v(t)}{2}+\bar{\beta}_1\nR_1(\theta(t))\\
     -\bar{\beta}_2 \frac{s(t)}{2}+\bar{\beta}_2\nR_1^{\otimes 2}(\theta(t))+\bar{\beta}_2\nR_1(\theta(t))\otimes\nR_2(\theta(t)) \\
  \end{array}\right).
\end{eqnarray}
The above splitting fulfills the conditions of theorem \ref{th}, i.e. $F=F_1+F_2$. But it needs the evaluation of the gradient on the $2$ batches. Thanks to the consistency need only, operator 
$
\nR_1(\theta(t))\otimes\nR_2(\theta(t))
$
can be consistently approximated by  
$
\nR_1(\theta(t_n))\otimes\nR_2(\theta(t_n)) = \nR_1(\theta^n)\otimes\nR_2(\theta^{n-\frac12}) + \mathcal{O}(\eta)
$
making it usable in a mini-batch context, at the cost of memory consumption. 

Note finally that choosing $F_b$ as
\begin{eqnarray}
\label{F_j_adam_3}
F_b(\theta(t),v(t),s(t))=\left(\begin{array}{l}
  \frac{1}{m}f(t) \frac{v(t)}{\sqrt{\epsilon_a+s(t)}}\\
     -\bar{\beta}_1 \frac{v(t)}{m}+\bar{\beta}_1\nR_b(\theta(t))\\
     -\bar{\beta}_2 \frac{s(t)}{m}+\bar{\beta}_2\nR_b^{\otimes 2}(\theta(t))\\
  \end{array}\right)
\end{eqnarray}
which is commonly done in RR-Adam for example, {\em via} our splitting perspective, 
leads to an $F$ given by 
\begin{eqnarray}
\label{F_adam_mod}
F_b(\theta(t),v(t),s(t))=\left(\begin{array}{l}
  f(t) \frac{v(t)}{\sqrt{\epsilon_a+s(t)}}\\
     -\bar{\beta}_1 {v(t)}+\bar{\beta}_1\nR(\theta(t))\\
     -\bar{\beta}_2 {s(t)}+\bar{\beta}_2\sum_{b=1}^m \nR_b^{\otimes 2}(\theta(t))\\
  \end{array}\right),
\end{eqnarray}
which is {\bf not} the classical Adam update rule. Note that, still, it satisfies
{\em condition 1} of theorem \ref{th}. But strictly speaking, in our splitting point of view, the mini-batch update \ref{F_j_adam_3} is not the split version of Adam. It is the split version of the full batch optimizer of flow \eqref{F_adam_mod}, which, in a sense, is a new full batch optimizer.  
\ \\

In this section, we have highlighted the necessity of having resort to a balanced splitting in order to guarantee the restoration of a "correct" stationary distribution (a support related to critical
points of $F$) with constant learning rates. Finally, we have suggested a balanced splitting formula which differs from the classical ones (and encompass some classical ones) %by their memory complexity and 
depending on the particular approximation of $F$. 
\ \\ \ \\

\section{Numerical results}
\label{num_res}


In this section, we suggest applying the new previously described {low memory} splitting combined to GD to the 
analytical {\em non interpolating} benchmarks described in section \ref{subsection_beyond_interpolation}. 
It is denoted as {eS-GD} for {\em extended Speth GD}.
The study hence numerically emphasizes that this mini-batch optimiser also
satisfies condition 5 relative to the existence of a stopping criterion \eqref{stopping_criterion} which does not need a full batch estimation and condition 1 related to fact of
not resorting to the interpolating hypothesis (depicted in section \ref{intro}). 
The section also emphasizes the fact that the next step for further improvements may be closely related to adaptiveness. 
%Part II of this paper will be dedicated to condition 6, namely adaptativeness. \\ %It will be applied to Speth-GD but, once again, the adaptive strategy we suggest can be applied to any flow $F$ decomposed into a sum. \\ 


%\begin{figure}[h!]
%	\centering
%	\scalebox{0.60}{\input{chapitre5_img/SPETH_exs.pgf}}
%        \caption{
%          Application of Speth-GD for the eight benchmarks depicted in section \ref{subsection_beyond_interpolation}, with $n_{max}=10000$ and 
% $\eta = 1   \times 10^{-1}$  for \exOne    (a), 
% $\eta = 5.81\times 10^{-3}$  for \exTwo    (b), 
% $\eta = 1   \times 10^{-3}$  for \exThree  (c), 
% $\eta = 3.33\times 10^{-2}$  for \exFour   (d), 
% $\eta = 5   \times 10^{-2}$  for \exFive   (e), 
% $\eta = 1   \times 10^{-2}$  for \exSix    (f). 
% $\eta = 1   \times 10^{-2}$  for \exSeven  (g), 
% $\eta = 2.5 \times 10^{-2}$  for \exHeight (h). 
%}
%	\label{SPETH_exs}
%\end{figure}

\begin{figure}[h!]
	\centering
	\scalebox{0.60}{\input{chapitre5_img/SPETH_RED_MEM_exs.pgf}}
        \caption{
          Application of low memory eS-GD for the eight benchmarks depicted in section \ref{subsection_beyond_interpolation}, with $n_{max}=10000$ and
 $\eta = 1   \times 10^{-1}$  for \exOne    (a),
 $\eta = 5.81\times 10^{-3}$  for \exTwo    (b),
 $\eta = 1   \times 10^{-3}$  for \exThree  (c),
 $\eta = 3.33\times 10^{-2}$  for \exFour   (d),
 $\eta = 5   \times 10^{-2}$  for \exFive   (e),
 $\eta = 1   \times 10^{-2}$  for \exSix    (f).
 $\eta = 1   \times 10^{-2}$  for \exSeven  (g),
 $\eta = 2.5 \times 10^{-2}$  for \exHeight (h).
}
	\label{SPETH_exs}
\end{figure}

\begin{figure}[h!]
	\centering
	\scalebox{0.60}{\input{chapitre5_img/RR-SPETH_exs.pgf}}
        \caption{
          Application of RR-eS-GD for the eight benchmarks depicted in section \ref{subsection_beyond_interpolation}, with $n_{max}=10000$ and
 $\eta = 1   \times 10^{-1}$  for \exOne    (a),
 $\eta = 5.81\times 10^{-3}$  for \exTwo    (b),
 $\eta = 1   \times 10^{-3}$  for \exThree  (c),
 $\eta = 3.33\times 10^{-2}$  for \exFour   (d),
 $\eta = 5   \times 10^{-2}$  for \exFive   (e),
 $\eta = 1   \times 10^{-2}$  for \exSix    (f).
 $\eta = 1   \times 10^{-2}$  for \exSeven  (g),
 $\eta = 2.5 \times 10^{-2}$  for \exHeight (h).
}
	\label{RR_SPETH_exs}
\end{figure}


\begin{figure}[h!]
	\centering
	\scalebox{0.60}{\input{chapitre5_img/S-SPETH_exs.pgf}}
        \caption{
          Application of S-eS-GD for the eight benchmarks depicted in section \ref{subsection_beyond_interpolation}, with $n_{max}=10000$ and
 $\eta = 1   \times 10^{-1}$  for \exOne    (a),
 $\eta = 5.81\times 10^{-3}$  for \exTwo    (b),
 $\eta = 1   \times 10^{-3}$  for \exThree  (c),
 $\eta = 3.33\times 10^{-2}$  for \exFour   (d),
 $\eta = 5   \times 10^{-2}$  for \exFive   (e),
 $\eta = 1   \times 10^{-2}$  for \exSix    (f).
 $\eta = 1   \times 10^{-2}$  for \exSeven  (g),
 $\eta = 2.5 \times 10^{-2}$  for \exHeight (h).
}
	\label{S_SPETH_exs}
\end{figure}

\begin{table}[h!]
	\caption{Runtimes on the different examples}
        \hspace{-3cm}\begin{tabular}{lllllllll}
%         Runtimes (s.)  & \exOne                &  \exTwo               & \exThree              & \exFour               & \exFive               & \exSix                & \exFive                  & \exSix\\ 
          Runtimes (s.) for & (a)                &  (b)              & (c)              & (d)               & (e)               & (f)               & (g)                  & (h)\\ 
          RRGD           & $2.46 \times 10^{-0}$ & $2.40\times 10^{+1}$  & $6.83 \times 10^{+1}$ & $3.21 \times 10^{+1}$ & $4.26 \times 10^{+1}$ & $2.81\times 10^{+1}$& $6.20 \times
          10^{+1}$ & $5.14\times 10^{+1}$ \\ 
          eS-GD         & $7.28 \times 10^{-1}$ & $9.49\times 10^{-1}$  & $7.23 \times 10^{+1}$ & $1.32 \times 10^{+0}$ & $1.32 \times 10^{+0}$ & $9.00\times 10^{-1}$& $2.14 \times  10^{+0}$ & $1.46\times 10^{-0}$ \\ 
          low memory eS-GD   & $8.56 \times 10^{-1}$ & $9.76\times 10^{-1}$  & $1.12 \times 10^{+2}$ & $4.04 \times 10^{+0}$ & $1.68 \times 10^{+0}$ & $1.83\times 10^{+0}$& $3.70 \times 10^{+0}$ & $1.57\times 10^{+0}$\\ 
      S-eS-GD         & $1.50 \times 10^{-0}$ & $2.00\times 10^{-0}$  & $1.12 \times 10^{+2}$ & $1.85 \times 10^{+0}$ & $1.30 \times 10^{+0}$ & $1.45\times 10^{-0}$& $3.61 \times  10^{+0}$ & $2.11\times 10^{-0}$ \\ 
     RR-eS-GD         & $1.38 \times 10^{-0}$ & $1.81\times 10^{-0}$  & $1.27 \times 10^{+2}$ & $1.27 \times 10^{+0}$ & $6.71 \times 10^{-1}$ & $8.20\times 10^{-1}$& $2.54 \times 10^{+0}$ & $1.31\times 10^{-0}$ \\ 

	\end{tabular}
	\label{ex1_example}
\end{table}
%ex1 SPETH
%time = 7.285564e-01
%ex2 SPETH
%time = 9.496744e-01
%ex3 SPETH
%time = 7.236630e+01
%ex4 SPETH
%time = 3.278054e+00
%ex5 SPETH
%time = 1.328246e+00
%ex6 SPETH
%time = 9.002311e-01
%ex7 SPETH
%time = 2.140584e+00
%ex8 SPETH
%time = 1.462657e+00
%ex1 SPETH_RED_MEM
%time = 8.569081e-01
%ex2 SPETH_RED_MEM
%time = 9.764585e-01
%ex3 SPETH_RED_MEM
%time = 1.122314e+02
%ex4 SPETH_RED_MEM
%time = 4.035030e+00
%ex5 SPETH_RED_MEM
%time = 1.681520e+00
%ex6 SPETH_RED_MEM
%time = 1.836829e+00
%ex7 SPETH_RED_MEM
%time = 3.703890e+00
%ex8 SPETH_RED_MEM
%time = 1.574455e+00
%SGD ex1
%time = 2.463673e+00
%SGD ex2
%time = 2.400417e+01
%SGD ex3
%time = 6.831501e+01
%SGD ex4
%time = 3.213683e+01
%SGD ex5
%time = 4.266384e+01
%SGD ex6
%time = 2.813513e+01
%SGD ex7
%time = 6.209145e+01
%SGD ex8
%time = 5.146769e+01


%ex1 S-SPETH
%time = 1.505394e+00
%ex2 S-SPETH
%time = 2.009192e+00
%ex3 S-SPETH
%time = 1.120153e+02
%ex4 S-SPETH
%time = 1.855267e+00
%ex5 S-SPETH
%time = 1.309805e+00
%ex6 S-SPETH
%time = 1.458017e+00
%ex7 S-SPETH
%time = 3.611255e+00
%ex8 S-SPETH
%time = 2.113607e+00

%ex1 RR-SPETH
%time = 1.386537e+00
%ex2 RR-SPETH
%time = 1.819242e+00
%ex3 RR-SPETH
%time = 1.271130e+02
%ex4 RR-SPETH
%time = 1.275844e+00
%ex5 RR-SPETH
%time = 6.712873e-01
%ex6 RR-SPETH
%time = 8.200662e-01
%ex7 RR-SPETH
%time = 2.541934e+00
%ex8 RR-SPETH
%time = 1.307210e+00

\begin{figure}[h!]
	\centering
	%\scalebox{0.60}{\input{chapitre5_img/DGD_exs_1.pgf}}
        \scalebox{0.60}{\input{chapitre5_img/SGD_exs_1.pgf}}
	\caption{
          Application of RRGD for the eight benchmarks depicted in section \ref{subsection_beyond_interpolation}, with $n_{max}=10000$ and 
 $\eta = 1   \times 10^{-1}$  for \exOne    (a), 
 $\eta = 5.81\times 10^{-3}$  for \exTwo    (b), 
 $\eta = 1   \times 10^{-3}$  for \exThree  (c), 
 $\eta = 3.33\times 10^{-2}$  for \exFour   (d), 
 $\eta = 5   \times 10^{-2}$  for \exFive   (e), 
 $\eta = 1   \times 10^{-2}$  for \exSix    (f). 
 $\eta = 1   \times 10^{-2}$  for \exSeven  (g), 
 $\eta = 2.5 \times 10^{-2}$  for \exHeight (h). 
        }
	\label{SGD_exs}
\end{figure}

Figure \ref{SPETH_exs} presents the results obtained with eS-GD on the eight examples described in section \ref{subsection_beyond_interpolation}. 
The results are of the same type as the one of all the previous figures (but only \exOne and \exTwo were tackled in the latter): the final distributions of the weights are
displayed at a final $t_f$, for some constant learning rates $\eta$ (their values will be detailed and commented on later on).  
In practice, the final time $t_f$ is not explicitly calculated as it is either obtained when the stopping criterion \eqref{stopping_criterion} is fulfilled or when the maximum number of iteration is reached
$n_{max} = 10000$.  
Note that for eS-GD, the final time always corresponds to the activation of the stopping criterion \eqref{stopping_criterion}, representative of the full gradient being smaller than $\epsilon =
10^{-4}$ here. We remind the reader that stopping criterion \eqref{stopping_criterion} never needs the estimation on the full batch: it is estimated {\em on-the-fly} during the successive
epochs/batches.  
\ \\ \ \\
For the eight next benchmarks, the first step was to tune of the (constant) learning rate $\eta$ and the total number of epochs $n_{max}$ for eS-GD (figure \ref{SPETH_exs}). 
By tuning the learning rate for eS-GD, we mean having an estimation of the larger one we can use on the benchmarks in order to 
have stable resolutions. 
By tuning $n_{max}$, we mean ensuring stationarity of the distributions for the algorithms which suffer from not having a stopping criterion.
For all the benchmarks, $n_{max}=10000$ seems to be a qualitatively good choice. 
For benchmarks \exOne and \exTwo, we used $\eta=\frac{1}{L_1+L_2}$, as the Lipshitz constants of $F_1,F_2$ are available: they correspond to 
the largest estimation of the stability condition available. 
For all the other benchmarks, the larger learning rates have been (coarsely) manually obtained. They are given by 
 $\eta = 1   \times 10^{-1}$  for \exOne    (a), 
 $\eta = 5.81\times 10^{-3}$  for \exTwo    (b), 
 $\eta = 1   \times 10^{-3}$  for \exThree  (c), 
 $\eta = 3.33\times 10^{-2}$  for \exFour   (d), 
 $\eta = 5   \times 10^{-2}$  for \exFive   (e), 
 $\eta = 1   \times 10^{-2}$  for \exSix    (f). 
 $\eta = 1   \times 10^{-2}$  for \exSeven  (g), 
 $\eta = 2.5 \times 10^{-2}$  for \exHeight (h). 
As can be seen on figure \ref{SPETH_exs}, the distributions obtained with eS-GD are all peaked on the different local and global minimums, for the eight benchmarks, with
constant learning
rates which are tractable. 
%Note that on example \exHeight, figure \ref{SPETH_exs} (f), the eS-GD optimiser does not find the global minimum and remains stuck on the local one: 
Note that on these benchmarks, we noticed that decreasing the time step does help giving more weight to the global minimum. 
It hints toward the fact that adaptivity may be necessary here in order to capture stiff global minima.  
We will see in the descriptions of the behaviors of RRGD in the same conditions that the balanced splitting already provides significant benefits (with a way smaller cost
than reducing and tuning the learning rate). \\ 
Now, in the eight cases, the distributions are made up of Dirac delta functions at the minimums, regardless of the distance between the minimums of $\R_i$ and $\R$ (zero distance as
soon as there is interpolation, "small" for \exThree and "large" for \exFour), which suggests that the interpolation assumption is not necessary anymore. 
The benchmarks also point out that
our stopping criterion seems sufficient: when {\em a posteriori} computing the full-batch gradient, it satisfies $\|\nR(\theta)\|/P<\epsilon$. \\
\begin{figure}[h!]
	\centering
	%\scalebox{0.60}{\input{chapitre5_img/DGD_exs_001.pgf}}
	\scalebox{0.60}{\input{chapitre5_img/SGD_exs_001.pgf}}
	\caption{
          Application of RRGD for the eight benchmarks depicted in section \ref{subsection_beyond_interpolation}, with $n_{max}=10000$ and 
 $\eta = 1   \times 10^{-1}$  for \exOne    (a), 
 $\eta = 5.81\times 10^{-3}$  for \exTwo    (b), 
 $\eta = 1   \times 10^{-3}$  for \exThree  (c), 
 $\eta = 3.33\times 10^{-2}$  for \exFour   (d), 
 $\eta = 5   \times 10^{-2}$  for \exFive   (e), 
 $\eta = 1   \times 10^{-2}$  for \exSix    (f). 
 $\eta = 1   \times 10^{-2}$  for \exSeven  (g), 
 $\eta = 2.5 \times 10^{-2}$  for \exHeight (h). 
}
	\label{SGD_exs_001}
\end{figure}


Figure \ref{SGD_exs} is obtained in the previous configurations of learning rates and maximum of epochs, the same as the ones of figure \ref{SPETH_exs} for eS-GD, but with RRGD. 
On \exTwo, the results are satisfactory. But they are far from being acceptable for all the other benchmarks: for \exOne, \exThree, \exFour, \exFive, \exSeven and \exHeight,
artificial minimums are captured. These artificial minimums can be quite far from the true ones, see \exFour, \exSeven, \exHeight.    
Very often, the distributions are not peaked in the vicinities of the minimums.
On the last benchmark (figure \ref{SGD_exs} f)), neither the local nor the global minimums is captured. 
The first hypothesis coming to mind to obtain better results would be to decrease the learning rate: figure \ref{SGD_exs_001} is obtained in exactly the same configurations but
with learning rates $100$ times smaller than for figures \ref{SPETH_exs} and \ref{SGD_exs}. 
For benchmarks \exOne and \exFour, it does mitigate the problem: the minimums of $\R$ are recovered with good probabilities. 
However, for all the other benchmarks, this is far from being enough: on benchmark \exTwo, i.e. on figure \ref{SGD_exs_001} (b), the distribution is not anymore
converged: with a  learning rate $100$ times smaller, $100$ times more epochs are needed to reach stationarity.
On \exThree, the local and global minimums are now within the support of the distribution, but their probabilities of occurrence remain low.
For benchmarks \exFive, \exSeven and \exHeight, the global minimum is far from being captured even if an improvement is noticeable. 
Globally, without the balanced splitting we suggest in this paper, interpolatory minimums are favored, independently of their performances.  

\ \\
In this numerical study, the learning rate being constant, it had to be tuned to produce stable trajectories. 
It is the only hyperparameter to tune because we considered eS-GD here. 
But other variations such as eS-Momentum or eS-Adam for example would have needed to tune other hyperparameters (classically denoted by $\beta_1,\beta_2, \epsilon_a$ in the literature, see
\cite{Bilel_thesis}). %Part II will be focused to overcoming this tedious tuning phase.  

\subsection{MNIST}

\begin{figure}[!h]
  \includegraphics[width=0.8\linewidth]{chapitre5_img/comparison_plot.png}
  \caption{MNIST}
  \label{mnist}
\end{figure}


\section{Conclusion}
\label{section_conclusion}

The goal of this paper is to serve as a first step towards an extension of full-batch adaptive methods \cite{Bilel} to mini-batch algorithms which ensures well-balancedness.
More precisely, the following points are tackled:
\begin{itemize}
	\item the interpolation hypothesis is largely discussed through different configurations in one dimension. It appears that two of the most popular algorithms used in practice, namely SGD and RRGD, admit (in case of convergence) stationary distributions with undesirable supports: they may not contain minimums of the objective function and create new steady states, when this assumption is not satisfied. This is very concerning if one wants to reduce the size of a model (possibly under-parametrized networks).
	\item To understand this undesirable behavior, we suggest a new point of view on RRGD reinterpreted as a splitting scheme applied to the gradient flow. This turns the
          classical approaches upside down: it consists in suggesting a new continuous system (a stochastic ODE) and then to discretize it with an explicit Euler scheme. This
          splitting approach makes it possible to explain the appearance of "artificial" steady states that are related to the unbalancing of the splitting: the steady states may depend on the learning rate. With the classical approach based on the brownian motion, it is impossible to explain the source of such a singular behavior (if the diffusion is constant, the stationary distribution is the Gibbs distribution).
         \item A new family of algorithms is then proposed to overcome the former issue. Based on a well-balanced strategy, it avoids to capture artificial minimums due to the batch
           decomposition. In particular, it allows to recover even non-interpolating cases. Furthermore, it respects 5 properties that seem desirable, including a natural stopping
           criterion which doesn't require to compute full batch gradients. 
\end{itemize}

The notion of \textbf{balanced splitting is then crucial} to get an appropriate behavior with constant learning rate and {\em a fortiori} with adaptive learning rate. Improving the
proposed algorithms to include adaptative learning rate selection is therefore the natural next step to obtain efficient algorithms, able to produce comparable results on
machine-learning test than the full-batch counterpart. %This is the subject of part II.



\section*{Data availibility and harware}
The examples are implemented in Python and Tensorflow: the code is available at \textcolor{blue} {https://github.com/bbensaid30/TOptimizers.git}. 

\section*{Conflict of interest}
The authors declare that they have no conflict of interest.

\newpage

%\bibliographystyle{Bibliography_Style}
\bibliographystyle{plain}


\bibliography{biblio}   % name your BibTeX data base

\appendix

%\section{Interpolating benchmarks}
%\label{annexe_polys}
%
%Here we recall the benchmarks of \cite{Bilel}. They are made up of one neuron with a polynomial activation function $g$ (architecture $1(g)$). For $\theta=(w,b)$, the functions are of the form:
%\begin{equation*}
%	\R(w,b)=\frac{1}{4}\left[g(b)^2+g(w+b)^2\right].
%\end{equation*}
%Each activation function makes the training more complex.
%
%\paragraph{Table description}
%~~\\
%Each caracteristic of the benchmarks is described in tables \ref{polyTwo_example}, \ref{polyThree_example}, \ref{polyFive_example1} and \ref{polyFive_example2}. 
%The term "order of dominating terms" refers to the order of the first non-zero term in the asymptotic development of $\mathcal{R}(\theta)-\mathcal{R}(\theta^*)$ in the neighborhood of a critical point $\theta^*$. 
%The values of the critical points $\theta^*$, in other words $\R(\theta^*)$, appear in the same order than the minimums.
%The critical points can be global minimum (\mg), local one (\ml) or saddle point (\ps).
%
%\paragraph{{\it Benchmark} \polyTwo}
%~~\\
%
%The activation is simply defined by:
%\begin{equation}
%	g(z)=z^2-1.
%	\label{def_polyTwo}
%\end{equation}
%
%\begin{table}[h!]
%	\centering
%	\caption{Description of benchmark \polyTwo}
%	\begin{tabular}{lll}
%		
%		\begin{bf} \diagbox{Properties}{Behaviors} \end{bf} & \begin{bf}1\end{bf} & \begin{bf}2\end{bf} \\ 
%		
%		\begin{bf}Nature \end{bf} & \mg & \ps \\ 
%		\begin{bf}$\nnR(\theta^*)$ invertible/zero ?\end{bf} & invertible & invertible\\ 
%		\begin{bf}Order of dominating terms \end{bf} & 2 & 2 \\ 
%		\begin{bf}Set of points\end{bf} & (-2,1); (2,-1) & (-1,0); (1,0)\\ 
%		& (0,-1); (0,1) & (1,-1); (-1,1)\\ 
%		\begin{bf}Values\end{bf} & 0; 0 & 0.5; 0.5 \\ 
%		& 0; 0 & 0.5; 0.5 \\ 
%	\end{tabular}
%	\label{polyTwo_example}
%\end{table}
%
%\paragraph{{\it Benchmark} \polyThree}
%~~\\
%In this case:
%\begin{equation}
%	g(z)=2z^3-3z^2+5.
%	\label{def_polyThree}
%\end{equation}
%
%\begin{table}[h!]
%	\centering
%	\caption{Description of benchmark \polyThree}
%	\begin{tabular}{llll}
%		\toprule
%		
%		\begin{bf} \diagbox{Properties}{Behaviors} \end{bf} & \begin{bf}1\end{bf} & \begin{bf}2\end{bf} & \begin{bf}3\end{bf} \\ 
%		
%		\begin{bf}Nature \end{bf} & \ml & \mg & \ps \\ 
%		\begin{bf}$\nnR(\theta^*)$ invertible/zero ?\end{bf} & invertible & invertible & invertible \\  
%		\begin{bf}Order of dominating terms \end{bf} & 2 & 2 & 2 \\  
%		\begin{bf}Set of points\end{bf} & (-2,1); (2,-1) & (0,-1) & (-1,0); (1,0) \\ 
%		& (0,1) & & (-1,1); (1,-1) \\ 
%		\begin{bf}Values \end{bf} & 4; 4 & 0 & 6.25; 10.25 \\ 
%		& 8 & & 10.25; 6.25 \\  
%	\end{tabular}
%	\label{polyThree_example}
%\end{table}
%
%\paragraph{{\it Benchmark} \polyFive}
%~~\\
%In this case:
%\begin{equation}
%	g(z)=z^5-4z^4+2z^3+8z^2-11z-12.
%	\label{def_polyFive}
%\end{equation}.
%
%\begin{table}[h!]
%	\centering
%	\caption{Description of benchmark \polyFive (part 1)}
%	\begin{tabular}{lllll}
%		
%		\toprule
%		\begin{bf} \diagbox{Properties}{Behaviors} \end{bf} & \begin{bf}1\end{bf} & \begin{bf}2\end{bf} & \begin{bf}3\end{bf} & \begin{bf}4\end{bf}  \\
%		
%		
%		\begin{bf}Nature\end{bf} & \mg & \mg & \ml & \mg \\ 
%		\begin{bf}$\nnR(\theta^*)$ invertible/zero ?\end{bf} & invertible & singular & singular & zero \\ 
%		\begin{bf}Order of dominating terms\end{bf} & 2 & 2 & 2 & 4 \\ 
%		\begin{bf}Set of points\end{bf} & (0,3) & (-4,3); (4,-1) & (2,1); (-2,3) & (0,-1) \\ 
%		\begin{bf}Values\end{bf} & 0 & 0; 0 & 64; 64 & 0 \\ 
%	\end{tabular}
%	\label{polyFive_example1}
%\end{table}
%
%\begin{table}[h!]
%	\centering
%	\caption{Description of benchmark \polyFive (part 2)}
%	\begin{tabular}{lll}
%		
%		\toprule
%		\begin{bf} \diagbox{Properties}{Behaviors} \end{bf} & \begin{bf}5\end{bf} & \begin{bf}6\end{bf} \\
%		
%		
%		\begin{bf}Nature\end{bf} & \ps & \ps \\ 
%		\begin{bf}$\nnR(\theta^*)$ invertible/zero ?\end{bf} & invertible & zero \\ 
%		\begin{bf}Order of dominating terms\end{bf} & 2 & 3 \\ 
%		\begin{bf}Set of points\end{bf} & (4/5,11/5); (-4/5,3) & (-2,1); (2,-1); (0,1) \\ 
%		\begin{bf}Values\end{bf} & 84.2; 84.2 & 64; 64; 128 \\ 
%	\end{tabular}
%	\label{polyFive_example2}
%\end{table}


\section{Non-interpolating benchmarks}
\label{annexe_non_interpolated}

For each example, we present the functions $\R_i$ as well as the position $\theta$ of their global minimums (\mg) and local ones (\ml). For \exOne and \exTwo, the Lipschitz constants of $\nR$ on $[-1,1]$ are computed.

\subsection{Example \exOne}
~~\\

The functions are the following ($m=2$):
\begin{equation*}
	\R_1(\theta) = \theta^2+1,
\end{equation*}
\begin{equation*}
	\R_2(\theta) = (2\theta-1)^2+1.
\end{equation*}
The Lipschitz constants of the gradients on $[-1,1]$ are: $L_1=2$ and $L_2=8$.

\begin{table}[h!]
	\centering
	\caption{Description of benchmark \exOne}
	\begin{tabular}{ll}
		
		\begin{bf} \diagbox{Functions}{Minimums} \end{bf} & \begin{bf}\mg\end{bf} \\ 
		
		\begin{bf}$\R_1$\end{bf} &  0   \\ 
		\begin{bf}$\R_2$\end{bf} &  0.5 \\ 
		\begin{bf}$\R$\end{bf}   &  0.4 \\ 
	\end{tabular}
	\label{ex1_example}
\end{table}

\subsection{Example \exTwo}
~~\\
The functions $\R_i$ are the following:
\begin{equation*}
	\R_1(\theta) = 2\theta^4-\theta^2+1,
\end{equation*}
\begin{equation*}
	\R_2(\theta) = 12\theta^4+4\theta^3-9\theta^2+4.
\end{equation*}
The Lipschitz constants of the gradients on $[-1,1]$ are respectively: $L_1=22$ and $L_2=150$.

\begin{table}[h!]
	\centering
	\caption{Description of benchmark \exTwo}
	\begin{tabular}{lll}
		
		\begin{bf} \diagbox{Functions}{Minimums} \end{bf} & \begin{bf}\mg\end{bf} & \begin{bf}\ml\end{bf} \\
		
		
		\begin{bf}$\R_1$\end{bf} & 0.5 & -0.5  \\ 
		\begin{bf}$\R_2$\end{bf} & 0.5 & -0.75 \\ 
		\begin{bf}$\R$\end{bf}   & -0.71 & 0.5 \\ 
	\end{tabular}
	\label{ex2_example}
\end{table}

\subsection{Example \exThree}
~~\\
The benchmark is described by the functions ($m=2$):
\begin{equation*}
	\R_1(\theta) = 12\theta^{10}-4\theta^9+5\theta^8+\theta^6-3\theta^5-2\theta^4-\theta^3+\theta^2-\theta+1,
\end{equation*}
\begin{equation*}
	\R_2(\theta) = \theta^6-\theta^5-\theta^3+\theta+1.
\end{equation*}
Let us notice that at the minimum $\theta=0$: $\R_1(0)=\R_2(0)$. The function $\R$ is quasi-constant between $\theta=0$ and $\theta=0.229$ where it varies between $1$ and $1.01$. The global minimums of $\R_1$ and $\R$ are close.

\begin{table}[h!]
	\centering
	\caption{Description of benchmark \exThree}
	\begin{tabular}{lll}
		
		\begin{bf} \diagbox{Functions}{Minimums} \end{bf} & \begin{bf}\mg\end{bf} & \begin{bf}\ml\end{bf} \\
		
		
		\begin{bf}$\R_1$\end{bf} & 0.707 &   \\ 
		\begin{bf}$\R_2$\end{bf} & -0.463 & 1.124 \\ 
		\begin{bf}$\R$\end{bf}   & 0.721 &  0 \\ 
	\end{tabular}
	\label{ex3_example}
\end{table}

\subsection{Example \exFour}
~~\\
The functions are of the form ($m=2$):
\begin{equation*}
	\R_1(\theta) = \theta^6+\theta^5+\theta^3-2\theta^2+9,
\end{equation*}
\begin{equation*}
	\R_2(\theta) = \theta^6-\theta^5-\theta^3+\theta+1.
\end{equation*}

\begin{table}[h!]
	\centering
	\caption{Description of benchmark \exFour}
	\begin{tabular}{lll}
		
		\begin{bf} \diagbox{Functions}{Minimums} \end{bf} & \begin{bf}\mg\end{bf} & \begin{bf}\ml\end{bf} \\
		
		
		\begin{bf}$\R_1$\end{bf} & -1.364 & 0.624  \\ 
		\begin{bf}$\R_2$\end{bf} & -0.463 & 1.124 \\ 
		\begin{bf}$\R$\end{bf}   & -0.812 &  0.677 \\ 
	\end{tabular}
	\label{ex4_example}
\end{table}

\subsection{Example \exFive}
~~\\
The three functions are:
\begin{equation*}
	\R_1(\theta) = \theta^2+1,
\end{equation*}
\begin{equation*}
	\R_2(\theta) = (2\theta-1)^2+1,
\end{equation*}
and
\begin{equation*}
	\R_3(\theta) = (2\theta+3)^2+1.
\end{equation*}

\begin{table}[h!]
	\centering
	\caption{Description of benchmark \exFive}
	\begin{tabular}{ll}
		
		\begin{bf} \diagbox{Functions}{Minimums} \end{bf} & \begin{bf}\mg\end{bf} \\ 
		
		\begin{bf}$\R_1$\end{bf} & 0 \\ 
		\begin{bf}$\R_2$\end{bf} & 0.5  \\ 
		\begin{bf}$\R_3$\end{bf} & -1.5  \\ 
		\begin{bf}$\R$\end{bf}   & -0.444  \\ 
	\end{tabular}
	\label{ex5_example}
\end{table}

\subsection{Example \exSix}
~~\\
The three functions are described by the following expressions:
\begin{equation*}
	\R_1(\theta) = 2\theta^4-\theta^2+1,
\end{equation*}
\begin{equation*}
	\R_2(\theta) = 12\theta^4+4\theta^3-9\theta^2+4,
\end{equation*}
and
\begin{equation*}
	\R_3(\theta) = 4\theta^6+\frac{14}{5}\theta^5-\frac{7}{4}\theta^4-\theta^3+1.
\end{equation*}

\begin{table}[h!]
	\centering
	\caption{Description of benchmark \exSix}
	\begin{tabular}{lll}
		
		\begin{bf} \diagbox{Functions}{Minimums} \end{bf} & \begin{bf}\mg\end{bf} & \begin{bf}\ml\end{bf} \\
		
		
		\begin{bf}$\R_1$\end{bf} & 0.5 & -0.5  \\ 
		\begin{bf}$\R_2$\end{bf} & 0.5 & -0.75 \\ 
		\begin{bf}$\R_3$\end{bf} & -0.75; 0.5  &  \\ 
		\begin{bf}$\R$\end{bf}   & -0.718 &  0.5 \\ 
	\end{tabular}
	\label{ex6_example}
\end{table}

\subsection{Example \exSeven}
~~\\
The three functions are described by:
\begin{equation*}
	\R_1(\theta) = \theta^6+\theta^5+\theta^3-2\theta^2+9,
\end{equation*}
\begin{equation*}
	\R_2(\theta) = \theta^6-\theta^5-\theta^3+\theta+1,
\end{equation*}
and
\begin{equation*}
	\R_3(\theta) = \theta^6+\frac{\theta^5}{2}+2\theta^3+2.
\end{equation*}

\begin{table}[h!]
	\centering
	\caption{Description of benchmark \exSeven}
	\begin{tabular}{lll}
		
		\begin{bf} \diagbox{Functions}{Minimums} \end{bf} & \begin{bf}\mg\end{bf} & \begin{bf}\ml\end{bf} \\
		
		
		\begin{bf}$\R_1$\end{bf} & -1.364 & 0.624  \\ 
		\begin{bf}$\R_2$\end{bf} & -0.462 & 1.125 \\ 
		\begin{bf}$\R_3$\end{bf} & -1.159 &  \\ 
		\begin{bf}$\R$\end{bf}   & -0.912 &   \\ 
	\end{tabular}
	\label{ex7_example}
\end{table}

\subsection{Example \exHeight}
~~\\

Here, there are $m=4$ batches with functions:
\begin{equation*}
	\R_1(\theta) = \theta^6+\theta^5+\theta^3-2\theta^2+9,
\end{equation*}
\begin{equation*}
	\R_2(\theta) = \theta^6-\theta^5-\theta^3+\theta+1,
\end{equation*}
\begin{equation*}
	\R_3(\theta) = \theta^6+\frac{\theta^5}{2}+2\theta^3+2,
\end{equation*}
and
\begin{equation*}
	\R_4(\theta) = \theta^3+\theta^2-2\theta+1.
\end{equation*}

\begin{table}[h!]
	\centering
	\caption{Description of benchmark \exHeight}
	\begin{tabular}{lll}
		
		\begin{bf} \diagbox{Functions}{Minimums} \end{bf} & \begin{bf}\mg\end{bf} & \begin{bf}\ml\end{bf} \\
		
		
		\begin{bf}$\R_1$\end{bf} & -1.364 & 0.624  \\ 
		\begin{bf}$\R_2$\end{bf} & -0.463 & 1.124 \\ 
		\begin{bf}$\R_3$\end{bf} & -1.159 &  \\ 
		\begin{bf}$\R_4$\end{bf} & 0.548 &   \\ 
		\begin{bf}$\R$\end{bf} & -0.870 & 0.413 \\ 
	\end{tabular}
	\label{ex8_example}
\end{table}


\section{Figures relative to the studies of section \ref{section_splitting_schemes}}
%In this section, we gather the figures relative to the studies of section \ref{section_splitting_schemes} as we think it eases the readability of the paper. 

\begin{figure}[h!]
	\centering
	\scalebox{0.60}{\input{chapitre5_img/RRGD_ex1.pgf}}
	\caption{Stationary distributions of RRGD for different learning rates on \exOne: a) $\eta=\frac{1}{\max(L_1,L_2)}$, b) $\eta=\frac{1}{3\max(L_1,L_2)}$, c) $\eta=\frac{1}{L_1+L_2}$, d) $\eta=\frac{1}{3(L_1+L_2)}$. The green straight line represents the position of the global minimum of $\R$.}
	\label{RRGD_ex1}
\end{figure}

\begin{figure}[h!]
	\centering
	\scalebox{0.60}{\input{chapitre5_img/sgd_ex2.pgf}}
	\caption{Stationary distributions of SGD for different learning rates on \exTwo: a) $\eta=\frac{1}{\max(L_1,L_2)}$, b) $\eta=\frac{1}{3\max(L_1,L_2)}$, c) $\eta=\frac{1}{L_1+L_2}$, d) $\eta=\frac{1}{3(L_1+L_2)}$. The green straight line represents the position of the global minimum of $\R$ and the blue one the position of the local minimum.}
	\label{sgd_ex2}
\end{figure}

\begin{figure}[h!]
	\centering
	\scalebox{0.60}{\input{chapitre5_img/RRGD_ex2.pgf}}
	\caption{Stationary distributions of RRGD for different learning rates on \exTwo: a) $\eta=\frac{1}{\max(L_1,L_2)}$, b) $\eta=\frac{1}{3\max(L_1,L_2)}$, c) $\eta=\frac{1}{L_1+L_2}$, d) $\eta=\frac{1}{3(L_1+L_2)}$. The green straight line represents the position of the global minimum of $\R$ and the blue one the position of the local minimum.}
	\label{RRGD_ex2}
\end{figure}



\begin{figure}[h!]
	\centering
	\scalebox{0.60}{\input{chapitre5_img/IG_ex1.pgf}}
	\caption{Stationary distributions of IG for different learning rates on \exOne: a) $\eta=\frac{1}{\max(L_1,L_2)}$, b) $\eta=\frac{1}{3\max(L_1,L_2)}$, c) $\eta=\frac{1}{L_1+L_2}$, d) $\eta=\frac{1}{3(L_1+L_2)}$. The green straight line represents the position of the global minimum of $\R$.}
	\label{IG_ex1}
\end{figure}

\begin{figure}[h!]
	\centering
	\scalebox{0.60}{\input{chapitre5_img/IG_ex2.pgf}}
	\caption{Stationary distributions of IG for different learning rates on \exTwo: a) $\eta=\frac{1}{\max(L_1,L_2)}$, b) $\eta=\frac{1}{3\max(L_1,L_2)}$, c) $\eta=\frac{1}{L_1+L_2}$, d) $\eta=\frac{1}{3(L_1+L_2)}$. The green straight line represents the position of the global minimum of $\R$ and the blue one the position of the local minimum.}
	\label{IG_ex2}
\end{figure}

\begin{figure}[h!]
	\centering
	\scalebox{0.60}{\input{chapitre5_img/Strang_ex1.pgf}}
	\caption{Stationary distributions of the Strang splitting for different learning rates on \exOne: a) $\eta=\frac{1}{\max(L_1,L_2)}$, b) $\eta=\frac{1}{3\max(L_1,L_2)}$, c) $\eta=\frac{1}{L_1+L_2}$, d) $\eta=\frac{1}{3(L_1+L_2)}$. The green vertical straight line represents the position of the global minimum of $\R$.}
	\label{Strang_ex1}
\end{figure}

\begin{figure}[h!]
	\centering
	\scalebox{0.60}{\input{chapitre5_img/Strang_ex2.pgf}}
	\caption{Stationary distributions of the Strang splitting for different learning rates on \exTwo: a) $\eta=\frac{1}{\max(L_1,L_2)}$, b) $\eta=\frac{1}{3\max(L_1,L_2)}$, c) $\eta=\frac{1}{L_1+L_2}$, d) $\eta=\frac{1}{3(L_1+L_2)}$. The green vertical line represents the position of the global minimum and the blue one the position of the local minimum of $\R$.}
	\label{Strang_ex2}
\end{figure} 

\begin{figure}[h!]
	\centering
	\scalebox{0.60}{\input{chapitre5_img/Speth_ex1.pgf}}
	\caption{Stationary distributions of the Speth splitting for different learning rates on \exOne: a) $\eta=\frac{1}{\max(L_1,L_2)}$, b) $\eta=\frac{1}{3\max(L_1,L_2)}$, c) $\eta=\frac{1}{L_1+L_2}$, d) $\eta=\frac{1}{3(L_1+L_2)}$. The green vertical line represents the position of the global minimum of $\R$.}
	\label{speth_ex1}
\end{figure}

\begin{figure}[h!]
	\centering
	\scalebox{0.60}{\input{chapitre5_img/Speth_ex2.pgf}}
	\caption{Stationary distributions of the Speth splitting for different learning rates on \exTwo: a) $\eta=\frac{1}{\max(L_1,L_2)}$, b) $\eta=\frac{1}{3\max(L_1,L_2)}$, c) $\eta=\frac{1}{L_1+L_2}$, d) $\eta=\frac{1}{3(L_1+L_2)}$. The green vertical line represents the position of the global minimum and the blue one the position of the local minimum of $\R$.}
	\label{speth_ex2}
\end{figure}

\begin{figure}[h!]
	\centering
	\scalebox{0.60}{\input{chapitre5_img/IAG_ex1.pgf}}
	\caption{Stationary distributions of \eqref{RAG_2batch} for different learning rates on \exOne: a) $\eta=\frac{1}{\max(L_1,L_2)}$, b) $\eta=\frac{1}{3\max(L_1,L_2)}$, c) $\eta=\frac{1}{L_1+L_2}$, d) $\eta=\frac{1}{3(L_1+L_2)}$. The green vertical line represents the position of the global minimum of $\R$.}
	\label{IAG_ex1}
\end{figure}

\begin{figure}[h!]
	\centering
	\scalebox{0.60}{\input{chapitre5_img/IAG_ex2.pgf}}
	\caption{Stationary distributions of \eqref{RAG_2batch} for different learning rates on \exTwo: a) $\eta=\frac{1}{\max(L_1,L_2)}$, b) $\eta=\frac{1}{3\max(L_1,L_2)}$, c) $\eta=\frac{1}{L_1+L_2}$, d) $\eta=\frac{1}{3(L_1+L_2)}$. The green vertical line represents the position of the global minimum and the blue one the position of the local minimum of $\R$.}
	\label{IAG_ex2}
\end{figure}

\clearpage

\section{\newG{Pseudo-code of the algorithms of sections \ref{extendedSpeth} and \ref{red_mem}}}
\label{pseudo_code}


\begin{algorithm}[H]
\DontPrintSemicolon
\SetAlgoLined
\SetKwInput{KwInput}{Input}
\SetKwInput{KwOutput}{Output}
\KwInput{Training set $(X_{\text{train}},Y_{\text{train}})$, test set $(X_{\text{test}},Y_{\text{test}})$\\
         model hyper‑parameters: \texttt{max\_epochs}, \texttt{batch\_size}, learning rate $\eta$, tolerance $\varepsilon$}
\KwOutput{Trained network parameters $\theta$, test accuracy}

%\BlankLine
%\tcc{Model definition}
%$\theta \gets$ \{Flatten$(28\times28)$, Dense$(128,\text{ReLU})$, Dense$(10,\text{softmax})$\}\;

\BlankLine
\tcc{Initialise loss, network, aggregator, gradient table}
$\mathcal{L}\leftarrow$ Loss \;
$f(\theta, \cdot)$ \;
%$\text{opt}\leftarrow$ SGD$(\eta)$\;
$Agg \gets 0$ (aggregator list matching $y=(\theta,v,...)^t$ depending on the used optimiser)\;
$Grad\_tab[\;]\gets 0$ (gradient table, one entryusedr per batch)\;

\BlankLine
\For{$\texttt{epochs}\gets 1$ \KwTo \texttt{max\_epochs}}{
    \tcc{Shuffle $(X_{\text{train}},Y_{\text{train}})$ to apply stochastic batching}
    $\,\text{loss}_{epochs}\gets 0 \;$\;
    
    \ForEach{batch index $b$}{
        Extract mini‑batch $(x_b,y_b)$\;
        Forward pass: $z_b\leftarrow f_\theta(x_b)$\;
        $\ell_b\leftarrow\mathcal{L}(y_b,z_b)$\;
        Back‑propagate gradients: $g_b\leftarrow\nabla_\theta \ell_b$\;
        \tcc{remove previous contribution for this batch}
        $Agg \leftarrow Agg - F_b(y,Grad\_tab[b])$\;  
        \tcc{Update the gradient table}
        $Grad\_tab[b]\leftarrow g_b$\;
        \tcc{Update aggregator}
        $Agg \leftarrow Agg + F_b(y,Grad\_tab[b])$\;
        
        \tcc{Optimiser step with aggregated gradient}
        $y \leftarrow y - \eta F_b(y,\,Agg)$\;
    }
    
    Compute aggregator norm $\,\|Grad\_tab\|$\;
    \If{$\|Grad\_tab\| < \varepsilon$}{
        \textbf{break}\;  \tcc{converged}
    }
}

\BlankLine
\tcc{Evaluation}
Test $z_{\text{test}}\gets f_\theta(X_{\text{test}})$\;
Compute $\text{acc}_{\text{test}}$\;

\Return{$\theta,\,\text{acc}_{\text{test}}$}
  \caption{Pseudo-code for the extended-Speth splitting procedure (see section \ref{extendedSpeth}) on an arbitrary full batch optimizer of flow $\dot{y}=F(y)=\sum_{i=1}^m F_i(y)$.}
\label{algo_speth}
\end{algorithm}















\begin{algorithm}[H]
\DontPrintSemicolon
\SetAlgoLined
\SetKwInput{KwInput}{Input}
\SetKwInput{KwOutput}{Output}
\KwInput{Training set $(X_{\text{train}},Y_{\text{train}})$, test set $(X_{\text{test}},Y_{\text{test}})$\\
         model hyper‑parameters: \texttt{max\_epochs}, \texttt{batch\_size}, learning rate $\eta$, tolerance $\varepsilon$}
\KwOutput{Trained network parameters $\theta$, test accuracy}

%\BlankLine
%\tcc{Model definition}
%$\theta \gets$ \{Flatten$(28\times28)$, Dense$(128,\text{ReLU})$, Dense$(10,\text{softmax})$\}\;

\BlankLine
\tcc{Initialise loss, network, aggregator, grad., pivot }
$\mathcal{L}\leftarrow$ Loss \;
%$\text{opt}\leftarrow$ SGD$(\eta)$\;
$Agg \gets 0$ (aggregator list matching $y=(\theta,v)^t$ depending on the used optimiser)\;
SumAgg \gets 0 \;
%$Grad\_tab[\;]\gets 0$ (gradient table, one entry per batch)\;
$L\gets 0$ (stiffness evaluation, one entry per batch)\;
$\theta_{\text{pivot}} = \theta$ \;

\BlankLine
\For{$\texttt{epochs}\gets 0$ \KwTo \texttt{max\_epochs}}{
    \tcc{Shuffle $(X_{\text{train}},Y_{\text{train}})$ to apply stochastic batching}
    $\,\text{loss}_{epochs}\gets 0 \;$\;
    \If{\texttt{epochs}==0}{
      \eta = 0\;
    }
 
    \ForEach{batch index $b$}{
        Extract mini‑batch $(x_b,y_b)$\;
        Forward pass: $z_b\leftarrow f_\theta(x_b)$\;
        $\ell_b\leftarrow\mathcal{L}(y_b,z_b)$\;
        Back‑propagate gradients at pivot   location: $g_\text{pivot}\leftarrow\nabla_{\theta} \ell_b(\theta_\text{pivot})$\;
        Back‑propagate gradients at current location: $g_b\leftarrow\nabla_\theta \ell_b(\theta)$\;
        $Agg \leftarrow Agg - F_b(y,g_\text{pivot})$ \tcc*{remove prev. contrib. }
        $Grad\_tab[b]\leftarrow g_b$ \tcc*{Update the gradient table}
        SumAgg \gets SumAgg + Grad\_tab[g]  \tcc*{Update $g_n$}
        $Agg \leftarrow Agg + F_b(y,Grad\_tab[b])$ \tcc*{Update aggregator}
        \If{$L < \|g_b\|$}{ 
           $\theta_\text{pivot} = \theta$\tcc*{Update the pivot weights}

           $L =  \|g_b\|$
        }
        \If{b==number\_of\_batch}{
           Agg = SumAgg
        }
        \tcc{Optimiser step with aggregated gradient}
        $y=(\theta,v) \leftarrow y(\theta,v) - \eta F_b(y,\,Agg)$\;
    }
    
    Compute aggregator norm $\,\|Grad\_tab\|$\;
    \If{$\|Grad\_tab\| < \varepsilon$}{
        \textbf{break}\;  \tcc{converged}
    }
}
\BlankLine
\tcc{Evaluation}
Test $z_{\text{test}}\gets f_\theta(X_{\text{test}})$\;
Compute $\text{acc}_{\text{test}}$\;

\Return{$\theta,\,\text{acc}_{\text{test}}$}
\caption{Pseudo-code for the extended-Speth splitting procedure {\em with memory reduction} (see section \ref{red_mem}) on an arbitrary full batch optimizer of flow $\dot{y}=F(y)=\sum_{i=1}^m F_i(y)$.}
\label{algo_red_mem}
\end{algorithm}


\section{\newG{Proof of theorem \ref{th}}}
\label{proof}

Assume $(F_i)_{i\in\{1,...,m\}}$ as in theorem \ref{th} and consider first update \eqref{RAG_scheme} recalled here: 
\begin{equation}
  y_{n+i/m} = y_{n+(i-1)/m}+\eta \left(\sum_{j=1}^i F_j\left(y_{n+(j-1)/m}\right)+\sum_{j=i+1}^m F_j\left(y_{n-1+(j-1)/m}\right)\right).
\end{equation}
Assume furthermore that $(y_n)_{n\in\mathbb{N}}$ converges toward $y_\infty$. 
Then we have 
\begin{equation}
  0= \eta \left(\sum_{j=1}^i F_j\left(y_{\infty}\right)+\sum_{j=i+1}^m F_j\left(y_{\infty}\right)\right) = \eta \sum_{j=1}^m F_j(y_\infty) = \eta F(y_\infty),
\end{equation}
leading to $F(y_\infty) = 0$ as soon as $\eta\neq 0$. 
The property of the full batch optimizer (condition 1 of theorem \ref{th}) allows to conclude. \\ \ \\

Assume once again that $(F_i)_{i\in\{1,...,m\}}$ are as in theorem \ref{th} and consider update \eqref{RAGL_splitting} recalled here: 
\begin{equation}
  y_{n+i/m} = y_{n+(i-1)/m}+\eta \left(g_n+\displaystyle{\sum_{j=1}^i} \left[F_j\left(y_{n+(j-1)/m}\right)-F_j\left(y_{{n-1+(i_{\text{pivot}}-1})/m}\right)\right]\right).
\end{equation}
Assume furthermore that $(y_n)_{n\in\mathbb{N}}$ converges toward $y_\infty$. 
Then we have 
\begin{equation}
  0= \eta \left(g_\infty+\displaystyle{\sum_{j=1}^i} \left[F_j\left(y_{\infty}\right)-F_j\left(y_{\infty}\right)\right] \right) = \eta \left(\sum_{j=1}^m F_j(y_\infty) +0\right)= \eta F(y_\infty),
\end{equation}
which ends the proof. 


%\section{Classical algorithms}
%\label{annexe_adam}
%
%In this section, we recall the stochastic optimizers used in this paper, since there exists many versions of a same algorithm. 
%
%\begin{algorithm}[h!]
%	\caption{{\it Stochastic Gradient Descent}: SGD}
%	\begin{algorithmic}
%		\REQUIRE initial values $\theta$ and $\eta$.
%		
%		\FOR{$n=0, \dots, mn_{max}-1$}
%		\STATE $i \leftarrow$ uniformly sampled in $\{1,\dots,m\}$
%		\STATE $\theta \leftarrow \theta-\eta \nR_i(\theta)$
%		\ENDFOR
%		
%		\RETURN $\theta$
%	\end{algorithmic}
%	\label{algo_sgd}
%\end{algorithm} 
%
%\begin{algorithm}[h!]
%	\caption{{\it Random Reshuffle Gradient Descent}: RRGD}
%	\begin{algorithmic}
%		\REQUIRE initial values $\theta$ and $\eta$.
%		
%		\FOR{$n=0, \dots, n_{max}-1$}
%		\STATE $\Pi \leftarrow$ random permutation of $\{1,\dots,m\}$
%		\FOR{$i=1, \dots, m$}
%		\STATE $\theta \leftarrow \theta-\eta \nR_{\Pi(i)}(\theta)$
%		\ENDFOR
%		\ENDFOR
%		\RETURN $\theta$
%	\end{algorithmic}
%	\label{algo_RRGD}
%\end{algorithm} 
%
%\begin{algorithm}[h!]
%	\caption{{\it Random Reshuffle AWB}: RRAWB}
%	\begin{algorithmic}
%		\REQUIRE initial values $\theta$, $\eta$, $\betone$, $\bettwo$ and $\epsilon_a$.
%		\STATE $v\leftarrow 0$, $s\leftarrow 0$
%		\FOR{$n=0, \dots, n_{max}-1$}
%		\STATE $\Pi \leftarrow$ random permutation of $\{1,\dots,m\}$
%		\FOR{$i=1, \dots, m$}
%		\STATE $v \leftarrow(1-\betone)v + \betone \nR_{\Pi(i)}(\theta)$
%		\STATE $s \leftarrow (1-\bettwo)s + \bettwo \nR_{\Pi(i)}(\theta)^2$
%		\STATE $\theta \leftarrow \theta -\eta \dfrac{v}{\epsilon_a+\sqrt{s}}$
%		\ENDFOR
%		\ENDFOR
%		\RETURN $\theta$
%	\end{algorithmic}
%	\label{algo_RRAWB}
%\end{algorithm} 
%
%\begin{algorithm}[h!]
%	\caption{{\it Random Reshuffle Adam}: RRAdam}
%	\begin{algorithmic}
%		\REQUIRE initial values $\theta$, $\eta$, $\betone$, $\bettwo$ and $\epsilon_a$.
%		\STATE $v\leftarrow 0$, $s\leftarrow 0$
%		\FOR{$n=0, \dots, n_{max}-1$}
%		\STATE $\Pi \leftarrow$ random permutation of $\{1,\dots,m\}$
%		\FOR{$i=1, \dots, m$}
%		\STATE $v \leftarrow(1-\betone)v + \betone \nR_{\Pi(i)}(\theta)$
%		\STATE $s \leftarrow (1-\bettwo)s + \bettwo \nR_{\Pi(i)}(\theta)^2$
%		\STATE $\theta \leftarrow \theta -\eta \dfrac{\sqrt{1-\bettwo^{n+1}}}{1-\betone^{n+1}} \dfrac{v}{\epsilon_a+\sqrt{s}}$
%		\ENDFOR
%		\ENDFOR
%		\RETURN $\theta$
%	\end{algorithmic}
%	\label{algo_RRAdam}
%\end{algorithm} 
%
%\begin{remark}
%	We have written above the stochastic algorithms with the classical stopping criterion, namely the maximal number of epochs. All through the sections
%        \ref{section_stationary}, \ref{section_splitting_schemes}, \ref{num_res}, they are implemented with different criteria: maximal time $t_f$ and full-batch gradient $\|\nR(\theta)\|/P \leq \epsilon$.
%\end{remark}


\clearpage


\end{document}
