@article{Lyap_Theory_Bilel,
	author = {B. Bensaid and G. Poette and R. Turpault},
	title = {{An Abstract Lyapunov Control Optimizer: Local Stabilization and Global Convergence}},
	journal = {Arxiv},
	year = {2024}
}
@article{DESPRES2020109275,
title = {Machine Learning design of Volume of Fluid schemes for compressible flows},
journal = {Journal of Computational Physics},
volume = {408},
pages = {109275},
year = {2020},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2020.109275},
url = {https://www.sciencedirect.com/science/article/pii/S0021999120300498},
author = {Bruno Després and Hervé Jourdren},
keywords = {VOF, CFD, ML},
abstract = {Our aim is to establish the feasibility of Machine-Learning-designed Volume of Fluid algorithms for compressible flows. We detail the incremental steps of the construction of a new family of Volume of Fluid-Machine Learning (VOF-ML) schemes adapted to bi-material compressible Euler calculations on Cartesian grids. An additivity principle is formulated for the Machine Learning datasets. We explain a key feature of this approach which is how to adapt the compressible solver to the preservation of natural symmetries. The VOF-ML schemes show good accuracy for advection of a variety of interfaces, including regular interfaces (straight lines and arcs of circle), Lipschitz interfaces (corners) and non Lipschitz triple point (the Trifolium test problem). Basic comparisons with a SLIC/Downwind scheme are presented together with elementary bi-material calculations with shocks.}
}

@book{despres2022neural,
  title={Neural Networks and Numerical Analysis},
  author={Despr{\'e}s, B.},
  isbn={9783110783186},
  series={De Gruyter Series in Applied and Numerical Mathematics},
  url={https://books.google.fr/books?id=brB7EAAAQBAJ},
  year={2022},
  publisher={De Gruyter}
}
@inproceedings{Bilel_ICML,
	author = {B. Bensaid and G. Poette and R. Turpault},
	title = {{Convergence of the Iterates for Momentum and RMSProp for Local Lipshitz Functions: Adaptation is the Key}},
	booktitle = {Arxiv},
	year = {2024}
}

@phdthesis{Bilel_thesis,
	TITLE = {{Analyse et D{\'e}veloppement de Nouveaux Optimiseurs en Machine Learning}},
	AUTHOR = {Bensaid, Bilel},
	URL = {https://theses.hal.science/tel-04808839},
	NUMBER = {2024BORD0218},
	SCHOOL = {{Universit{\'e} de Bordeaux}},
	YEAR = {2024},
	MONTH = Oct,
	KEYWORDS = {Neural Networks ; Non-Convex Optimization ; Lyapunov Stability ; Kurdyka-Lojasiewicz ; Dissipative Scheme ; Armijo ; Mini-Batch Optimization ; Splitting scheme ; R{\'e}seaux de Neurones ; Optimisation Non-Convexe ; Stabilit{\'e} de Lyapunov ; Kurdyka-Lojasiewicz ; Sch{\'e}ma Dissipatif ; Armijo ; Optimisation Mini-Lot ; Sch{\'e}ma de Splitting},
	TYPE = {Theses},
	PDF = {https://theses.hal.science/tel-04808839v1/file/BENSAID_BILEL_2024.pdf},
	HAL_ID = {tel-04808839},
	HAL_VERSION = {v1},
}

%-------------------------------------------------------------------

@article{Bilel,
	author = {B. Bensaid and G. Poette and R. Turpault},
	title = {Deep {L}earning {O}ptimization from a {C}ontinuous and {E}nergy {P}oint of {V}iew},
	journal = {Journal of Scientific Computing},
	year = {2023}
}

@inproceedings{Xavier,
	title={{Understanding the Difficulty of Training Deep Feedforward Neural Networks}},
	author={G. Xavier and Y. Bengio},
	booktitle={AISTATS},
	year={2010}
}

@article{mnist_lecun,
	author={Y. Lecun and L. Bottou and Y. Bengio and P. Haffner},
	journal={Proceedings of the IEEE}, 
	title={{Gradient-Based Learning Applied to Document Recognition}}, 
	year={1998},
	volume={86},
	number={11},
	pages={2278-2324},
	doi={10.1109/5.726791}
}

@inproceedings{LeNet1,
	author = {Y.LeCun and B. Boser and J. Denker and D.Henderson and R. Howard and W. Hubbard and L. Jackel},
	booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
	editor = {D. Touretzky},
	publisher = {Morgan-Kaufmann},
	title = {{Handwritten digit recognition with a back-propagation network}},
	url = {https://proceedings.neurips.cc/paper/1989/file/53c3bce66e43be4f209556518c2fcb54-Paper.pdf},
	volume = {2},
	year = {1989}
}

@article{gelu,
	title={{Gaussian Error Linear Units (GELUs)}},
	author={D. Hendrycks and K. Gimpel},
	journal={arXiv: Learning},
	year={2016}
}

@article{continuous_general,
	author = {A.B. Da Silva and M. Gazeau},
	year = {2020},
	month = {06},
	pages = {1-42},
	journal={Journal of Machine Learning Research (JMLR)},
	title = {{A} {G}eneral {S}ystem of {D}ifferential {E}quations to {M}odel {F}irst {O}rder {A}daptive {A}lgorithms}
}

@article{Nesterov,
	title={{A method for Solving the Convex Programming Problem with Convergence Rate $O(1/k^2)$}},
	author={Y. Nesterov},
	journal={Proceedings of the USSR Academy of Sciences},
	year={1983},
	volume={269},
	pages={543-547}
}

@article{Polyak,
	title = {{Some Methods of Speeding Up the Convergence of Iteration Methods}},
	journal = {USSR Computational Mathematics and Mathematical Physics},
	volume = {4},
	number = {5},
	pages = {1-17},
	year = {1964},
	issn = {0041-5553},
	doi = {https://doi.org/10.1016/0041-5553(64)90137-5},
	author = {B.T. Polyak}
}

% Value convergence by Lyapunov function and states that it is stable (argument is false).
@article{polyak_momentum_stability,
	title = {{Lyapunov Functions: An Optimization Theory Perspective}},
	journal = {IFAC-PapersOnLine},
	volume = {50},
	number = {1},
	pages = {7456-7461},
	year = {2017},
	note = {20th IFAC World Congress},
	issn = {2405-8963},
	doi = {https://doi.org/10.1016/j.ifacol.2017.08.1513},
	url = {https://www.sciencedirect.com/science/article/pii/S2405896317320955},
	author = {B. Polyak and P. Shcherbakov},
	keywords = {optimization, Lyapunov functions, asymptotic stability, gradient method, heavy-ball method, pendulum equation, synchronous motor, basin of attraction},
}

@article{Lyapunov_Nesterov,
	author = {A.C. Wilson and B. Recht and M.I. Jordan},
	keywords = {Optimization and Control (math.OC), Data Structures and Algorithms (cs.DS), FOS: Mathematics, FOS: Mathematics, FOS: Computer and information sciences, FOS: Computer and information sciences},
	title = {{A Lyapunov Analysis of Accelerated Methods in Optimization}},
	journal = {Journal of Machine Learning Research},
	year = {2021}
}

%Bregman ODE: generalizes Nesterov one, Cubic-regularized Nesterov, .... Discretization mathching the continuous cv rates for uniformly convex functions.
@article{variational_perspective,
	author = {A. Wibisono  and A.C. Wilson  and M.I. Jordan },
	title = {{A Variational Perspective on Accelerated Methods in Optimization}},
	journal = {Proceedings of the National Academy of Sciences},
	volume = {113},
	number = {47},
	pages = {E7351-E7358},
	year = {2016},
	doi = {10.1073/pnas.1614734113}
}

@article{armijo,
	author = {L. Armijo},
	title = {{Minimization of Functions Having Lipschitz Continuous First Partial Derivatives.}},
	volume = {16},
	journal = {Pacific Journal of Mathematics},
	number = {1},
	publisher = {Pacific Journal of Mathematics},
	pages = {1 -- 3},
	abstract = {},
	year = {1966},
}

@article{Rondepierre,
	author = {D. Noll and A. Rondepierre},
	year = {2013},
	month = {07},
	pages = {},
	title = {{Convergence of Linesearch and Trust-Region Methods using the Kurdyka-Łojasiewicz
	Inequality}},
	volume = {50},
	isbn = {978-1-4614-7620-7},
	journal = {Springer Proceedings in Mathematics and Statistics},
	doi = {10.1007/978-1-4614-7621-4_27}
}

%------------------------------------------------ Notations/Bases -----------------------------------------------------------------------------
@article{maths_DL,
	doi = {10.48550/ARXIV.2105.04026},
	author = {J. Berner and P. Grohs and G. Kutyniok and P. Petersen},
	keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	title = {The {M}odern {M}athematics of {D}eep {L}earning},
	publisher = {arXiv},
	year = {2021},
	copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{backpropagation,
	title = {Backpropagation {N}eural {N}etworks: a {T}utorial},
	journal = {Chemometrics and Intelligent Laboratory Systems},
	volume = {18},
	number = {2},
	pages = {115-155},
	year = {1993},
	issn = {0169-7439},
	doi = {https://doi.org/10.1016/0169-7439(93)80052-J},
	url = {https://www.sciencedirect.com/science/article/pii/016974399380052J},
	author = {B.J. Wythoff}
}

@misc{DL_opti,
	title={Optimization for {D}eep {L}earning: {T}heory and {A}lgorithms}, 
	author={R. Sun},
	year={2019},
	eprint={1912.08957},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}


%--------------------------------------------------Applications Physiques-----------------------------------------------------------------------------------

@article{plasma,
	author = {G. Kluth  and K.D. Humbird  and B.K. Spears  and J.L. Peterson  and H.A. Scott  and M.V. Patel  and J. Koning  and M. Marinak  and L. Divol  and C.V. Young},
	title = {Deep {L}earning for {NLTE} {S}pectral {O}pacities},
	journal = {Physics of Plasmas},
	volume = {27},
	number = {5},
	pages = {052707},
	year = {2020},
	doi = {10.1063/5.0006784}
}

@INPROCEEDINGS{KluthRipoll,
	author={J.F. Ripoll and G. Kluth and S. Has and A. Fischer and M. Mougeot and E. Camporeale},
	booktitle={2022 3rd URSI Atlantic and Asia Pacific Radio Science Meeting (AT-AP-RASC)},
	title={Exploring {P}itch-{A}ngle {D}iffusion during {H}igh {S}peed {S}treams with {N}eural {N}etworks},
	year={2022},
	volume={},
	number={},
	pages={1-4},
	doi={10.23919/AT-AP-RASC54737.2022.9814235}
}

@article{DespresJourdren,
	title = {{M}achine {L}earning {D}esign of {V}olume of {F}luid {S}chemes for {C}ompressible {F}lows},
	journal = {Journal of Computational Physics},
	volume = {408},
	pages = {109275},
	year = {2020},
	issn = {0021-9991},
	doi = {https://doi.org/10.1016/j.jcp.2020.109275},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999120300498},
	author = {B. Després and H. Jourdren},
	keywords = {VOF, CFD, ML}
}

@article{LamyFeugeas,
	title = {Modeling of {E}lectron {N}onlocal {T}ransport in {P}lasmas using {A}rtificial {N}eural {N}etworks},
	author = {C. Lamy and B. Dubroca and P. Nicola\"{\i} and V. Tikhonchuk and J.L. Feugeas},
	journal = {Phys. Rev. E},
	volume = {105},
	issue = {5},
	pages = {055201},
	numpages = {10},
	year = {2022},
	month = {May},
	publisher = {American Physical Society},
	doi = {10.1103/PhysRevE.105.055201},
	url = {https://link.aps.org/doi/10.1103/PhysRevE.105.055201}
}
@article{FRANCK,
title = {Approximately well-balanced Discontinuous Galerkin methods using bases enriched with Physics-Informed Neural Networks},
journal = {Journal of Computational Physics},
volume = {512},
pages = {113144},
year = {2024},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2024.113144},
url = {https://www.sciencedirect.com/science/article/pii/S0021999124003930},
author = {Emmanuel Franck and Victor Michel-Dansac and Laurent Navoret},
keywords = {Hyperbolic balance laws, Discontinuous Galerkin schemes, Scientific machine learning, Well-balanced schemes, PINNs},
abstract = {This work concerns the enrichment of Discontinuous Galerkin (DG) bases, so that the resulting scheme provides a much better approximation of steady solutions to hyperbolic systems of balance laws. The basis enrichment leverages a prior – an approximation of the steady solution – which we propose to compute using a Physics-Informed Neural Network (PINN). To that end, after presenting the classical DG scheme, we show how to enrich its basis with a prior. Convergence results and error estimates follow, in which we prove that the basis with prior does not change the order of convergence, and that the error constant is improved. To construct the prior, we elect to use parametric PINNs, which we introduce, as well as the algorithms to construct a prior from PINNs. We finally perform several validation experiments on four different hyperbolic balance laws to highlight the properties of the scheme. Namely, we show that the DG scheme with prior is much more accurate on steady solutions than the DG scheme without prior, while retaining the same approximation quality on unsteady solutions.}
}
@article{acc_newt,
title = {Accelerating the convergence of Newton’s method for nonlinear elliptic PDEs using Fourier neural operators},
journal = {Communications in Nonlinear Science and Numerical Simulation},
volume = {140},
pages = {108434},
year = {2025},
issn = {1007-5704},
doi = {https://doi.org/10.1016/j.cnsns.2024.108434},
url = {https://www.sciencedirect.com/science/article/pii/S1007570424006191},
author = {Joubine Aghili and Emmanuel Franck and Romain Hild and Victor Michel-Dansac and Vincent Vigon},
keywords = {Newton’s method, Neural networks, Fourier neural operators, Nonlinear elliptic PDEs},
abstract = {It is well known that Newton’s method can have trouble converging if the initial guess is too far from the solution. Such a problem particularly occurs when this method is used to solve nonlinear elliptic partial differential equations (PDEs) discretized via finite differences. This work focuses on accelerating Newton’s method convergence in this context. We seek to construct a mapping from the parameters of the nonlinear PDE to an approximation of its discrete solution, independently of the mesh resolution. This approximation is then used as an initial guess for Newton’s method. To achieve these objectives, we elect to use a Fourier neural operator (FNO). The loss function is the sum of a data term (i.e., the comparison between known solutions and outputs of the FNO) and a physical term (i.e., the residual of the PDE discretization). Numerical results, in one and two dimensions, show that the proposed initial guess accelerates the convergence of Newton’s method by a large margin compared to a naive initial guess, especially for highly nonlinear and anisotropic problems, with larger gains on coarse grids.}
}
@article{FV_scheme,
	author = {A. Bourriaud and R. Loubère and R. Turpault},
	year = {2020},
	month = {07},
	pages = {},
	title = {A {P}riori {N}eural {N}etworks {V}ersus A {P}osteriori {MOOD} {L}oop: A {H}igh {A}ccurate {1D} {FV} {S}cheme {T}esting {B}ed},
	volume = {84},
	journal = {Journal of Scientific Computing},
	doi = {10.1007/s10915-020-01282-1}
}

@article{reentry,
	author = {P. Novello and G. Poëtte and D. Lugato and S. Peluchon and P.M. Congedo},
	keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), Computational Physics (physics.comp-ph), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Physical sciences, FOS: Physical sciences},
	title = {{Accelerating Hypersonic Reentry Simulations using Deep Learning-Based Hybridization (with guarantees)}},
	journal = {Journal of Computational Physics},
	volume = {498},
	pages = {112700},
	year = {2024},
	issn = {0021-9991},
	doi = {https://doi.org/10.1016/j.jcp.2023.112700},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999123007957},
}

@article{Raissi,
	title = {{Physics-Informed Neural Networks: A Deep Learning Framework for Solving Forward and Inverse Problems involving Nonlinear Partial Differential Equations}},
	journal = {Journal of Computational Physics},
	volume = {378},
	pages = {686-707},
	year = {2019},
	issn = {0021-9991},
	doi = {https://doi.org/10.1016/j.jcp.2018.10.045},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999118307125},
	author = {M. Raissi and P. Perdikaris and G.E. Karniadakis},
	keywords = {Data-driven scientific computing, Machine learning, Predictive modeling, Runge–Kutta methods, Nonlinear dynamics}
}

%---------------------------------------- Classical Benchmarks------------------------------------------------------------------------

@article{runge_test,
	author = {I. Ahmadianfar and A.A. Heidari and A. Gandomi and X. Chu and H. Chen},
	year = {2021},
	month = {04},
	pages = {115079},
	title = {{RUN Beyond the Metaphor: An Efficient Optimization Algorithm Based on Runge Kutta Method}},
	volume = {https://aliasgharheidari.com/RUN.html},
	journal = {Expert Systems with Applications},
	doi = {10.1016/j.eswa.2021.115079}
}

@article{image_recognition,
	author = {W. Mo and X. Luo and Y. Zhong and W. Jiang},
	year = {2019},
	month = {06},
	pages = {022026},
	title = {{Image Recognition Using Convolutional Neural Network Combined with Ensemble Learning Algorithm}},
	volume = {1237},
	journal = {Journal of Physics: Conference Series},
	doi = {10.1088/1742-6596/1237/2/022026}
}

@article{language_recognition,  
	author={F. Richardson and D. Reynolds and N. Dehak},  
	journal={IEEE Signal Processing Letters},   
	title={{Deep Neural Network Approaches to Speaker and Language Recognition}},   
	year={2015},  
	volume={22},  
	number={10},  
	pages={1671-1675},  
	doi={10.1109/LSP.2015.2420092}
}

@article{tS1,
	author = {G.M. Behery and A. El-Harby and M. El-Bakry},
	year = {2009},
	month = {01},
	pages = {},
	title = {{Reorganizing Neural Network System for Two Spirals and Linear Low-Density Polyethylene Copolymer Problems}},
	volume = {2009},
	journal = {Applied Computational Intelligence and Soft Computing},
	doi = {10.1155/2009/721370}
}

@article{tS2,
	author = {D. Tian and Y. Liu and J. Wang},
	year = {2007},
	month = {12},
	pages = {231-242},
	title = {{Fuzzy Neural Network Structure Identification Based on Soft Competitive Learning}},
	volume = {4},
	journal = {Int. J. Hybrid Intell. Syst.},
	doi = {10.3233/HIS-2007-4403}
}

@article{LeCunMNIST,
	author = {Y. Lecun and L. Bottou and Y. Bengio and P. Haffner},
	year = {1998},
	month = {12},
	pages = {2278 - 2324},
	title = {{Gradient-Based Learning Applied to Document Recognition}},
	volume = {86},
	journal = {Proceedings of the IEEE},
	doi = {10.1109/5.726791}
}

@Techreport{CIFAR,
	author = {A. Krizhevsky and G. Hinton},
	address = {Toronto, Ontario},
	institution = {University of Toronto},
	number = {0},
	publisher = {Technical report, University of Toronto},
	title = {{Learning Multiple Layers of Features from Tiny Images}},
	year = {2009}
}

@article{CaliforniaHousing,
	title = {{Sparse Spatial Autoregressions}},
	journal = {Statistics \& Probability Letters},
	volume = {33},
	number = {3},
	pages = {291-297},
	year = {1997},
	issn = {0167-7152},
	doi = {https://doi.org/10.1016/S0167-7152(96)00140-X},
	url = {https://www.sciencedirect.com/science/article/pii/S016771529600140X},
	author = {R.K. Pace and R. Barry},
	keywords = {Spatial autoregression, SAR, Sparse matrices}
}

@article{twoSpiral,
	author = {K.C. Stephan and W. Lukasz},
	title = {{Variations of the Two-Spiral Task}},
	journal = {Connection Science},
	volume = {19},
	number = {2},
	pages = {183-199},
	year  = {2007},
	publisher = {Taylor & Francis},
	doi = {10.1080/09540090701398017},
}

%------------------------------------ Classical Optimizers------------------------------------------------------------------------

% sufficient condition for rms/adam cv, gt bounded
@INPROCEEDINGS{zou_rms,
	author={F. Zou and L. Shen and Z. Jie and W. Zhang and W. Liu},
	booktitle={Conference on Computer Vision and Pattern Recognition (CVPR)}, 
	title={{A} {S}ufficient {C}ondition for {C}onvergences of {A}dam and {RMSP}rop}, 
	year={2019},
	pages={11119-11127},
	doi={10.1109/CVPR.2019.01138}
}

%without bounded assumption: cv of the min gradients for all beta and eta=1/sqrt(t).
@inproceedings{rms_not_bounded,
	title={{RMSP}rop {C}onverges with {P}roper {H}yper-{P}arameter},
	author={N. Shi and D. Li and M. Hong and R. Sun},
	booktitle={International Conference on Learning Representations (ICLR)},
	year={2021},
	url={https://api.semanticscholar.org/CorpusID:235420301}
}

% Clipping gradient
@inproceedings{RNN_difficult,
	author = {R. Pascanu and T. Mikolov and Y. Bengio},
	title = {{On the difficulty of training recurrent neural networks}},
	year = {2013},
	booktitle = {International Conference on International Conference on Machine Learning (ICML)}
}

% Convergence of the gradients, the function values (convex or p growth condition) under a more restrictive assumption than Lipshitz gradient (strongly smooth)
% Called rescaled gradient descent in this paper.
@inproceedings{pGD,
	author = {A.C. Wilson and L. Mackey and A. Wibisono},
	title = {{Accelerating Rescaled Gradient Descent: Fast Optimization of Smooth Functions}},
	booktitle = {Neural Information Processing Systems (NeurIPS)},
	year = {2019},
}


%p-GD, continuous proof, discrete convergence for bounded gradient and sufficiently small step size for p-growth condition
@INPROCEEDINGS{cv_pflow,
	author={O. Romero and M. Benosman and G.J. Pappas},
	booktitle={Conference on Decision and Control (CDC)}, 
	title={{ODE discretization schemes as optimization algorithms}}, 
	year={2022},
	doi={10.1109/CDC51059.2022.9992691}
}

@inproceedings{RAdam,
	doi = {10.48550/ARXIV.1908.03265},
	author = {L. Liu and H. Jiang and P. He and W. Chen and X. Liu and J. Gao and J. Han},
	year = {2020},
	month = {08},
	pages = {13},
	organization={ICLR},
	title = {{On the Variance of the Adaptive Learning Rate and Beyond}}
}

@inproceedings{LookAhead,
	doi = {10.48550/ARXIV.1907.08610},
	author = {M. Zhang and J. Lucas and G. Hinton and J. Ba},
	year = {2019},
	organization={NeurIPS},
	month = {07},
	pages = {12},
	title = {{Lookahead Optimizer: k steps forward, 1 step back}}
}


@article{RMSProp,
	title={{Lecture 6.5-rmsprop: Divide the Gradient by a Running Average of its Recent Magnitude}},
	author={T. Tieleman and G. Hinton and al},
	journal={COURSERA: Neural networks for machine learning},
	volume={4},
	number={2},
	pages={26--31},
	year={2012}
}

@article{Adadelta,
	doi = {10.48550/ARXIV.1212.5701},
	publisher={ArXiv},
	author = {M. Zeiler},
	year = {2012},
	month = {12},
	pages = {},
	title = {{ADADELTA: An adaptive learning rate method}},
	volume = {1212}
}

@article{Adagrad,
	author = {A. Lydia and S. Francis},
	journal={International journal of information and computing science},
	year = {2019},
	month = {05},
	pages = {566-568},
	title = {{Adagrad - An Optimizer for Stochastic Gradient Descent}},
	volume = {Volume 6}
}

@inproceedings{Adam,
	doi = {10.48550/ARXIV.1412.6980},
	author = {D. Kingma and J. Ba},
	year = {2014},
	month = {12},
	pages = {},
	title = {{Adam: A Method for Stochastic Optimization}},
	booktitle = {International Conference on Learning Representations (ICLR)}
}

@inproceedings{AMSGrad,
	doi = {10.48550/ARXIV.1904.09237},
	title={{On the Convergence of Adam and Beyond}},
	author={S.J. Reddi and S. Kale and S. Kumar},
	booktitle={International Conference on Learning Representations (ICLR)},
	year={2018},
}

@article{LM1,
	author={M.T. Hagan and M.B. Menhaj},
	journal={IEEE Transactions on Neural Networks}, 
	title={{Training feedforward networks with the Marquardt algorithm}}, 
	year={1994},
	volume={5},
	number={6},
	pages={989-993},
	doi={10.1109/72.329697}
}

%Momentum applied to RNN
@article{momentum_sutskever_RNN,
	author = {I. Sutskever and J. Martens and G. Dahl and G. Hinton},
	title = {{On the Importance of Initialization and Momentum in Deep Learning}},
	journaltitle = {Journal of Machine Learning Research (JMLR)},
	year = {2013},
}


%-------------------- Adam-rms---------------------------

@article{adam1,
	title={{Convergence Guarantees for RMSProp and ADAM in Non-Convex Optimization and an Empirical Comparison to Nesterov Acceleration}},
	author={S. De and A. Mukherjee and E. Ullah},
	journal={arXiv: Learning},
	year={2018},
	url={https://api.semanticscholar.org/CorpusID:52944421}
}

@article{adam2,
	author = {C. Chen and L. Shen and F. Zou and W. Liu},
	title = {{Towards practical Adam: Non-Convexity, Convergence Theory, and Mini-Batch Acceleration}},
	year = {2022},
	issue_date = {January 2022},
	publisher = {JMLR.org},
	volume = {23},
	number = {1},
	issn = {1532-4435},
	journal = {Journal of Machine Learning Research},
	month = {jan},
	articleno = {229},
	numpages = {47}
}

%------------------------------------------ SGD bounded assumption -------------------------------------------------------------------

% The original article about SGD.
@article{SGD_Robins,
	author = {H. Robbins and S. Monro},
	title = {{A Stochastic Approximation Method}},
	volume = {22},
	journal = {The Annals of Mathematical Statistics},
	number = {3},
	publisher = {Institute of Mathematical Statistics},
	pages = {400 -- 407},
	year = {1951},
	doi = {10.1214/aoms/1177729586},
	URL = {https://doi.org/10.1214/aoms/1177729586}
}

% Basis on pseudotrajectory and bounded assumption.
@InProceedings{Bertsekas_basis,
	author={M. Bena{\"i}m},
	title={{Dynamics of Stochastic Approximation Algorithms}},
	booktitle={S{\'e}minaire de Probabilit{\'e}s XXXIII},
	year={1999},
	publisher={Springer Berlin Heidelberg},
	pages={1--68},
	isbn={978-3-540-48407-3}
}

% Same thing
@book{sgd_dynamical_basis,
	author = {V.S. Borkar},
	title = {{Stochastic Approximation}},
	year = {2009},
	doi={https://doi.org/10.1007/978-93-86279-38-5},
	pages={174}
}

%--------------------------------- Bounded noise --------------------------------------------------------------------------------------

% Convergence of values and gradients without bounded assumption but with bounded variance and standard diminishing step sizes.
@article{bertsekas_theorem,
	author = {D.P. Bertsekas and J.N. Tsitsiklis},
	title = {{Gradient Convergence in Gradient methods with Errors}},
	journal = {SIAM Journal on Optimization},
	volume = {10},
	number = {3},
	pages = {627-642},
	year = {2000},
	doi = {10.1137/S1052623497331063}
}


% Almost sure convergence of the iterates to a connected component of critical points for a general diminishing step size.
% Assumption: q-th moment of the error bounded.
% Strict saddle point avoidance.
@inproceedings{sgd_general_diminishing_lr,
	author = {M. Panayotis and H. Nadav and K. Ali and C. Volkan},
	title = {{On the Almost Sure Convergence of Stochastic Gradient Descent in Non-Convex Problems}},
	year = {2020},
	isbn = {9781713829546},
	publisher = {Curran Associates Inc.},
	address = {Red Hook, NY, USA},
	booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
	articleno = {95},
	numpages = {12},
	series = {NIPS '20}
}


% SGD upper bound (gradient) for lipshitz functions (eps^{-4}) under bounded variance.
@article{SGD_upper_bound,
	author = {S. Ghadimi and G. Lan},
	title = {{Stochastic First and Zeroth-Order Methods for Nonconvex Stochastic Programming}},
	journal = {SIAM Journal on Optimization},
	volume = {23},
	number = {4},
	pages = {2341-2368},
	year = {2013},
	doi = {10.1137/120880811}
}

% SGD cannot attain a precision below the noise for constant step size parameter.
@misc{sgd_prec,
	title={{Non-Convex Stochastic Composite Optimization with Polyak Momentum}}, 
	author={Y. Gao and A. Rodomanov and S. Stich},
	year={2024},
	eprint={2403.02967},
	archivePrefix={arXiv},
	primaryClass={math.OC}
}

%------------------------------------------------ Maximal strong growth assumption ----------------------------------------------------------------------

% gradient bounded and lipshitz +MSG -> convergence of gradient IG with Momentum
@article{MSG_IG,
	author = {P. Tseng},
	title = {{An Incremental Gradient(-Projection) Method with Momentum Term and Adaptive Stepsize Rule}},
	journal = {SIAM Journal on Optimization},
	volume = {8},
	number = {2},
	pages = {506-531},
	year = {1998},
	doi = {10.1137/S1052623495294797},
}

% strongly convex + MSG -> linear cv.
@article{MSG_strong_convex,
	title={{Fast Convergence of Stochastic Gradient Descent under a Strong Growth Condition}},
	author={M. Schmidt and N. LeRoux},
	journal={arXiv preprint arXiv:1308.6370},
	year={2013}
}


%------------------------------------------- Expected strong growth ------------------------------------------------------------------------------------

% Convergence of IG for 'constant step size' 
@article{ESG_IG,
	author = {M. Solodov},
	year = {1998},
	month = {01},
	pages = {23-35},
	title = {{Incremental Gradient Algorithms with Stepsizes Bounded Away from Zero}},
	volume = {11},
	journal = {Computational Optimization and Applications},
	doi = {10.1023/A:1018366000512}
}

% upper bound for SGD in eps^{-2} (overparametrized)
@inproceedings{ESG_upper_bound,
	title={{Fast and Faster Convergence of SGD for Over-Parameterized Models and an Accelerated Perceptron}},
	author={S. Vaswani and F. Bach and M.W. Schmidt},
	booktitle={Proceedings of the 22 nd International Conference on Artificial Intelligence and Statistics (AISTATS) },
	year={2018},
	volume={89},
	url={https://api.semanticscholar.org/CorpusID:52988335}
}

% Non interpolation for decentralized optimization.
@inproceedings{ESG_decentralized,
	title={{Communication-Efficient Learning of Deep Networks from Decentralized Data}},
	author={H.B. McMahan and E. Moore and D. Ramage and S. Hampson and B. Ag{\"u}era},
	booktitle={International Conference on Artificial Intelligence and Statistics},
	year={2016},
	url={https://api.semanticscholar.org/CorpusID:14955348}
}

%------------------------------------------------------ Relaxed growth condition -----------------------------------------------------------------

%Almost sure convergence of SGD.
@article{RG_almost_sure,
	author = {D. Bertsekas and J. Tsitsiklis},
	year = {1999},
	month = {03},
	title = {{Gradient Convergence In Gradient Methods With Errors}},
	volume = {10},
	journal = {SIAM Journal on Optimization},
	doi = {10.1137/S1052623497331063}
}

% Bound in eps^{-2} in a neigborhood of a critical point proportional to the affine noise.
@article{RG_mean,
	author = {{L. Bottou and F.E. Curtis and J. Nocedal}},
	title = {{Optimization Methods for Large-Scale Machine Learning}},
	journal = {SIAM Review},
	volume = {60},
	number = {2},
	pages = {223-311},
	year = {2018},
	doi = {10.1137/16M1080173}
}

%----------------------------------------- Other hypotheses --------------------------------------------------------

% Gradient confusion and PL.
@inproceedings{gradient_confusion,
	author = {K. Sankararaman and S. De and Z. Xu and W. Huang and T. Goldstein},
	year = {2020},
	booktitle={International Conference on Machine Learning (ICML)},
	title = {{The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent}}
}

% Almost Holder -> convergence of the min of the gradient in L2 norm 
@article{sure_smoothness_sgd,
	author={Y. Lei and T. Hu and G. Li and K. Tang},
	journal={IEEE Transactions on Neural Networks and Learning Systems}, 
	title={{Stochastic Gradient Descent for Nonconvex Learning Without Bounded Gradient Assumptions}}, 
	year={2020},
	volume={31},
	number={10},
	pages={4394-4400},
	keywords={Convergence;Training;Stochastic processes;Optimization;Loss measurement;Nickel;Learning systems;Learning theory;nonconvex optimization;Polyak–Łojasiewicz condition;stochastic gradient descent (SGD)},
	doi={10.1109/TNNLS.2019.2952219}
}


% ------------------------------------------------------- Expected smoothness ---------------------------------------------------------------------

% ES first introduction in 2018 (arxiv) but corrections in 2021
@article{ES1,
	author = {R.M. Gower and P. Richtárik and F. Bach},
	title = {{Stochastic Quasi-Gradient Methods: Variance Reduction via Jacobian Sketching}},
	journaltitle = {Mathematical Programming},
	date = {2021},
	month={07},
	doi={https://doi.org/10.1007/s10107-020-01506-0},
	volume={188},
	pages={135–192}
}


% Convex objectives: constant learning rate depending on the precision required. 
@inproceedings{convex_ES,
	title={{SGD: General Analysis and Improved Rates}}, 
	author={R.M. Gower and N. Loizou and X. Qian and A. Sailanbayev and E. Shulgin and P. Richtarik},
	year={2019},
	booktitle={International Conference on Machine Learning (ICML)}
}

% Expected smoothness generalization and biblio of the different assumptions. Prop1: weaknesses of RG assumption.
@article{ES_sgd,
	title={{Better Theory for {SGD} in the Nonconvex World}},
	author={A. Khaled and P. Richtarik},
	journal={Transactions on Machine Learning Research},
	issn={2835--8856},
	year={2023},
	%url={https://openreview.net/forum?id=AU4qHN2VkS},
}

%----------------------------------------------------- PL functions ---------------------------------------------------------------------------------

% Convergence of RR weights (random reshuffling) for cubic summable learning rates.
@article{PL_reshuffle,
	author = {X. Li and A. Milzarek and J. Qiu},
	title = {{Convergence of Random Reshuffling under the Kurdyka–Łojasiewicz Inequality}},
	journal = {SIAM Journal on Optimization},
	volume = {33},
	number = {2},
	pages = {1092-1120},
	year = {2023},
	doi = {10.1137/21M1468048}
}

% KL functions with some sort of affine square noise and artificial descent conditions. 
@inproceedings{sgd_descent_condition,
	title = {{A New Non-Convex Framework to Improve Asymptotical Knowledge on Generic Stochastic Gradient Descent}},
	keywords = {Kurdyka-Lojasiewicz, Stochastic gradient descent, convergence analysis, non-convex optimization},
	author = {J.B. Fest and A. Repetti and E. Chouzenoux},
	year = {2023},
	month = {10},
	day = {23},
	doi = {10.1109/mlsp55844.2023.10285917},
	editor = {D.Comminiello and M.Scarpiniti},
	booktitle = {33rd IEEE International Workshop on Machine Learning for Signal Processing (MLSP)}
}

% Analysis of SGD (with restarting) under expected smoothness noise and global alpha-PL (diminishing step size): upper bound on the values depending on alpha.
% Analysis of a reduced variance algo (PAGER): need full-batch sometimes.
@inproceedings{sgd_global_KL,
	title={{Sharp Analysis of Stochastic Optimization under Global Kurdyka-Lojasiewicz Inequality}},
	author={I. Fatkhullin and J. Etesami and N. He and N. Kiyavash},
	booktitle={Advances in Neural Information Processing Systems},
	year={2022},
	url={https://openreview.net/forum?id=4FSfANJp8Qx}
}

%----------------------------------------- RR is commonly used in practice over sgd --------------------------------------------------------------------------
% RR means random reshuffling

@Inbook{RR_use1,
	author={L. Bottou},
	title={{Stochastic Gradient Descent Tricks}},
	bookTitle={Neural Networks: Tricks of the Trade},
	year={2012},
	publisher={Springer Berlin Heidelberg},
	pages={421--436},
	isbn={978-3-642-35289-8},
	doi={10.1007/978-3-642-35289-8_25}
}

@inproceedings{RR_use_superior,
	title={{Curiously Fast Convergence of some Stochastic Gradient Descent Algorithms}},
	author={L. Bottou},
	year={2009},
	url={https://api.semanticscholar.org/CorpusID:16822133}
}

@inproceedings{RR_use2,
	author = {O. Shamir},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	title = {{Without-Replacement Sampling for Stochastic Gradient Methods}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2016/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf},
	volume = {29},
	year = {2016}
}

% Analysis of RR in convex setting.
@inproceedings{RR_use3,
	title={{SGD without Replacement: Sharper Rates for General Smooth Convex Functions}},
	author={P. Jain and D. Nagaraj and P. Netrapalli},
	booktitle={International Conference on Machine Learning},
	year={2019},
	url={https://api.semanticscholar.org/CorpusID:67877163}
}

%------------------------------------------- Practical/Theorical RR performances over sgd ------------------------------------------------------------



% IG_proximal.
@article{IG_proximal,
	author = {D.P. Bertsekas},
	title = {{Incremental Proximal Methods for Large Scale Convex Optimization}},
	journaltitle = {Mathematical Programming},
	year = {2011},
	month={10},
	doi={https://doi.org/10.1007/s10107-011-0472-0},
	volume={129},
	pages={163–195}
}

% Better upper bound for convex objectives compared to sgd.
@InProceedings{RR_outperforms_convex,
	title = {{Random Shuffling Beats {SGD} after Finite Epochs}},
	author = {J. Haochen and S. Sra},
	booktitle = {Proceedings of the 36th International Conference on Machine Learning},
	pages = 	 {2624--2633},
	year = 	 {2019},
	editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	volume = 	 {97},
	series = 	 {Proceedings of Machine Learning Research},
	month = 	 {06},
	publisher =    {PMLR},
	url = 	 {https://proceedings.mlr.press/v97/haochen19a.html}
}

% Quadratics and smooth strongly convex.
@article{RR_outperforms_quadratics,
	author = {M. Gürbüzbalaban and A. Ozdaglar and P.A. Parrilo},
	title = {{Why Random Reshuffling Beats Stochastic Gradient Descent}},
	journaltitle = {Mathematical Programming},
	year = {2021},
	month={03},
	volume={86},
	pages={49-84},
	doi={https://doi.org/10.1007/s10107-019-01440-w}
}

% RR strongly convex without big number of epochs
@InProceedings{RR_small_epochs,
	title = {{Random Reshuffling: Simple Analysis
	with Vast Improvements}},
	author = {K. Mishchenko and A. Khaled and P. Richtárik},
	booktitle = {Conference on Neural Information Processing Systems (NeurIPS)},
	pages = {12},
	year = 	 {2020}
}

% convergence to a smaller neighborhood than sgd
@INPROCEEDINGS{RR_voisinage,
	author={B. Ying and K. Yuan and S. Vlaski and A.H. Sayed},
	booktitle={Information Theory and Applications Workshop (ITA)}, 
	title={{On the Performance of Random Reshuffling in Stochastic Learning}}, 
	year={2017},
	pages={1-5},
	keywords={Random reshuffling;stochastic gradient descent;mean-square performance;convergence analysis},
	doi={10.1109/ITA.2017.8023470}
}


% RR for non convex and convex under some affine noise. General framework for shuffling gradient.
@article{RR_general_framework,
	author = {M.M. Nguyen and Q. Tran-Dinh and T.D. Phan and P.H. Nguyen and M.D. Van},
	title = {{A Unified Convergence Analysis for Shuffling-Type Gradient Methods}},
	year = {2021},
	issue_date = {January 2021},
	publisher = {JMLR.org},
	volume = {22},
	number = {1},
	issn = {1532-4435},
	journal = {Journal of Machine Learning Research},
	month = {01},
	articleno = {207},
	numpages = {44},
	keywords = {Stochastic gradient algorithm, shuffling-type gradient scheme, sampling without replacement, non-convex finite-sum minimization, strongly convex minimization}
} 

%------------------------------------------------------ Variance reduction Algos ----------------------------------------------------------------------------

% First introduction of the IAG algo -> IG with variance reduction. Global convergence in the quadratic case.
@article{IAG_first,
	author = {D. Blatt and A.O. Hero and H. Gauchman},
	title = {{A Convergent Incremental Gradient Method with a Constant Step Size}},
	journal = {SIAM Journal on Optimization},
	volume = {18},
	number = {1},
	pages = {29-51},
	year = {2007},
	doi = {10.1137/040615961}
}

% Some sort of IAG with uniformly chosen batch. Linear convergence rate for strongly convex functions.
@inproceedings{SAG_first,
	author = {N. LeRoux and M. Schmidt and F. Bach},
	title = {{A Stochastic Gradient Method with an Exponential Convergence Rate for Finite Training Sets}},
	year = {2012},
	publisher = {Curran Associates Inc.},
	address = {Red Hook, NY, USA},
	booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems},
	volume={2},
	pages = {2663–2671},
	numpages = {9},
	location = {Lake Tahoe, Nevada},
	series = {NIPS'12}
}

% First introduction of the SVRG method. Geometric convergence in expectation for sum of convex functions.
% Use the full gradient at the beginning of each epoch and a new hyperparameter m.
@inproceedings{SVRG_first,
	author = {R. Johnson and T. Zhang},
	title = {{Accelerating Stochastic Gradient Descent Using Predictive Variance Reduction}},
	year = {2013},
	publisher = {Curran Associates Inc.},
	address = {Red Hook, NY, USA},
	booktitle = {Proceedings of the 26th International Conference on Neural Information Processing Systems},
	volume={1},
	pages = {315–323},
	numpages = {9},
	location = {Lake Tahoe, Nevada},
	series = {NIPS'13}
}

% Looks like SARAH but uses the first weight in an epoch and not the previous one.
% As SVRG achieves upper bound in n^{2/3}eps^{-2}.
@inproceedings{SCSG,
	author = {L. Lei and C. Ju and J. Chen and M.I. Jordan},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	pages = {},
	publisher = {Curran Associates, Inc.},
	title = {{Non-convex Finite-Sum Optimization Via SCSG Methods}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/81ca0262c82e712e50c580c032d99b60-Paper.pdf},
	volume = {30},
	year = {2017}
}

% Mixture between SAG and SVRG. For sum of convex functions: value convergence for the average iterate.
@inproceedings{SAGA_first,
	author = {A. Defazio and F. Bach and S. Lacoste-Julien},
	title = {{SAGA: a Fast Incremental Gradient Method with Support for Non-Strongly Convex Composite Objectives}},
	year = {2014},
	publisher = {MIT Press},
	address = {Cambridge, MA, USA},
	booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems},
	volume={1},
	pages = {1646–1654},
	numpages = {9},
	location = {Montreal, Canada},
	series = {NIPS'14}
}

% SNVRG (generalization of SVRG). Convergence under average L-smoothness and bounded noise. Optimal bound in n+min(eps^{-3},sqrt(n)eps^{-2}) for particular
% batch size
@article{SNVRG_first,
	author = {D. Zhou and P. Xu and Q. Gu},
	title = {{Stochastic Nested Variance Reduction for Nonconvex Optimization}},
	year = {2020},
	publisher = {JMLR.org},
	volume = {21},
	number = {1},
	issn = {1532-4435},
	journal = {Journal of Machine Learning Research},
	month = {01},
	articleno = {103},
	numpages = {63},
	keywords = {variance reduction, finding local minima, nonconvex optimization}
}

% SARAH algo: full batch at each epoch. Close to RAG-light. Convex analysis. Strongly convex: n+log(eps^{-1}).
@inproceedings{SARAH_first,
	author = {L.M. Nguyen and J. Liu and K. Scheinberg and M. Tak\'{a}\v{c}},
	title = {{SARAH: a Novel Method for Machine Learning Problems using Stochastic Recursive Gradient}},
	year = {2017},
	publisher = {JMLR.org},
	booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
	pages = {2613–2621},
	numpages = {9},
	location = {Sydney, NSW, Australia},
	series = {ICML'17}
}

% GEOM-SARAH. SARAH with the lengh of the outer loop sampled from a geometric distribution (more hyperparameters).
% upper bound in n^{3/2}+sqrt(n)eps^{-2} for smooth PL functions.
@article{GEOM_SARAH,
	author = {S. Horv\'{a}th and L. Lei and P. Richt\'{a}rik and M.I. Jordan},
	title = {{Adaptivity of Stochastic Gradient Methods for Nonconvex Optimization}},
	journal = {SIAM Journal on Mathematics of Data Science},
	volume = {4},
	number = {2},
	pages = {634-648},
	year = {2022},
	doi = {10.1137/21M1394308}
}

% ZERO-SARAH. SARAH without full batch gradient using some momentum recursive equation. But it seems (not discussed in the paper) that the 
% storage is the same as IAG.
@inproceedings{ZERO_SARAH,
	title={{Zero{SARAH}: Efficient Nonconvex Finite-Sum Optimization with Zero Full Gradient Computations}},
	author={Z. Li and S. Hanzely and P. Richt{\'a}rik},
	year={2022},
	booktitle={International Conference on Learning Representations (ICLR)},
	url={https://openreview.net/forum?id=LQnyIk5dUA}
}

% STORM method: Momentum with variance reduction. Bound for expected gradient for bounded noise and gradient.
@inproceedings{STORM_first,
	author = {A. Cutkosky and F. Orabona},
	title = {{Momentum-Based Variance Reduction in Non-Convex SGD}},
	year = {2019},
	publisher = {Curran Associates Inc.},
	address = {Red Hook, NY, USA},
	booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
	articleno = {1365},
	numpages = {10}
}

% SPIDER method: SARAH algo applied to NGD. Optimal upper bound in the finite sum-case as SNVRG. Sufficient decrease for batch size depending on eps.
@inproceedings{SPIDER_first,
	author = {C. Fang and C.J. Li and Z. Lin and T. Zhang},
	title = {{SPIDER: Near-Optimal Non-Convex Optimization via Stochastic Path Integrated Differential Estimator}},
	year = {2018},
	publisher = {Curran Associates Inc.},
	address = {Red Hook, NY, USA},
	booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
	pages = {687–697},
	numpages = {11},
	location = {Montr\'{e}al, Canada},
	series = {NIPS'18}
}

%SSRGD. Mix between SARAH and perturbed GD to deal with saddle points.
% Optimal bound for first order stationary bound with b=sqrt(n).
% Bound for finding second order stationary points.
@inproceedings{SSRGD,
	title={{SSRGD: Simple Stochastic Recursive Gradient Descent for Escaping Saddle Points}}, 
	author={Z. Li},
	year={2019},
	booktitle={Neural Information Processing Systems}
}

% PAGE: switch between GD and SARAH with probability p: optimal upper bound for the gradient but for a iterate chosen uniformly between all the iterates.
% Lower bound for gradient complexity in n+L\sqrt(n)eps^{-2}.
@inproceedings{PAGE,
	author = {Z. Li and H. Bao and X. Zhang and P. Richtárik},
	year = {2021},
	title = {{PAGE: A Simple and Optimal Probabilistic Gradient Estimator for Nonconvex Optimization}},
	booktitle={International Conference on Machine Learning (ICML)}
}

% Katyusha: mix SVRG with a negative Momentum to achieve acceleration on strongly convex functions.
@inproceedings{Katyusha,
	author = {Z. Allen-Zhu},
	title = {{Katyusha: the First Direct Acceleration of Stochastic Gradient Methods}},
	year = {2017},
	isbn = {9781450345286},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3055399.3055448},
	doi = {10.1145/3055399.3055448},
	booktitle = {Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing},
	pages = {1200–1205},
	numpages = {6},
	keywords = {accelerated gradient descent, acceleration, first-order method, momentum, stochastic gradient descent},
	location = {Montreal, Canada},
	series = {STOC 2017}
}

% DIAG: better upper bound than IAG for strongly convex function using both average on the gradient and the weights.
@article{DIAG,
	author = {A. Mokhtari and M. G\"{u}rb\"{u}zbalaban and A. Ribeiro},
	title = {{Surpassing Gradient Descent Provably: A Cyclic Incremental Method with Linear Convergence Rate}},
	journal = {SIAM Journal on Optimization},
	volume = {28},
	number = {2},
	pages = {1420-1447},
	year = {2018},
	doi = {10.1137/16M1101702}
}

% Analysis of SVRG and SAGA with random reshuffling.
% AVRG: sort of SAGA without full batch by using an incremental gradient estimator (like previous RAG_L): strongly convex analysis.
@inproceedings{AVRG,
	title={{Variance-Reduced Stochastic Learning under Random Reshuffling}}, 
	author={B. Ying and K. Yuan and A.H. Sayed},
	year={2020},
	booktitle={IEEE Transactions on Signal Processing},
	volume={68},
	pages={1390-1408}
}

% RR SARAH: very close update rule compared to the previous version of RAG_L
% Analysis for strongly convex functions and delta-similarity assumption (looks like bound variance).
@article{RR_SARAH,
	author = {A. Beznosikov and M. Takáč},
	title = {{Random-Reshuffled SARAH Does Not Need Full Gradient Computations}},
	journaltitle = {Optimization Letters},
	year = {2024},
	month={04},
	doi={https://doi.org/10.1007/s11590-023-02081-x},
	volume={18},
	pages={727-749}
}

%Adam variance reduction
@article{adam_variance1,
	title={{Adam+: A Stochastic Method with Adaptive Variance Reduction}},
	author={M. Liu and W. Zhang and F. Orabona and T. Yang},
	journal={arXiv:2011.11985},
	year={2020}
}

% General analysis, most interesting
@inbook{variance_reduced_general,
	author = {T. Sun and Y. Sun and D. Li and Q. Liao},
	title = {{General Proximal Incremental Aggregated Gradient Algorithms: Better and Novel Results under General Scheme}},
	year = {2019},
	publisher = {Curran Associates Inc.},
	address = {Red Hook, NY, USA},
	booktitle = {PInternational Conference on Neural Information Processing Systems (NeurIPS)},
	articleno = {90},
	numpages = {11}
}

%---------------------------------------------------- Stochastic Armijo ---------------------------------------------------------

% Simple Armijo on each batch.
% Analysis under interpolated assumptions.
@inproceedings{stochastic_armijo,
	author = {S. Vaswani and A. Mishkin and I. Laradji and M. Schmidt and G. Gidel and S. Lacoste-Julien},
	title = {{Painless Stochastic Gradient: Interpolation, Line-search, and Convergence Rates}},
	year = {2019},
	publisher = {Curran Associates Inc.},
	address = {Red Hook, NY, USA},
	booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
	articleno = {335},
	numpages = {14}
}

% Two articles with adaptive batch size to be representative enough of the full gradient.

@article{armijo_batch_size,
	author = {C. Paquette and K. Scheinberg},
	title = {{A Stochastic Line Search Method with Expected Complexity Analysis}},
	journal = {SIAM Journal on Optimization},
	volume = {30},
	number = {1},
	pages = {349-376},
	year = {2020},
	doi = {10.1137/18M1216250}
}

@article{armijo_variance_reduction,
	author = {G. Franchini and F. Porta and V. Ruggiero and al.},
	title = {{A Line Search Based Proximal Stochastic Gradient Algorithm with Dynamical Variance Reduction}},
	journaltitle = {Journal of Scientific Computing},
	year = {2023},
	month={12},
	doi={https://doi.org/10.1007/s10915-022-02084-3}
}

% Gaussian process uses as a surrogate to approximate the full-batch function.
% No guarantee. Suitable for high dimension ?
@article{armijo_GP,
	author = {M. Mahsereci and P. Hennig},
	title = {{Probabilistic Line Searches for Stochastic Optimization}},
	year = {2017},
	publisher = {JMLR.org},
	volume = {18},
	number = {1},
	issn = {1532-4435},
	journal = {Journal of Machine Learning Research (JMLR)},
	month = {},
	pages = {4262–4320},
	numpages = {59},
	keywords = {Bayesian optimization, Gaussian processes, learning rates, line searches, stochastic optimization}
}

%Most interesting idea in the litterature (not yet published) to generalize Armijo: taylor expansion in L2 space
% Problem: need gaussian process.
@misc{armijo_L2,
	title={{Random Function Descent}}, 
	author={F. Benning and L. Döring},
	year={2023},
	eprint={2305.01377},
	archivePrefix={arXiv},
	primaryClass={math.OC}
}

% --------------------------------------------------- Stochastic Trust Region --------------------------------------------------

% For PL functions, need the knowlege of the affine constants of the noise.
@article{sto_TR_affine_noise,
	author = {F.E. Curtis and K. Scheinberg and R. Shi},
	title = {{A Stochastic Trust Region Algorithm Based on Careful Step Normalization}},
	journal = {INFORMS Journal on Optimization},
	volume = {1},
	number = {3},
	pages = {200-220},
	year = {2019},
	doi = {10.1287/ijoo.2018.0010}
}

% noise bounded p.s
@article{sto_TR_bounded,
	author = {N. Li and D. Xue and W. Sun and J. Wang},
	year = {2019},
	month = {02},
	pages = {1-10},
	title = {{A Stochastic Trust Region Method for Unconstrained Optimization Problems}},
	volume = {2019},
	journal = {Mathematical Problems in Engineering},
	doi = {10.1155/2019/8095054}
}

% Need the stochastic gradient to be representative enough of the full gradient (strong assumptions). 
@Article{sto_TR_representative,
	author = {X. Wang and Y. Yuan},
	title = {{Stochastic Trust-Region Methods with Trust-Region Radius Depending on Probabilistic Models}},
	journal = {Journal of Computational Mathematics},
	year = {2022},
	volume = {40},
	number = {2},
	pages = {294--334},
	issn = {1991-7139},
	doi = {https://doi.org/10.4208/jcm.2012-m2020-0144},
	url = {http://global-sci.org/intro/article_detail/jcm/20188.html}
}

% Sampling strategy (varying batch size) to solve the previous problem.
@article{sto_TR_sampling,
	author = {S. Bellavia and N. Krejić and B. Morini and al.},
	title = {{A Stochastic First-Order Trust-Region Method with Inexact Restoration for Finite-Sum Minimization}},
	journaltitle = {Computational Optimization and Applications},
	year = {2023},
	month={01},
	doi={https://doi.org/10.1007/s10589-022-00430-7},
	volume={84},
	pages={53–84}
}

% ----------------------------------- Other methods to reduce hyperparameters ---------------------------------------------------- 

% Two articles (practical and theoretical) about varying batch size.

@inproceedings{adaptive_batch,
	title={{Stochastic Optimization with Adaptive Batch Size : Discrete Choice Models as a Case Study}},
	author={G.Lederrey},
	year={2019},
	url={https://api.semanticscholar.org/CorpusID:201697519}
}

% Scalar product test to detect if a selected sample is a descent direction with high probability.
@article{adaptive_sampling,
	author = {R. Bollapragada and R. Byrd and J. Nocedal},
	title = {{Adaptive Sampling Strategies for Stochastic Optimization}},
	journal = {SIAM Journal on Optimization},
	volume = {28},
	number = {4},
	pages = {3312-3343},
	year = {2018},
	doi = {10.1137/17M1154679}
}

% Some form of free parameter sgd but you need to lauch some intern optimization loops (expensive and huge storage)
% Assumption: almost sure bounded noise.
@misc{sgd_lauched,
	title={{How Free is Parameter-Free Stochastic Optimization?}}, 
	author={A. Attia and T. Koren},
	year={2024},
	eprint={2402.03126},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

% Impossibility result to find a free hyperparameter sgd under the bounded variance assumption for the general problem (stochastic optimization).
% Maybe possible in the finite sum case.
@misc{adaptive_sgd_impossible,
	title={{Tuning-Free Stochastic Optimization}}, 
	author={A. Khaled and C. Jin},
	year={2024},
	eprint={2402.07793},
	archivePrefix={arXiv},
	primaryClass={math.OC}
}

CMA algorithms: do not need assumptions on data distribution but use full gradient in some linesearches.

@misc{CMA,
	title={{Convergence under Lipschitz Smoothness of Ease-Controlled Random Reshuffling Gradient Algorithms}}, 
	author={G. Liuzzi and L. Palagi and R. Seccia},
	year={2023},
	eprint={2212.01848},
	archivePrefix={arXiv},
	primaryClass={math.OC}
}
@misc{CMA_Light,
	title={{CMA Light: a Novel Minibatch Algorithm for Large-Scale Non Convex Finite Sum Optimization}}, 
	author={C. Coppola and G. Liuzzi and L. Palagi},
	year={2023},
	eprint={2307.15775},
	archivePrefix={arXiv},
	primaryClass={math.OC}
}

%----------------------------------------------- Diffusion models -----------------------------------------------------------

% Article establishing the SDE and check it experimentally
@inproceedings{SDE,
	title={{On the Validity of Modeling SGD with Stochastic Differential Equations (SDEs)}},
	author={Z. Li and S. Malladi and S. Arora},
	booktitle={Neural Information Processing Systems},
	year={2021},
	url={https://api.semanticscholar.org/CorpusID:232035583}
}

% Comparison between gradient flow, SDE with constant diffusion and matrix diffusion (depending on the distribution of data: gaussian, ...).
@article{SDE_comparison,
	author  = {S. Ankirchner and S. Perko},
	title   = {{A Comparison of Continuous-Time Approximations to Stochastic Gradient Descent}},
	journal = {Journal of Machine Learning Research},
	year    = {2024},
	volume  = {25},
	number  = {13},
	pages   = {1--55},
	url     = {http://jmlr.org/papers/v25/23-0237.html}
}

% Same thing for Nesterov algo and some sort of central limit theorem for gd SDE.
@article{SDE_Nesterov,
	author = {Y. Wang and S. Wu},
	title = {{Asymptotic Analysis via Stochastic Differential Equations of Gradient Descent Algorithms in Statistical and Computational Paradigms}},
	year = {2020},
	publisher = {JMLR.org},
	volume = {21},
	number = {1},
	issn = {1532-4435},
	month = {01},
	articleno = {199},
	numpages = {103},
	keywords = {acceleration, gradient descent, gradient flow central limit theorem, joint asymptotic analysis, joint computational and statistical analysis, Lagrangian flow central limit theorem, mini-batch, optimization, ordinary differential equation, stochastic differential equation, stochastic gradient descent, weak convergence}
}

% Convergence result for SDE of the form -grad*dt+beta_t*dBt where the function is Lojasiewicz and beta_t = O(t^{-alpha}) with alpha>1 
% Divergence counter example for alpha=1.
% No real understanding of the diffusion SDE distribution except for noise of the form t^{-alpha}
@article{SDE_Lojasiewicz,
	title = {{Cooling Down Stochastic Differential Equations: Almost Sure Convergence}},
	journal = {Stochastic Processes and their Applications},
	volume = {152},
	pages = {289-311},
	year = {2022},
	issn = {0304-4149},
	doi = {https://doi.org/10.1016/j.spa.2022.06.020},
	url = {https://www.sciencedirect.com/science/article/pii/S0304414922001557},
	author = {S. Dereich and S. Kassing},
	keywords = {Stochastic gradient flow, Brownian particle, Cooling down, Łojasiewicz-inequality, Almost sure convergence},
}

% Extension of the modified equations framework for SDEs.
@article{SDE_modified,
	author  = {Q. Li and C. Tai and W. E},
	title   = {{Stochastic Modified Equations and Dynamics of Stochastic Gradient Algorithms I: Mathematical Foundations}},
	journal = {Journal of Machine Learning Research},
	year    = {2019},
	volume  = {20},
	number  = {40},
	pages   = {1--47},
	url     = {http://jmlr.org/papers/v20/17-526.html}
}

% SDE for variance reduced algos without gaussian assumptions.
% Analysis of convergence (gradient or value in expectation) under bounded noise, weakly convex hypotheses.  
@inproceedings{SDE_variance_reduction,
	author = {A. Orvieto and A. Lucchi},
	title = {{Continuous-Time Models for Stochastic Optimization Algorithms}},
	year = {2019},
	publisher = {Curran Associates Inc.},
	address = {Red Hook, NY, USA},
	booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
	articleno = {1130},
	numpages = {13}
}

% Analysis of the pde associated to the brownian gd approximation with non constant diffusion matrix.
% The continuous model does not minimize the function itself but another one for non-isotropic noise.
@inproceedings{SDE_edp,
	author={P. Chaudhari and S. Soatto},
	booktitle={Information Theory and Applications Workshop (ITA)}, 
	title={{Stochastic Gradient Descent Performs Variational Inference, Converges to Limit Cycles for Deep Networks}}, 
	year={2018},
	pages={1-10},
	keywords={Steady-state;Trajectory;Limit-cycles;Entropy;Force;Mathematical model;Temperature},
	doi={10.1109/ITA.2018.8503224}
}

% Interpretation of sgd as a riemannian gd with a metric tensor given by the diffusion matrix.
@article{sgd_Riemannian,
	AUTHOR = {R. Fioresi and P. Chaudhari and S. Soatto},
	TITLE = {{A Geometric Interpretation of Stochastic Gradient Descent Using Diffusion Metrics}},
	JOURNAL = {Entropy},
	VOLUME = {22},
	YEAR = {2020},
	NUMBER = {1},
	ARTICLE-NUMBER = {101},
	URL = {https://www.mdpi.com/1099-4300/22/1/101},
	PubMedID = {33285876},
	ISSN = {1099-4300},
	DOI = {10.3390/e22010101}
}

% Stability results for stochastic differential equations.
@book{book_stability_SDE,
	author = {R. Khasminskii},
	title = {{Stochastic Stability of Differential Equations}},
	year = {2011},
	month={09},
	doi={https://doi.org/10.1007/978-3-642-23280-0},
	isbn={978-3-642-23279-4},
	pages={342},
	publisher={Springer Berlin, Heidelberg}
}

% sde to understand batch and learning rate role (adam, rms)
@inproceedings{malladi_adam,
	title={{On the {SDE}s and Scaling Rules for Adaptive Gradient Algorithms}},
	author={S. Malladi and K. Lyu and A. Panigrahi and S. Arora},
	booktitle={Advances in Neural Information Processing Systems},
	year={2022},
	url={https://openreview.net/forum?id=F2mhzjHkQP}
}

% phase oscillation
@article{Kunin_2023,
	title={{The Limiting Dynamics of SGD: Modified Loss, Phase-Space Oscillations, and Anomalous Diffusion}},
	volume={36},
	ISSN={1530-888X},
	url={http://dx.doi.org/10.1162/neco_a_01626},
	DOI={10.1162/neco_a_01626},
	number={1},
	journal={Neural Computation},
	publisher={MIT Press},
	author={D. Kunin and B. Sagastuy and L. Gillespie and E. Margalit and H. Tanaka and S. Ganguli and D. Yamins},
	year={2023},
	month=dec, pages={151–174}
}

% flat minima sgd
@Article{Wojtowytsch2023,
	author={S. Wojtowytsch},
	title={{Stochastic Gradient Descent with Noise of Machine Learning Type Part II: Continuous Time Analysis}},
	journal={Journal of Nonlinear Science},
	year={2023},
	month={Nov},
	day={15},
	volume={34},
	number={1},
	pages={16},
	issn={1432-1467},
	doi={10.1007/s00332-023-09992-0},
	url={https://doi.org/10.1007/s00332-023-09992-0}
}

% batch normalization sgd
@inproceedings{li2020reconciling,
	title={{Reconciling Modern Deep Learning with Traditional Optimization Analyses: The Intrinsic Learning Rate}},
	author={Z. Li and K. Lyu and S. Arora},
	booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
	volume={33},
	pages={14544--14555},
	year={2020}
}

%escape saddle points and shape min
@misc{hu2018diffusion,
	title={{On the Diffusion Approximation of Nonconvex Stochastic Gradient Descent}}, 
	author={W. Hu and C.J. Li and L. Li and J.G. Liu},
	year={2018},
	eprint={1705.07562},
	archivePrefix={arXiv},
	primaryClass={stat.ML}
}

%favours exponentially flat minima
@inproceedings{flat_minima_exponential,
	title={{A Diffusion Theory For Deep Learning Dynamics: Stochastic Gradient Descent Exponentially Favors Flat Minima}},
	author={Z. Xie and I. Sato and M. Sugiyama},
	booktitle={International Conference on Learning Representations},
	year={2021},
	url={https://openreview.net/forum?id=wXgk_iCiYGo}
}

% linear time escaping saddles morse function
@misc{yang2020fast,
	title={{On the Fast Convergence of Random Perturbations of the Gradient Flow}}, 
	author={J. Yang and W. Hu and C.J. Li},
	year={2020},
	eprint={1706.00837},
	archivePrefix={arXiv},
	primaryClass={math.PR}
}

@inproceedings{sgd_implicit_regularisation,
	author = {A. Ali and E. Dobriban and R.J. Tibshirani},
	title = {{The Implicit Regularization of Stochastic Gradient Flow for Least Squares}},
	year = {2020},
	publisher = {JMLR.org},
	booktitle = {International Conference on Machine Learning (ICML)},
	articleno = {23},
	numpages = {12},
	series = {ICML'20}
}

@inproceedings{sgd_implicit_regularisation2,
	title={{What Happens after {SGD} Reaches Zero Loss? --A Mathematical Framework}},
	author={Z. Li and T. Wang and S. Arora},
	booktitle={International Conference on Learning Representations},
	year={2022},
	url={https://openreview.net/forum?id=siCt4xZn5Ve}
}

%sgd distributed opti
@article{sgd_distributed,
	author = {N. Boffi and J.E. Slotine},
	title = {{A Continuous-Time Analysis of Distributed Stochastic Gradient}},
	journal = {Neural Computation},
	volume = {32},
	number = {1},
	pages = {36-96},
	year = {2020},
	month = {01},
	issn = {0899-7667},
	doi = {10.1162/neco_a_01248},
	url = {https://doi.org/10.1162/neco\_a\_01248},
	eprint = {https://direct.mit.edu/neco/article-pdf/32/1/36/1864518/neco\_a\_01248.pdf},
}


%------------------------------------------------- Markovian Switching -------------------------------------------------------

% Introduction of a markov switching system to analyse SGD. Analysis in the continuous case for strongly convex functions.
% Constant and diminishing step size.
@article{sgd_markov1,
	author = {J. Latz},
	title = {{Analysis of Stochastic Gradient Descent in Continuous Time}},
	journaltitle = {Statistics and Computing},
	year = {2021},
	month={05},
	volume={31},
	pages={39},
	doi={https://doi.org/10.1007/s11222-021-10016-8}
}

% Book on deterministic switching dynamical systems
@book{book_switch_determinist,
	author = {Z. Sun and S. Ge},
	year = {2011},
	month = {01},
	title = {{Stability Theory of Switched Dynamical Systems. Springer, London}},
	isbn = {978-0-85729-255-1},
	doi = {10.1007/978-0-85729-256-8}
}

% The most complete book on non uniform markovian switching sde (even with diffusion): notion of Lyapunov functions, stability theory
% ergodic theorems, associated pde (transport equation).
@book{book_markov_switch,
	author = {G. Yin and C. Zhu},
	year = {2010},
	month = {01},
	title = {{Hybrid Switching Diffusions: Properties and Applications}},
	volume = {63},
	isbn = {978-1-4419-1106-3},
	doi = {10.1007/978-1-4419-1105-6}
}

% First definition of PDMP
@article{PDMP_Davis,
	ISSN = {00359246},
	URL = {http://www.jstor.org/stable/2345677},
	author = {M.H.A. Davis},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	number = {3},
	pages = {353--388},
	publisher = {[Royal Statistical Society, Wiley]},
	title = {{Piecewise-Deterministic Markov Processes: A General Class of Non-Diffusion Stochastic Models}},
	urldate = {2024-04-25},
	volume = {46},
	year = {1984}
}

% Recent advances on PDMP: the stationary distribution could be characterized by a complex inclusion ODE or a control system.
@article{PDMP_recent_inclusion,
	author = {R. Azaïs and J.B. Bardet and A. Génadot and N. Krell and P.A. Zitt},
	title = {{Piecewise Deterministic Markov Process — Recent Results}},
	DOI= {10.1051/proc/201444017},
	url= {https://doi.org/10.1051/proc/201444017},
	journal = {ESAIM: Proc.},
	year = {2014},
	volume = {44},
	pages = {276-290}
}
@article{PDMP_support,
	doi = {10.1088/1361-6544/aa7e94},
	url = {https://dx.doi.org/10.1088/1361-6544/aa7e94},
	year = {2017},
	month = {08},
	publisher = {IOP Publishing},
	volume = {30},
	number = {9},
	pages = {3400},
	author = {M. Benaïm and F. Colonius and R. Lettau},
	title = {{Supports of Invariant Measures for Piecewise Deterministic Markov Processes}},
	journal = {Nonlinearity}
}

% Convergence of markovian switching ODE to an average ODE when the collision frequency tends to infinity.
@article{fast_switching,
	title = {{Dynamical Systems under Random Perturbations with Fast Switching and Slow Diffusion: Hyperbolic Equilibria and Stable Limit Cycles}},
	journal = {Journal of Differential Equations},
	volume = {293},
	pages = {313-358},
	year = {2021},
	issn = {0022-0396},
	doi = {https://doi.org/10.1016/j.jde.2021.05.032},
	url = {https://www.sciencedirect.com/science/article/pii/S0022039621003223},
	author = {N.H. Du and A. Hening and D.H. Nguyen and G. Yin},
	keywords = {Random perturbation, Random dynamical system, Invariant probability measure, Hybrid diffusion, Rapid switching, Limit cycle}
}

% Analysis of the edp associated to the markovian swithching gd with constant collision parameter.
% Existence and unicity of solutions
% Existence and unicity of a stationary distribution for strongly convex functions
% Existence of a stationary distribution for non-convex functions but no unicity.
@article{switch_kinetic,
	title = {{Analysis of Kinetic Models for Label Switching and Stochastic Gradient Descent}},
	journal = {Kinetic and Related Models},
	volume = {16},
	number = {5},
	pages = {717-747},
	year = {2023},
	issn = {1937-5093},
	doi = {10.3934/krm.2023005},
	url = {https://www.aimsciences.org/article/id/63e390a15f0ada745978adc1},
	author = {M. Burger and A. Rossi},
	keywords = {Integro-partial differential equations, Label switching, Run–and–tumble particles, Stochastic gradient descent, Machine learning}
}

% ----------------------------------------------- Splitting Schemes -----------------------------------------------------------

% Link between sgd and Lie-Trotter splitting. Apply basic splitting with high order RK. Apply this to convex problems: linear/softmax regression.
@inproceedings{splitting_sgd,
	author = {D. Merkulov and I. Oseledets},
	year = {2020},
	month = {04},
	pages = {12},
	booktitle={International Conference on Learning Representations (ICLR)},
	title = {{Stochastic Gradient Algorithms from ODE Splitting Perspective}}
}

% Reviews of classical splitting schemes for ODEs.
@article{splitting_ode_review,
	author = {S. Blanes and F. Casas and A. Murua},
	year = {2009},
	month = {01},
	pages = {57},
	title = {{Splitting and Composition Methods in the Numerical Integration of Differential Equations}},
	volume = {45},
	journal = {Bol. Soc. Esp. Mat. Apl.}
}
@misc{splitting_ode_review2,
	title={{Splitting Methods for Differential Equations}}, 
	author={S. Blanes and F. Casas and A. Murua},
	year={2024},
	eprint={2401.01722},
	archivePrefix={arXiv},
	primaryClass={mBilel_thesisA}
}

% Splitting methods for composite optimization (composition of gradient and proximal operator: not related to numerical splitting schemes until ...)
% Analysis in the convex case of forward-backward splitting, Douglas-Rachford, ADMM.
@Inbook{splitting_proximal,
	author={D. Davis and W. Yin},
	title={{Convergence Rate Analysis of Several Splitting Schemes}},
	bookTitle={Splitting Methods in Communication, Imaging, Science, and Engineering},
	year={2016},
	publisher={Springer International Publishing},
	pages={115--163},
	isbn={978-3-319-41589-5"},
	doi={10.1007/978-3-319-41589-5_4},
	url={https://doi.org/10.1007/978-3-319-41589-5_4}
}

% Same as the two previous ones.
@book{book_splitting,
	editors = {R. Glowinski and S.J. Osher and W. Yin},
	title = {{Splitting Methods in Communication, Imaging, Science, and Engineering}},
	year = {2017},
	month={07},
	doi={https://doi.org/10.1007/978-3-319-41589-5},
	isbn={978-3-319-41587-1},
	pages={820},
	publisher={Springer Cham}
}

% Interpretation of composite objective splitting as numerical splitting of ODEs.
@article{splitting_proximal_ode,
	title = {{Gradient Flows and Proximal Splitting Methods: A Unified View on Accelerated and Stochastic Optimization}},
	author = {G. França and D.P. Robinson and R. Vidal},
	journal = {Physical Review E},
	volume = {103},
	issue = {5},
	pages = {053304},
	numpages = {19},
	year = {2021},
	month = {05},
	publisher = {American Physical Society},
	doi = {10.1103/PhysRevE.103.053304},
	url = {https://link.aps.org/doi/10.1103/PhysRevE.103.053304}
}

% The rebalanced splitting scheme. Counter-example showing that classical splittig does not preserve steady states or create artificial states.
@article{rebalanced_splitting,
	ISSN = {00361429},
	URL = {http://www.jstor.org/stable/24511532},
	author = {R.L. Speth and W.H. Green and S. Macmanara and G. Strang},
	journal = {SIAM Journal on Numerical Analysis},
	number = {6},
	pages = {3084--3105},
	publisher = {Society for Industrial and Applied Mathematics},
	title = {{Balanced Splitting AND Rebalanced Splitting}},
	urldate = {2024-04-25},
	volume = {51},
	year = {2013}
}

% -------------------------------- SGD helps escaping saddle points ?

@inproceedings{sgd_escape1,
	author = {C. Fang and Z. Lin and T. Zhang},
	title = {{Sharp Analysis for Nonconvex SGD Escaping from Saddle Points}},
	booktitle = {Annual Conference on Learning Theory},
	year = {2019},
	volume={99},
	pages={1-43}
}


@InProceedings{sgd_escape2,
	title = {{Escaping Saddles with Stochastic Gradients}},
	author = {H. Daneshmand and J. Kohler and A. Lucchi and T. Hofmann},
	booktitle = {Proceedings of the 35th International Conference on Machine Learning},
	pages = {1155--1164},
	year = 	{2018},
	volume = {80},
	series = {Proceedings of Machine Learning Research},
	month = {07},
	publisher = {PMLR},
	url = {https://proceedings.mlr.press/v80/daneshmand18a.html},
}

@InProceedings{sgd_escape3,
	title = {{Escaping Saddle Points with Adaptive Gradient Methods}},
	author = {M. Staib and S. Reddi and S. Kale and S. Kumar and S. Sra},
	booktitle = {International Conference on Machine Learning (ICML)},
	pages = {10},
	year = 	{2019},
}

@inproceedings{sgd_escape4,
	author = {A. Roy and K. Balasubramanian and S. Ghadimi and P. Mohapatra},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
	pages = {12414--12425},
	publisher = {Curran Associates, Inc.},
	title = {{Escaping Saddle-Point Faster under Interpolation-like Conditions}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/92a08bf918f44ccd961477be30023da1-Paper.pdf},
	volume = {33},
	year = {2020}
}

------------------------------------------------------------

%-------------------------------- SGD better generalization ------------------------------------------------------------------------

@inproceedings{sgd_gen1,
	author = {M. Hardt and B. Recht and Y. Singer},
	booktitle = {International Conference on Machine
	Learning (ICML)},
	pages = {12414--12425},
	publisher = {Curran Associates, Inc.},
	title = {{Train Faster, Generalize Better: Stability of Stochastic Gradient Descent}},
	volume = {48},
	year = {2016}
}

@article{sgd_gen2,
	author = {A. Ramezani-Kebrya and K. Antonakopoulos and V.  Cevher and A. Khisti and B. Liang},
	journaltitle = {Journal of Machine Learning Research (JMLR)},
	pages = {1-56},
	title = {{On the Generalization of Stochastic
	Gradient Descent with Momentum}},
	year = {2024}
}

@inproceedings{sgd_gen3,
	author = {S.L. Smith and E. Elsen and S. De},
	booktitle = {International Conference on Machine
	Learning (ICML)},
	pages = {10},
	title = {{On the Generalization Benefit of Noise in Stochastic Gradient Descent}},
	year = {2020}
}

@inproceedings{sgd_gen4,
	author = {I. Amir and T. Koren and R. Livni},
	booktitle = {Annual Conference on Learning Theory},
	title = {{SGD Generalizes Better Than GD
	(And Regularization Doesn’t Help)}},
	year = {2021},
	volume={134},
	pages={1-30}
}

@article{IMEX,
	title = {IMEX Runge–Kutta schemes for reaction–diffusion equations},
	journal = {Journal of Computational and Applied Mathematics},
	volume = {215},
	number = {1},
	pages = {182-195},
	year = {2008},
	issn = {0377-0427},
	doi = {https://doi.org/10.1016/j.cam.2007.04.003},
	url = {https://www.sciencedirect.com/science/article/pii/S0377042707001951},
	author = {Toshiyuki Koto},
	keywords = {IMEX schemes, Reaction–diffusion equations, Stability analysis}
}
