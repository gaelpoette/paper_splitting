\documentclass[a4paper,11pt]{report}
%draft sert à compiler sans charger les images, c'est beaucoup moins long
% charger des modules à utiliser ...
\usepackage[english]{babel}
\usepackage[latin1]{inputenc}
\usepackage{amsmath}
\usepackage{fullpage}
\usepackage{delarray}
\usepackage{afterpage}
\usepackage{graphicx}
\usepackage{makeidx}
\usepackage{xcolor}

%\usepackage[T1]{fontenc}
\usepackage{latexsym}
\usepackage{amsfonts}
\usepackage{amssymb}

% c'est moche mais c'est pratique pour repérer les liens morts etc...
%\usepackage{hyperref}
\usepackage{backref}
\usepackage{setspace}
\singlespacing

\bibliographystyle{plain}

\usepackage{listings}
\usepackage{calc}
\usepackage{epsfig}



% NOUVEAUX ENVIRONNEMENTS
%\newtheorem{theoreme}{Théorème}[chapter]
%\newtheorem{corollaire}{Corollaire}[chapter]
%\newtheorem{lemme}{Lemme}[chapter]
%\newtheorem{proposition}{Proposition}[chapter]
%\newtheorem{definition}{Définition}[chapter]
%\newtheorem{propriete}{Propriété}[chapter]
%\newtheorem{hypotheses}{Hypothèses}[chapter]
\newenvironment{preuve}[1][Preuve]{\begin{trivlist}\item[\hskip \labelsep {\bfseries #1}]}{\hfill$\blacksquare$ \end{trivlist}}
\newenvironment{remarque}[1][Remarque]{\begin{trivlist}\item[\hskip \labelsep {\bfseries #1}]}{\hfill$\bullet$ \end{trivlist}}
\newenvironment{temporaire}[1][Commentaire temporaire]{\begin{trivlist}\item[\hskip \labelsep {\bfseries #1}]}{\hfill\checkmark \end{trivlist}}

\newcommand{\sousparagraphe}[1]{\textbf{\textit{#1}}}
\newcommand{\cacher}[1]{}
\newcommand{\mbf}[1]{{\mathversion{bold} #1}}
\newcommand{\Frac}[2]{\frac{\displaystyle #1}{\displaystyle #2}}
\newcommand{\Lim}[2]{\displaystyle \lim_{#1}{#2}}
\newcommand{\Int}{\displaystyle \int}

\newcommand{\INT}[4]{\Int_{#1}^{#2} #3 \: \textnormal{d}#4}
\newcommand{\Sum}{\displaystyle \sum}
\newcommand{\Prod}{\displaystyle \prod}
\newcommand{\SUM}[3]{\Sum_{#1}^{#2} #3}
\newcommand{\PD}[3]{\left.\Frac{\partial #1}{\partial #2}\right|_{#3}}
\newcommand{\TD}[2]{\Frac{\textnormal{d} #1}{\textnormal{d} #2}}
\newcommand{\dif}[1]{\textnormal{d} #1}
\newcommand{\pdt}{\partial_t}
\newcommand{\pdx}{\partial_x}
\newcommand{\pdy}{\partial_y}
\newcommand{\pdz}{\partial_z}
\newcommand{\pdX}{\partial_X}
\newcommand{\pdm}{\partial_m}
\newcommand{\pdtt}{\partial_{t^2}^2}
\newcommand{\pdxx}{\partial_{x^2}^2}
\newcommand{\pdxi}{\partial_{x_i}}
\newcommand{\pdxj}{\partial_{x_j}}
\newcommand{\pdxixi}{\partial_{x_i x_i}^2}
\newcommand{\pdxjxj}{\partial_{x_j x_j}^2}
\newcommand{\pdxixj}{\partial_{x_i x_j}^2}
\newcommand{\pdxjxi}{\partial_{x_j x_i}^2}
\newcommand{\pdmm}{\partial_{m^2}^2}
\newcommand{\pdvi}{\partial_{v_i}}
\newcommand{\pdvj}{\partial_{v_j}}
\newcommand{\pdma}{\partial_{m^\alpha}}
\newcommand{\pdmb}{\partial_{m^\beta}}
\newcommand{\iR}{_{i+\Frac{1}{2}}}
\newcommand{\iL}{_{i-\Frac{1}{2}}}
\newcommand{\iRR}{_{i+1}}
\newcommand{\iLL}{_{i-1}}
\newcommand{\nR}{^{n+\Frac{1}{2}}}
\newcommand{\nRR}{^{n+1}}
\newcommand{\vm}[1]{\left< #1 \right>}
\newcommand{\vmt}[1]{\left< #1 \: |_t\right>}
\newcommand{\vmE}[1]{\left< #1 \: |_E\right>}
\newcommand{\vmL}[1]{\left< #1 \: |_L\right>}
\newcommand{\vmm}[1]{\left< #1 \: |_m\right>}
\newcommand{\vmEs}[1]{\left< #1 \: |_E^*\right>}
\newcommand{\vms}[1]{\left< #1 \: |_v^s \right>}
\newcommand{\pdcg}{\left[\kern-0.12em\left[}
\newcommand{\pdcd}{\right]\kern-0.12em\right]}
\newcommand{\gdcg}{\left[\kern-0.25em\left[}
\newcommand{\gdcd}{\right]\kern-0.25em\right]}
\newcommand{\indic}{{\chi}}

\newtheorem{remarquenum}{Remark}[section]


\makeindex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% infos sur le document
%\title{Sujet de thèse} 

\date{\today}
%\author{Ga\"el Po\"ette}

\begin{document}

%insérer la page de titre
%\maketitle




%remarque, pour les encadrés oranges pour mettre les formules, il faut faire begin{block}{ (et peut etre mettre en taille \footnotesize


%\setcounter{page}{3}
%insérer le sommaire
\textbf{\large{Answer to reviewers for paper (JMLMC-59453): {\it NUMERICAL SPLITTING SCHEMES AS THE CORNERSTONE FOR MINI-BATCH OPTIMIZATION: ON THE IMPORTANCE OF WELL-BALANCED METHODS
}}} \\


\bigskip
\bigskip
\bigskip

We would like to thank the referees for taking the time to carefully read the article. 
We are happy that they found the results of the paper to be promising and insightful and for their general interest.  
We also thank the editor for giving us a second chance. 
The document has been edited under consideration of the remarks made in all
the reviews. The attached documents contain a version of the paper where all changes are highlighted
in blue. 
In the following paragraphs, we first group together the reviewers' common requests. We then respond to each reviewer's specific comments individually.

\ \\
\ \\
In the following paragraphs are our answers to the reviewers' questions and comments. The reviewers' comments are recalled in \textit{italic} while our answers follow. \\ \ \\ 

\textcolor{blue}{Vérifier que toutes les figures et les tables soient bien citées, vérifier s'il reste des ? dans le papier et des TODO dans ce doc}\\


\underline{Common points for all the reviewers} \\

\begin{itemize}
  \item 
    We apologize for the numbering issue (e.g., 1.0.0.1), which resulted from an incompatibility between one of the LaTeX macros we used and the JMLMC template. This has now been corrected.

  \item We also thoroughly reviewed the English (including issues such as "literature", "split" and "frequence") and identified additional errors. We sincerely apologize for this - thank you for your patience.
  \item Two reviewers considered the paper somewhat long. Reviewer \#2 suggested moving certain redundant but necessary figures to the appendix, which we have done. However, the reviewers also requested additional empirical investigations. As a result, while we have reorganized some content, the overall length of the paper has not decreased. We hope this is acceptable.
\end{itemize}
\underline{Reviewer \#1}\\

{\it This paper presents a description of undesirable behaviors in stationary optimizers and investigates a modification of unbalanced operator splitting. The simple modification
is analyzed across various benchmarks to explain its behavior.}

{\it Unfortunately, this reviewer finds that the quality of the manuscript does not meet the standards of the Journal of Machine Learning for Modeling and Computing for the
following reasons. First, the impact on the scientific community is unclear. The manuscript does not clearly state the scientific contribution of the proposed method. In the
current form of presentation, the proposed modification appears to be a minor incremental improvement of the original work of Speth et al. [80]. }\\

We regret that the reviewer found the scientific contribution of the manuscript unclear. In the revised version, we have made a stronger effort to highlight and clarify it (see
revisions marked in blue). We now have a new paragraph, just after the objectives, which lists our contributions. 

Besides, we respectfully disagree with the assessment that the paper offers only an incremental improvement over Speth {\it et al}'s work.
For instance, the aggregator $c_{n+1}$ (see (10)) is never explicitly computed in our algorithm.
In our view, Speth's contribution remains under-recognized, not only by the machine learning optimization community but even within the numerical analysis community to
which it was originally addressed: reviewer \#2 even finishes its report explaining that the "paper could generate interest in both the ML
optimization and numerical analysis communities". We also think that the {\em extended} Speth splitting we suggest could be of value for more classical numerical analysis
problems.\\  


{\it Additionally, the validation study is limited to relatively simple benchmark problems, and a larger scale validation on real-world data and algorithms is missing, despite that the authors mention deep learning and the relevant practical concerns as the motivation for this work.}\\

Our primary goal with this paper was to provide a pedagogical and convincing case for the need for balanced strategies. We believe this can only be achieved through a carefully
constructed progression of simple yet meaningful test cases - an approach that naturally requires both time and space. We acknowledge that the paper is lengthy, a concern also
raised by Reviewers \#2 and \#3, and we apologize if our intention was not sufficiently clear in the initial submission. We hope the revised version makes this clearer. \\ 



{\it Second, the experiments presented in the paper do not appear to have a strong connection to real-world practice in modern data science and machine learning. Stochastic
gradient descent in its basic form is rarely the choice for the training of real-world algorithms today. Many of the assumptions and the design of experiments appear to be too
simplistic and idealized. Such disconnection may be due to the weak literature review of the relevant literature in the deep learning community.}\\

We are somewhat puzzled by the comment regarding weak literature coverage. Once the importance of balancedness was identified, we intentionally explored works well beyond the core
machine learning optimization literature (Speth {\it et al}'s paper is a good example). In fact, Reviewer \#2 noted that the literature review might even be too exhaustive for the scope of the paper.
\textcolor{blue}{[TODO: Bilel, si tu me donnes une phrase pour l'endroit chirurgical où je dois ajouter la ref, je le fais]} In the revised version, we added another recent paper entitled "Randomised splitting methods and stochastic gradient descent" to complete the bibliography. 

On another hand, it is true that the original submission lacked multi-dimensional benchmarks. However, we stand by the use of one-dimensional examples, which - as acknowledged by Reviewers \#2 and
\#3 - offer strong pedagogical value and allow for clear visual demonstrations of the importance of balancedness.

That said, we have followed Reviewer \#3's recommendation and added the real-world benchmark MNIST using a multi-layer perceptron (MLP).
Gains are also obtained. 

Finally, we would like to reiterate that the one-dimensional setting revealed an undesirable behavior. While it is indeed challenging to prove whether this issue extends to higher
dimensions, it is equally difficult to prove that it does not. We believe that documenting this behavior in 1D is a valuable contribution, as it raises an important concern for the
machine learning optimization community and may inspire further investigation.\\



{\it This reviewer also believes that the manuscript can benefit from thorough proofreading and English editing. Additionally, many of the discussions are presented in the form of
bullet lists that lack a coherent flow of logic, making it harder to comprehend the message that the authors try to convey.} \\

We are sorry the english is not satisfactory for the reviewer. We went through even more thorough proofreading with this version. Please, be sure we did take the english question
seriously, even in
the first version of the paper. 
We simply regret that it doesn't come across clearly.

\ \\ \ \\

\underline{Reviewer \#2} \\
\begin{enumerate}
  \item {\bf Empirical Evaluation Limited to Determinisitic Settings}

    {\it While the proposed balanced splitting schemes are empirically compelling, the
paper would benefit from a more formal characterization of their convergence
properties in the stochastic mini-batch setting. Specifically, how does the
introduction of the correction term (e.g., $c_n$ in the Speth-style scheme) interact
with stochastic noise when using truly randomized mini-batches (as opposed to
deterministic cyclic updates)?
In practice, most training routines involve sampling batches with replacement or
random reshuffling. It's unclear whether the balance mechanism remains effective
under such stochasticity, especially in high-dimensional, non-convex landscapes.
A theoretical analysis-or at least empirical investigation-of this point would
strengthen the paper and clarify the robustness of the method under realistic
    training conditions.} \\

Thank you for pointing out the possibility of formally characterizing the convergence properties of our approach. We're not sure why this was omitted from the initial version of the paper, but we have now added an explicit statement of the property, along with relevant commentary.
This property do not need the interpolation condition, which, given our results, is certainly too restrictive. 
    In particular, we emphasize that this property both naturally leads to new mini-batch optimizers and the design of new full-batch ones (see the section about Adam).

    In the new version, we test the extended Speth strategy with random reshuffle. 
    The truth is, we did so before the submission but we did not put the results in the first version of the paper because, as you will see in the revised version, they are redundant (for this reason, they are put in the
    appendices). Besides, we thought the use of deterministic mini-batch could ease the reproducibility of the results. But yes, as you will see, the correction term behaves quite well
    with the stochastic noise.  
    The question is now, at least, empirically investigated in the revised version. 

  \item {\bf Simplified Empirical Evaluation.}

    {\it While the proposed method is evaluated extensively across several synthetic 1D
benchmarks, these settings-though illustrative-are quite limited in complexity
and dimensionality. Since mini-batch optimization is primarily used in high
dimensional and highly non-convex problems (e.g., neural networks)
    it would be
important to test whether the proposed balanced schemes scale to such settings.
Can the authors provide empirical evidence (even preliminary) that their methods
remain effective in higher-dimensional problems or small neural networks?
Alternatively, a discussion of computational bottlenecks, potential scalability
issues, or future plans to test on real-world tasks would help clarify the practical
    applicability of the approach.} \\


    It is true the previous version of the paper focused on 1D problem and the reviewer definitely understood our illustrative intentions with them. 
    In the revised version, we show that the method we suggest remains effective in high dimension. As suggested by reviewer \#3, comparisons are made on the MNIST benchmark. Note
    that we added performance tests too (on all the previous benchmarks). 

  \item {\bf Minor Comments}

    {\it Redundant Figures and Length:}
    \begin{itemize}
      \item [a.] {\it Several empirical figures across Sections 2, 3, and 5 are quite repetitive,
especially those showing stationary distributions for all learning rates and
optimizers across multiple similar benchmark types. While they are useful
for exhaustiveness, some of them might be moved to an appendix or
        summarized more concisely in the main text.} \\

        We moved the figures to the appendices while the comments on these figures remain in the corpse of the manuscript. 
        For readability, we also increased the font sizes and the axis. We hope this is ok.\\

      \item [b.] {\it  For example, Figures 1-4 and 13-17 contain overlapping insights that could
be summarized in fewer plots with aggregate comparisons or referenced
        from supplementary material.} \\

        The new version should be close enough to what the reviewer suggests. We insist we tried putting several results on the same plot but we think it does not help (on the
        contrary) the readability. We tried other formats too. But none of them are as visual as the spikes we display in the figures.  

    \end{itemize}
  \item {\bf Potential to Improve Summary and Discussion:}
    \begin{itemize}
      \item [a.] {\it The paper is quite lengthy, and while it is thorough, condensing some
sections-particularly those reviewing existing optimizers and background-
could improve readability. Streamlining parts of Sections 1.0.0.3 and 1.0.0.4,
for instance, would help sharpen the focus on the core contributions and
        make the paper more accessible to the reader. }

        While Reviewer \#2 suggested shortening the bibliography, Reviewer \#1 did not raise this concern, on the contrary. As a consequence, we regret that we were unable to
        follow Reviewer \#2's previous recommendation; in fact, we have added a few additional references to strengthen the context of our contributions.
    \end{itemize}

  \item {\bf Other Comments}:
  \begin{itemize}
    \item  {\it Typo in multiple places: 'litterature' ->'literature'.}\\

Typos have been corrected. \\

    \item  {\it Clarify early on that the proposed methods allow use of a constant learning rate
without requiring full gradient evaluations, as this is particularly relevant for the
      machine learning community.} \\

  We made some changes in the presentation of the contributions of the paper (as also demanded by reviewer \#1), we hope the new presentation also corresponds to reviewer \#2's
  remarks.\\

    \item {\it Please ensure the caption text in all figures is clear and that axis labels and legends
      are readable, particularly in the 1D plots.}\\

      This is now done in the revised version.\\ 

    \item {\it The github link attached to the paper appears to be not available. Please fix this.}\\

      \textcolor{blue}{[TODO: je ferai un jupyter-notebook à la fin]} {Sorry about that, it is a mistake. We changed the link. A jupyter-notebook can be found, for reproducibility
      of the results.}\\
  \end{itemize} 

\item {\bf Final thoughts:}:
  {\it The clarity of the operator splitting interpretation and the construction of the benchmarks
are appreciated. This paper could generate interest in both the ML optimization and
numerical analysis communities} \\

Thank you very much for this last comment, this is truly motivating. 
   \end{enumerate}
\ \\ \ \\

\underline{Reviewer \#3}\\

{\bf Topic \& Contribution} {\it The manuscript reinterprets mini-batch optimizers (such as SGD and random-reshuffle GD) through the lens of operator splitting and shows that
conventional splitting schemes (e.g., Lie-Trotter, Strang) are unbalanced. This imbalance causes steady states to depend on the learning rate and can result in convergence to
"artificial" stationary points when the interpolation condition is not met. The authors adapt a balanced splitting method from Speth (2020), generalize it to multiple batches, and
propose a low-memory variant.}

{\bf Strengths}

\begin{itemize}
  \item {\it Offers a fresh operator-splitting perspective that clearly explains the origin of optimization pathologies under common training regimes.}
  \item {\it Proposes a balanced update rule that aligns the fixed points of each sub-flow, eliminating the mismatch seen in standard methods.}
  \item {\it One-dimensional toy examples effectively illustrate how the proposed method avoids artificial stationary distributions and converges to true minima.}
\end{itemize}\ \\
We thank the reviewer for identifying the strengths of the paper.\\ 

{\bf Main Technical Weaknesses}
\begin{itemize}
  \item {\it Lacks theoretical guarantees: no convergence or stability theorem is provided for the proposed method in nonlinear or high-dimensional settings.}

    It is true the first version of the paper lacked some theoretical properties. A property has been added in the revised version. 
    It is not exactly a convergence theorem but it does not need the interpolation condition, which given the numerical results, is certainly too restrictive. 

    Note that some of our benchmarks already were nonlinear, even non-convex.  But it is true they were 1D. 
    We did as reviewer \#3 suggested and added results on the MNIST benchmark. 

  \item {\it Experimental validation is limited entirely to simple 1D benchmarks; no results are shown for neural networks or practical datasets.}

    It is true we did not, in the first version of the paper, tackle "real world problems". To our defense, this was because we thought a pedagogical and progressive emphase on the
    importance of using balanced strategies was prior to complex multi-dimensional problem, 
if only for display reasons. But results on MNIST benchmark have been added, as suggested.  

  \item {\it Extension to adaptive optimizers like Adam is only briefly discussed and not analyzed in depth.}

    We added a full subsection about Adam together with some detailed discussion.  
\end{itemize}

{\bf Empirical Limitations}
\begin{itemize}
  \item {\it Results do not include runtime, memory usage, or accuracy comparisons on realistic tasks (e.g., MNIST, CIFAR).}

    Some new benchmarks have been added, we hope the reviewer consider them realistic tasks. Note that the first version of the paper already discussed complexity, which is closely
    related to memory usage. We also added the runtimes for all the considered benchmarks.  

  \item {\it No benchmarking against variance reduction methods that also operate outside the interpolation regime, such as STORM or RR-SARAH.}

We acknowledge the value of comparing our methods with STORM and RR-SARAH. However, such comparisons fall outside the primary scope of this paper, which focuses on a different set of contributions. Additionally, two of the three referees noted that the manuscript was already quite lengthy. We hope this is ok. 

  \item {\it It is unclear how robust the learning rate and stopping criteria are across diverse problem settings.}

   We hope this is now clearer with the runtimes discussion and the freshly added more realistic benchmarks. 

\end{itemize}

{\bf Presentation Issues}
\begin{itemize}
  \item {\it Section and subsection numbering is broken throughout (e.g., "1.0.0.4 Objectives"; "3.0.0.3 Construction of Balanced Schemes")}

    This has been corrected, sorry if this made the first version hard to read. 
  \item {\it Contains numerous grammatical and typographical errors (e.g., splitted, litterature, frequence).}

    Thanks, we corrected these typos and found some more. We hope this is now ok. Please, be sure we already did take the english question seriously in the first version but it is
    sometimes hard. 
  \item {\it Many figures are hard to read due to missing or tiny axis labels and legends.}

    We changed the figures. 
  \item {\it Equations occasionally omit definitions or proper notation.}

    
    We carefully reviewed the paper multiple times and, to the best of our knowledge, did not find any missing definitions or inconsistencies in the notation. We apologize if anything was unclear.

Of course, we would be happy to clarify any definitions or notations if the reviewer can indicate the specific points of concern.

  \item {\it The reference list is not consistent, many entries lack publication venues, or follow different formatting styles..}

    \textcolor{blue}{[TODO: reference... Comment faire?]} We checked the list of publication. It now has the same formatting style. 
\end{itemize}

{\bf Recommendations for Revision}
\begin{itemize}
  \item {\it Include at least one high-dimensional experiment using a real neural network (e.g., MLP on MNIST) comparing standard and proposed methods in terms of training loss,
    accuracy, runtime, and robustness to initialization.}

    We added some benchmarks, one of which is exactly the one suggested by reviewer \#3 in the above recommandation.  

  \item {\it Provide a formal (even local) convergence result for the balanced splitting scheme under reasonable assumptions.}

    We added an important property in the revised version, which does not rely on the interpolation condition. It concerns the convergence toward critical points of the newly built mini-batch
    algorithms. 

  \item {\it Include clear pseudocode and computational cost analysis (both memory and FLOPs) for the proposed algorithm and its variants.}

    We have added pseudo-codes (see appendices), added runtimes comparisons (additional figures) and expanded our discussion of the memory implications associated with the complexity already analyzed in the first version. These additions have been
    placed in the appendices to address Reviewer \#2's concerns regarding the overall length of the paper.

  \item {\it Substantially revise the manuscript to correct grammar, improve figure quality, and ensure references are complete and properly formatted.}

    We hope this is now OK. 

\end{itemize}


{\it The paper introduces a promising and insightful idea that could have practical impact. However, the current manuscript is more of a conceptual proposal than a fully
substantiated research article. Major additions to the theory, experiments, and presentation are needed before it is suitable for publication.}

\ \\ \ \\


Thanks again for your remarks and questions, we hope the paper is now ready for publication.



{
\bibliography{./refs}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}


