%%%%%%%%%%%%%%%%%%%%%%% file template.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is a general template file for the LaTeX package SVJour3
% for Springer journals.          Springer Heidelberg 2010/09/16
%
% Copy it to a new file with a new name and use it as the basis
% for your article. Delete % signs as needed.
%
% This template includes a few options for different layouts and
% content for various journals. Please consult a previous issue of
% your journal as needed.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% First comes an example EPS file -- just ignore it and
% proceed on the \documentclass line
% your LaTeX will extract the file if required
\begin{filecontents*}{example.eps}
%!PS-Adobe-3.0 EPSF-3.0
%%BoundingBox: 19 19 221 221
%%CreationDate: Mon Sep 29 1997
%%Creator: programmed by hand (JK)
%%EndComments
gsave
newpath
  20 20 moveto
  20 220 lineto
  220 220 lineto
  220 20 lineto
closepath
2 setlinewidth
gsave
  .4 setgray fill
grestore
stroke
grestore
\end{filecontents*}
%
\RequirePackage{fix-cm}


%
%\documentclass{svjour3}                     % onecolumn (standard format)
%\documentclass[smallcondensed]{svjour3}     % onecolumn (ditto)
\documentclass[smallextended]{svjour3}       % onecolumn (second format)
%\documentclass[twocolumn]{svjour3}          % twocolumn
%
\smartqed  % flush right qed marks, e.g. at end of proof
%
% \usepackage{mathptmx}      % use Times fonts if available on your TeX system
%
% insert here the call for the packages your document requires
%\usepackage{latexsym}
% etc.
%
% please place your own definitions here and don't use \def but
% \newcommand{}{}
%
% Insert the name of "your journal" with
% \journalname{myjournal}
%

\usepackage{graphicx}
\usepackage{enumitem}
%
% \usepackage{mathptmx}      % use Times fonts if available on your TeX system
%
% insert here the call for the packages your document requires
%\usepackage{latexsym}
% etc.
%
% please place your own definitions here and don't use \def but
% \newcommand{}{}
%
% Insert the name of "your journal" with
% \journalname{myjournal}
%
\newcommand{\newG}{\textcolor{magenta}}
\usepackage[english]{babel} 
\usepackage[utf8]{inputenc}

\usepackage{pifont}
\usepackage{mathtools} %Paquet pour des équations et symboles mathématiques
\usepackage{amsfonts}
%\usepackage{unicode-math}
\usepackage{dsfont}
\usepackage{amsmath}

\usepackage{url}
\usepackage{tikz,xcolor}
\usepackage{hyperref}
\usepackage{booktabs}

% Make Orcid icon
\definecolor{lime}{HTML}{A6CE39}
\DeclareRobustCommand{\orcidicon}{%
	\begin{tikzpicture}
		\draw[lime, fill=lime] (0,0) 
		circle [radius=0.16] 
		node[white] {{\fontfamily{qag}\selectfont \tiny ID}};
		\draw[white, fill=white] (-0.0625,0.095) 
		circle [radius=0.007];
	\end{tikzpicture}
	\hspace{-2mm}
}

\foreach \x in {A, ..., Z}{%
	\expandafter\xdef\csname orcid\x\endcsname{\noexpand\href{https://orcid.org/\csname orcidauthor\x\endcsname}{\noexpand\orcidicon}}
}

% Define the ORCID iD command for each author separately. Here done for two authors.
\newcommand{\orcidauthorA}{0000-0002-2652-043X}
\newcommand{\orcidauthorB}{0000-0002-6964-4504}

\hypersetup{pdftoolbar=false,        % show Acrobat’s toolbar?
	pdfmenubar=true,        % show Acrobat’s menu?
	pdffitwindow=false,     % window fit to page when opened
	pdfstartview={FitH},    % fits the width of the page to the window
	pdfnewwindow=true,      % links in new PDF window
	pdfcreator={Bensaid Bilel}
}

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{tabularx}
\usepackage{diagbox}

\usepackage{appendix}

\DeclareMathOperator*{\argmin}{\arg\!\min}
\DeclareMathOperator*{\argmax}{\arg\!\max}

\renewcommand{\epsilon}{\varepsilon}
\newcommand{\R}{\mathcal{R}}
\newcommand{\tR}{\tilde{\mathcal{R}}}
\newcommand{\loss}{\mathcal{L}}
\newcommand{\Rb}{\mathbb{R}}
\newcommand{\nR}{\nabla \mathcal{R}}
\newcommand{\nnR}{\nabla^2 \mathcal{R}}
\newcommand{\alg}{\mathcal{A}}
\newcommand{\critic}{\mathcal{C}}
\newcommand{\statio}{\mathcal{E}}
\newcommand{\zerodot}{\mathcal{Z}}
\newcommand{\accum}{\mathcal{A}}
\newcommand{\invar}{\mathcal{M}}
\newcommand{\voisi}{\mathcal{V}}
\newcommand{\globalset}{\mathcal{G}}
\newcommand{\mean}{\mathbb{E}}
\newcommand{\prob}{\mathbb{P}}
\newcommand{\image}{\R\left(\statio\cap K\right)}

\newcommand{\betone}{\beta_1}
\newcommand{\bettwo}{\beta_2}
\newcommand{\bbetone}{\bar{\beta_1}}
\newcommand{\bbettwo}{\bar{\beta_2}}
\newcommand{\bbet}{\bar{\beta}}

\newcommand{\Frac}[2]{\displaystyle \frac{#1}{#2}\otimes }
\newcommand{\BlackBox}{\rule{1.5ex}{1.5ex}}
%\renewcommand{\qed}{\hfill\blacksquare}

\newcommand{\polyTwo}{PolyGlobalMild }
\newcommand{\polyThree}{PolyLocalMild }
\newcommand{\polyFour}{PolyGlobalStiff }
\newcommand{\polyFive}{PolyAllStiff }

\newcommand{\exOne}{$"2Gen"$ }
\newcommand{\exTwo}{$"2Ego>Gen"$ }
\newcommand{\exThree}{$"2Ego<Eq<Gen"$ }
\newcommand{\exFour}{$"2Ego\ll Gen"$ }
\newcommand{\exFive}{$"3Gen"$ }
\newcommand{\exSix}{$"3Ego<2Plot\approx Gen"$ }
\newcommand{\exSeven}{$"3Ego\ll Gen"$ }
\newcommand{\exHeight}{$"4Ego<2Plot<Gen"$ }

\newcommand{\mg}{gm} %minimum global
\newcommand{\ml}{lm}
\newcommand{\ps}{sp}

%\usepackage{pgfplots}
%\usepgfplotslibrary{external} 
%\tikzexternalize



\begin{document}

\title{Numerical splitting schemes as the cornerstone for mini-batch optimization}
\subtitle{}

\titlerunning{Optimization splitting schemes}        % if too long for running head

\author{Bilel Bensaid     \and
		Ga\"el Po\"ette   \and
        Rodolphe Turpault
}

%\authorrunning{Short form of author list} % if too long for running head

\institute{B.Bensaid \at
              \email{bilel.bensaid30@gmail.com}
           \and
           G.Po\"ette \at
              \email{gael.poette@cea.fr}
           \and
           R.Turpault \at
           \email{rodolphe.turpault@u-bordeaux.fr}
}

\date{Received: date / Accepted: date}
% The correct dates will be entered by the editor


\maketitle

\begin{abstract}
	Mini batch optimization is at the center of neural network training. 
        Many stochastic algorithms used in this setting are based on the interpolation condition related to the overparametrization of the network. 
        Besides, these optimizers need an extensive tuning of the hyperparameters to converge and give appropriate performances. 
        In this paper, we investigate the non-interpolating case and highlight some new undesirable behaviors for the stationary distributions of the most popular stochastic optimizers. 
        We exploit the notion of balanced splitting to explain such behavior and to build new mini-batch optimizers that work without tuning and without interpolation assumption. 
        These new algorithms are rigorously tested on a set of analytic and Machine Learning benchmarks where they outperform the famous Adam algorithm.
\keywords{Non-convex optimization \and Mini-batch algorithms \and Splitting schemes}
\subclass{65K10 \and 65L05 \and 60H10}
\end{abstract}

\section{Introduction}
\label{intro}

Deep Learning tasks \cite{image_recognition,language_recognition,plasma} imply the resolution of highly non-convex problems \cite{DL_opti}:
\begin{equation*}
\displaystyle{\min_{\theta \in \Rb^N}} \R(\theta).
\end{equation*}
This objective function has the particularity to involve a sum-structure on all the data of the problem:
\begin{equation*}
	\R(\theta) = \displaystyle{\sum_{p=1}^{P} \loss(u(\theta,x_p), y_p)},
\end{equation*}
where $(x_p,y_p)_{1\leq p \leq P}$ are the data of the problems, $u(\theta,x)$ a parametrized model (for example a neural network) and $\loss$ a loss function measuring the distance between predictions $u(\theta,x)$ and real data. In practice, the memory complexity is proportional to the number of data $P$ and $P$ is big. Therefore, the memory cost exceeds the RAM storage. To overcome this difficulty, the data are splitted into $m$ batches of size $b$ (except the last batch of size $P-(m-1)b$):
\begin{equation}
	\R(\theta) = \displaystyle{\sum_{p=1}^{P} \loss(u(\theta,x_p), y_p)} = \sum_{i=1}^m \R_i(\theta),
	\label{pb_somme_fini}
\end{equation}
where each function $\R_i$ is computed on $b$ data and not $P \gg b$. Then at every iteration, the algorithm has only access to one function $\R_i$. This problem is a particular case of stochastic optimization:
\begin{equation}
	\R(\theta) = \mean \left[\R_{\zeta}(\theta)\right],
	\label{opti_sto}
\end{equation}  
where $\zeta$ is a probability law. Although we study mini-batch optimization for memory consumption issue, some authors are interested in this for other reasons: it is commonly accepted that stochastic noise introduced by the batches helps escaping saddle points \cite{sgd_escape1,sgd_escape2,sgd_escape3,sgd_escape4} or achieving best generalization error \cite{sgd_gen1,sgd_gen2,sgd_gen3,sgd_gen4}. Many optimizers have been suggested these recent years to find critical points for the finite sum problem \eqref{pb_somme_fini}. In the following, we present the most important family of mini-batch optimizers.

\paragraph{Stochastic gradient descent (SGD)}
~~\\
The most studied optimizer to minimize \eqref{pb_somme_fini} is the stochastic gradient descent (SGD), introduced in \cite{SGD_Robins}. SGD consists in applying a gradient descent to each function $\R_i$, where $i$ is randomly sampled:
\begin{equation}
	\theta_{n+1} = \theta_n-\eta \nR_i(\theta_n),
	\label{SGD}
\end{equation}
uniformly in $\{1,\dots,m\}$. The convergence of SGD was studied under several assumptions:
bounded variance \cite{sgd_general_diminishing_lr,Bertsekas_basis,sgd_dynamical_basis,bertsekas_theorem,SGD_upper_bound}, relaxed strong growth condition \cite{RG_mean,RG_almost_sure}, gradient confusion \cite{gradient_confusion}, sure-smoothness \cite{sure_smoothness_sgd} and expected smoothness \cite{ES_sgd,sgd_descent_condition,ES_sgd,sgd_global_KL}. In all these studies, learning rates need to decrease as:
\begin{equation}
	\sum_{n\geq 0} \eta_n = +\infty \text{ , } \sum_{n\geq 0} \eta_n^{2}<+\infty.
	\label{diminishing_step}
\end{equation}
to ensure convergence. In practice, SGD is used with constant learning rate. However, the authors of \cite{sgd_prec} showed that even under bounded variance assumption, SGD with any constant learning rate does not converge to a critical point. To expect such a convergence, assumptions like Maximal Strong Growth Condition \cite{MSG_strong_convex,MSG_IG} or Expected Strong Growth Condition \cite{ESG_upper_bound} are necessary. All these hypotheses imply the famous interpolation condition:
\begin{equation}
	\nR(\theta)=0 \implies \forall i=1,\dots,m; \nR_i(\theta)=0.
	\label{interpolation}
\end{equation}
This assumption is very strong. For example, concerning neural networks, this assumption is related to the surparametrization ($N\gg P$) of the network \cite{ESG_upper_bound,ESG_IG}. Then it is not possible to build efficient small networks using SGD. One of the main concern of this work will be to overcome the interpolation condition \eqref{interpolation}. Stochastic inertial algorithms and Adam-like optimizers suffer from the same limitation \cite{sgd_prec,adam1,adam2,zou_rms,rms_not_bounded}.

\paragraph{Random reshuffle gradient descent (RRGD)}
~~\\
In practice, the reshuffle versions are prefered to classical ones \cite{RR_use1,RR_use2,RG_mean}. For example, the version of SGD with random reshuffling is called Random Reshuflle GD (RRGD) or SGD without replacement. It consists, at the beginning of every epoch, in sampling a random permutation and then to apply the $m$ updates \eqref{SGD} in the order given by the permutation. Empirically, it seems that RRGD converges faster than SGD and gives better performances \cite{RR_use1,RR_use_superior}. Many works \cite{RR_outperforms_quadratics,RR_use3,RR_outperforms_convex,RR_small_epochs} try to explain this superiority theoretically, especially for well-conditionned convex functions.

\paragraph{Variance reduction}  
~~\\
Despite being not used in practice, variance reduction optimizers have been extensively studied by ML researchers this last decade, since they can converge with constant learning rate. In broad outline, the aim is to conceive methods that ensure that the variance does not vanish asymptotically:
\begin{equation*}
	\displaystyle{\sum_{i=1}^m}\left[\|\nR(\theta_n)/m-\nR_i(\theta_n)\|^2\right] \to 0.
\end{equation*}
Now, we will present some of these methods describing their principal features:
\begin{itemize}
	\item historically, one of the first method of this kind introduced in the strongly convex case is SAG: Stochastic Average Gradient. The goal is to combine the small computational cost of SGD with the linear convergence of GD on strongly convex functions. The iterations are of the form:
	\begin{equation*}
		\theta_{n+1}=\theta_n-\eta \sum_{i=1}^m y_i^n
	\end{equation*}
	where at each iteration, one batch $i_n$ is sampled and the variables $y_i^n$ are updated according to:
	\begin{equation*}
		y_i^n =
		\left\{
		\begin{array}{ll}
			\nR_i(\theta_n) \text{ si } i=i_n, \\
			y_i^{n-1} \text{ sinon }.
		\end{array}
		\right.
	\end{equation*}
It is interesting to see that this algorithm is a random version of a former deterministic optimizer called IAG \cite{IAG_first}: Incremental Aggregated Gradient Descent. We will come back to this algorithm later. This optimizer needs to store the $m$ last gradients ($m$ is the number of batches). Then the memory cost involves the product $mN$ that may be big.
\item From now on, $i_n$ denotes a uniform sample in $\{1,\dots,m\}$ at iteration $n$. In order to reduce the memory cost, the authors of \cite{SVRG_first} suggest SVRG: Stochastic Variance Reduced Gradient which is the basis of many other methods. Let us imagine that we keep a weight $\tilde{\theta}$ every $\tilde{m}$ iterations. The SVRG update is:
\begin{equation*}
	\theta_{n+1} = \theta_n -\eta \left[\nR_{i_n}(\theta_n)-\nR_{i_n}(\tilde{\theta})+\nR(\tilde{\theta})\right].
\end{equation*}
Let us note two important facts: the introduction of a new hyperparameter $\tilde{m}$ representing the frequence of full-batch gradient evaluation and the computation of such a gradient. This is typical of many variance reduction methods, but it is an issue in our case because we want to avoid evaluation of $\R$ or $\nR$ because of the storage requirement. Based on the same ideas, we could mention SAGA \cite{SAGA_first} or SNVRG \cite{SNVRG_first}. For bounded variance noise, this algorithm admits a complexity that may be "best" than GD for large $b$. 
\item There are also methods called recursive variance reduction gradients . The prototype of these methods is SARAH \cite{SARAH_first}. The idea is to obtain recursively an estimation $g_n$ of $\nR$ without storing previous gradients:
\begin{equation*}
	g_n = \nR_{i_n}(\theta_n)-\nR_{i_n}(\theta_{n-1})+g_{n-1}
\end{equation*}
where every $\tilde{m}$ iterations, the estimation $g_n$ is the full batch gradient $\nR$. In the same family we find GEOM-SARAH \cite{GEOM_SARAH}, ZERO-SARAH (no computation of $\nR$ but introduction of a new hyperparameter and similar storage as SAG) \cite{ZERO_SARAH}, STORM (add an inertial hyperparameter $\beta$ to tune) \cite{STORM_first}, SPIDER (variance reduction on normalized gradient) \cite{SPIDER_first}, SSRGD (mix between SARAH and perturbed GD in order to escape more efficiently to saddle points) \cite{SSRGD}, PAGE (alternating between SARAH and SGD with some probability) \cite{PAGE}, Katyusha (acceleration on strongly convex functions) \cite{Katyusha}. 
\item Let us now present reduction variance methods that do not need computation of $\nR$, tuning of a hyperparameter like $\tilde{m}$ or $\beta$, or the storage of $m$ gradients. The first method that fits into this setting is SCSG \cite{SCSG}. The update looks like the one for SARAH (we do not substract the evaluation of the last iterate but of the first of each epoch) by replacing the full-batch gradient by a stochastic gradient. Coupling variance reduction with random reshuffling, the authors of \cite{RR_SARAH} suggest very recently RR-SARAH: its memory cost involves $3N$ and not $mN$ and it does not need computation of the full-batch gradient $\nR$. To the best of our knowledge, it is the only optimizer that enjoys these properties. However, the analysis is limited to strongly convex functions under strong assumption on the noise. Besides, the numerical experiences are limited to the logistic regression (for only one initialization). 
\end{itemize}

\paragraph{Adaptive time steps}
~~\\
So far, all the mini-batch optimizers mentioned above use decreasing or constant time step, so tuning is generally necessary to make the methods converge. In fact, there are very few works about adaptive learning rate (time step) mini-batch optimizers (algorithms that do not need tuning):
\begin{itemize}
	\item first we could find some extensions of the classical Armijo rule. S.Vaswani \cite{stochastic_armijo} suggested to apply the Armijo rule to each batch iteratively:
	\begin{equation*}
		\R_i(\theta_n-\eta_n^i \nR_i(\theta_n))-\R_i(\theta_n) \leq -\lambda \eta_n^i \|\nR_i(\theta_n)\|^2.
	\end{equation*}
	Under growth condition, it is proved that the minimum of the gradient converge if $\lambda=0.5$. But it is necessary that the maximal time step allowed in the linesearch (research of the time step satisfying the above inequality) depends on the stiffness of the function and of a constant involved in the growth condition. Then the algorithm is not free from hyperparameters to tune. To get rid of the overparametrized setting (interpolation condition), \cite{armijo_batch_size,armijo_variance_reduction} suggest to make the size $b$ vary in order to control the variance of the estimator of the gradient. Such an approach is not conceivable under memory constraints since $b$ is constrained by the hardware. Another approach consists in approximating the full-batch gradient thanks to a gaussian process \cite{armijo_GP}: non-realistic assumptions of independence between $\R(\theta_n)$ and $\nR(\theta_n)$ are necessary and the approach is limited to very small dimensions since it needs to add a point to the process when a batch is sampled. In the context of bayesian optimization ($\R$ is now a random variable) an approach based on a Taylor development in $L^2$ is proposed in \cite{armijo_L2} but isotropic and gaussian (on the kernel) assumptions are required. 
	\item After Armijo rule comes the extension of trust regions to the stochastic case. Most of the generalizations necessitate to require strong assumptions on the noise \cite{sto_TR_bounded} or a precise knowledge of it \cite{sto_TR_affine_noise,sto_TR_representative}. To overcome these limitations, \cite{sto_TR_sampling} suggested a trust-region method with adaptive $b$, which is not conceivable for the same reasons as Armijo. 
	\item Other works can be mentionned especially for varying batch sizes \cite{adaptive_sampling,adaptive_batch,sgd_lauched}. There are also algorithms called CMA \cite{CMA,CMA_Light} but they require computation of $\R$ at every epoch.
\end{itemize} 

\paragraph{Objectives}
~~\\
This recap of the mini-batch litterature encourages us to require the six following properties in the conception of a mini-batch optimizer, suitable for any networks (even for under-parametrized model/network):
\begin{enumerate}
	\item Ideally, we want to \textbf{avoid assumptions on the batch distribution}. Firstly, the bounded variance hypothesis could not be considered since it is not even satisfied by a linear regression. Moreover, the major part of stochastic optimizers need interpolation condition (then over-parametrized models) to be used with constant learning rate. At the best of our knowledge, the only optimizers that get rid of these conditions are IAG and DIAG \cite{DIAG}.
	\item It is essential that the optimizers could achieve an arbitrary precision (concerning the gradient) without requiring decreasing time steps. This is only verified by variance reduction methods. 
	\item The algorithms can not have the right to use the full-batch gradient due to storage requirement. The ones that fit into this category are: SGD, RRGD, IG, IAG, SAG, ZERO-SARAH, STORM, SCSG et RR-SARAH.
	\item If possible, we do not want to store $m$ gradients. For example, the optimizers that do not need such a storage are:  SGD, RRGD, IG, SVRG, SNVRG, SPIDER, SARAH, SSRGD, PAGE, SCSG et RR-SARAH.
	\item We absolutely want an \textbf{adaptive time step method} since the main goal of this paper is to avoid tuning hyperparameters. In this case, all the methods that do not necessitate a growth condition or a precise knowledge of the noise make the batch size vary with a risk to exceed the memory. Let us note that such an approach can generate strong slow down due to dynamic memory access. Let us also note that in practice the batch size is not meant to be a hyperparameter to tune since it is given by the quotient between the data size and the RAM size. The only optimizers that overcome this difficulty are the CMA algorithms. 
	\item Finally, we absolutely need a \textbf{stopping criterion} that ensures that the full-batch gradient is smaller than a fixed arbitrary precision. All the mentionned optimizers are stopped when a maximal number of epochs is achieved (at the best of our knowledge). 
\end{enumerate}
Most of the optimizers satisfy only one or two of the aforementioned aspects. IAG satisfies the first three criteria. RR-SARAH satisfies the requirements 2, 3 and 4 (for the second, it is proved in the strongly convex case and it will be clear in the general case thanks to the splitting operators point of view presented). Therefore the goal of this paper is to build a deterministic or stochastic (never mind) mini-batch optimizer that \textbf{satisfies all these criteria}. This is a crucial step in order to develop smaller machine learning models. 

\paragraph{Organization}
~~\\
To reach such a construction, the paper is organized as follows:
\begin{itemize}
	\item In section \ref{section_stationary}, the stationary distribution of SGD and RRGD (with constant learning rate) is investigated empirically on analytic examples. Indeed, it is necessary that SGD and RRGD recover the critical points of $\R$ in the long run, and not the ones of the $\R_i$. To do that, we build new benchmarks (in one dimension) to discuss the negation of the interpolation assumption. Undesirable behaviors (at the best of our knowledge not documented in the litterature) are identified. 
	\item An interpretation of RRGD as a specific splitting of operators is suggested in section \ref{section_splitting_schemes}. This enables to relate some "strange" stationary distributions to a disequilibrium of the splitting. 
	\item The construction of a balanced splitting gives birth to a first optimizer in section \ref{section_optimizers}. But it requires the storage of $m$ gradients. A new splitting formula makes it possible to overcome this limitation and to build the first optimizer satisfying the six criteria.
	\item In section \ref{section_results}, the two methods developped above are tested on analytical benchmarks and on classical machine learning tasks. They are compared with RRAdam (Random Reshuffle Adam) the favorite optimizer of the practitioners.   
\end{itemize}

\section{Stationary distributions of SGD}
\label{section_stationary}

The aim of this section is to investigate empirically the stationary distribution of SGD and RRGD with constant learning rate. We expect that the support is "centered" around the minimums of $\R$.

\subsection{Beyond the interpolation condition}
\label{subsection_beyond_interpolation}

\paragraph{Interpolating benchmarks}
~~\\
To evaluate the properties of the different optimizers (specifically their stability and speed), it is necessary to have some analytical benchmarks. In \cite{Bilel}, the authors build some toy neural networks in two dimensions with different properties. These examples are recalled in appendix \ref{annexe_polys} (their names are modified for pedagogical reasons) but we mention below some of their caracteristics:
\begin{itemize}
	\item the benchmark \polyTwo admits four global minimums of zero values and four non-degenerate saddle points. It is the most simple configuration presented in \cite{Bilel}.
	\item The benchmark \polyThree presents one global minimul of zero value and three local minimums with different values. The saddle points are still non-degenerated but their values are differents.
	\item The benchmark \polyFive is very stiff and possesses several mimimums with different orders of degeneracy. It also possesses degenerate saddle points. 
\end{itemize}
Despite their pertinence concerning the evaluation of full-batch optimizer, they are not sufficient for mini-batch optimization since they satisfy the interpolation condition. 

\paragraph{Non-interpolating benchmarks}
~~\\
As highlighted in the introduction, the major part of the studies proving that constant step size SGD converges to critical point of $\R$ is based on the interpolation condition or the overparametrization of the network. As all the critical points of the above benchmarks satisfy the interpolation assumption, it is necessary to suggest new functions taking into account all the possible setups in the non-interpolated case, in one dimension. These examples are presented in detail in appendix \ref{annexe_non_interpolated}. For pedagogical reasons, we have tried to give them a name related to game theory. Succintly, we present their caracteristics below:
\begin{itemize}
	\item for \exOne, the number of batches is $m=2$. Each function $\R_i$ is strongly convex and the function $\R$ admits a \textbf{unique non-interpolating minimum}. The name expresses the fact that the general interest prevails since being selfish, it always exists a penalized agent.
	\item In the case of \exTwo, $m=2$. Each function $\R_i$ is quadric (non-convex) and admits two minimums with a shared global minimum at $0.5$. Then the objective function $\R$ has an \textbf{interpolating local minimum} at $0.5$ and a \textbf{non-interpolating global minimum}. The name means that if the agents are self-centred, they are all individually satisfied (global minimum of $\R_i$) but it is not the best compromise (the global minimum of $\R_i$ becomes a local one for $\R$).
	\item Concerning \exThree, $m=2$. Each function $\R_i$ is a polynomial of degree 6 or 10. Each of the three minimums is \textbf{non-interpolating}. The local minimum in 0 is the \textbf{intersection of the graphs of $\R_1$ and $\R_2$}. Here, the \textbf{global minimum of $\R_1$ is very close to the one of $\R$}. Here, the general interest is not a big "sacrifice" for the first agent. There also exists a balanced choice ($\theta=0$) insofar as the two agents are just as much (un)satisfied. 
	\item For \exFour, $m=2$. Each function $\R_i$ is non-convex and the two minimums of $\R$ are \textbf{non-interpolating}. Here, the \textbf{global minimum of $\R$ is far from the ones of $\R_1$ and $\R_2$}. The general interest does not suit to the agent individually. 
	\item The benchmark \exFive concerns three strongly convex functions ($m=3$). The unique global minimum of $\R$ is non-interpolating. The situation is similar as \exOne but with three agents.
	\item For \exSix, $m=3$. The \textbf{local} minimum at $\theta=0.5$ is \textbf{interpolating}. There exists a point $\theta=-0.75$ which is the minimum of two functions among the three (\textbf{"semi-interpolation"}). Finally, the global minimum is non-interpolating. In this case, if two agents plot ($\theta=-0.75$), the last agent is badly done, but globally this plot leads to a choice close to the general interest. 
	\item \exSeven also possesses $m=3$ batches. Depite being non-convex, $\R$ admits an unique \textbf{non-interpolating global minimum}. The situation is similar as \exFour but with three agents.
	\item \exHeight is made up of $m=4$ batches. The function $\R$ admits a \textbf{non-interpolating local and global minimum}. The global minimums of $\R_1$ and $\R_3$ are close enough, which is not the case for the rest of the global minimums. Therefore, two of the agents may plot but this is detrimental to the general interest, contrary to \exSix.      
\end{itemize}

\paragraph{Graph description}
~~\\
Here, we compute numerically the stationary distribution of SGD/RRGD by Monte Carlo simulation on \exOne and \exTwo. These simulations are performed during a time $t_f=10$ large enough in order for the distributions to be stationary (visual verification). 10000 particles are sampled uniformly on $[-1,1]$ for each batch $i\in \{1,2\}$. The position of the particles after the time $t_f$ enables to build the densities $u^1$ and $u^2$. Here $u^1$ denotes the stationary distribution with the following initial condition (the second coordinate refers to the batch number):
\begin{equation*}
	u^1_0(\theta) = \left(\frac{1}{4} \mathds{1}_{[-1,1]}(\theta),1\right).
\end{equation*}
The same for $u^2$ with the initial condition:
\begin{equation*}
	u^2_0(\theta) = \left(\frac{1}{4} \mathds{1}_{[-1,1]}(\theta),2\right).
\end{equation*}
Let us denote by $L_1$ and $L_2$ the Lipschitz constants of $\nR_1$ and $\nR_2$ on $[-1,1]$ respectively. The figure \ref{sgd_ex1} corresponds to the applications of SGD on \exOne for four different values of the time steps: $\frac{1}{\max(L_1,L_2)}$ is the associated time step related to the stiffest batch whereas $\frac{1}{L_1+L_2}$ is an estimation of $\frac{1}{L}$, that is to say the stiffness of $\R$ ($L$ the lipschitz constant of $\nR$), since $L \leq L_1+L_2$. At each time, the third of these time steps is also considered in order to test a smaller value: the factor 3 could seem arbitrary for the moment but it will be explained in the section \ref{section_optimizers}. The figures \ref{sgd_ex2}, \ref{RRGD_ex1} and \ref{RRGD_ex2} correspond respectively to \exTwo on SGD, \exOne on RRGD and \exTwo on RRGD, for the same choices of time steps ($L_1,L_2$ depends on the benchmark). 

\begin{figure}[!h]
	\centering
	\scalebox{0.45}{\input{chapitre5_img/sgd_ex1.pgf}}
	\caption{Stationary distributions of SGD for different time steps on \exOne: a) $\eta=\frac{1}{\max(L_1,L2)}$, b) $\eta=\frac{1}{3\max(L_1,L2)}$, c) $\eta=\frac{1}{L_1+L_2}$, d) $\eta=\frac{1}{3(L_1+L_2)}$. The green straight line represents the position of the global minimum of $\R$.}
	\label{sgd_ex1}
\end{figure}

\begin{figure}[h!]
	\centering
	\scalebox{0.45}{\input{chapitre5_img/RRGD_ex1.pgf}}
	\caption{Stationary distributions of RRGD for different time steps on \exOne: a) $\eta=\frac{1}{\max(L_1,L2)}$, b) $\eta=\frac{1}{3\max(L_1,L2)}$, c) $\eta=\frac{1}{L_1+L_2}$, d) $\eta=\frac{1}{3(L_1+L_2)}$. The green straight line represents the position of the global minimum of $\R$.}
	\label{RRGD_ex1}
\end{figure}

\begin{figure}[h!]
	\centering
	\scalebox{0.45}{\input{chapitre5_img/sgd_ex2.pgf}}
	\caption{Stationary distributions of SGD for different time steps on \exTwo: a) $\eta=\frac{1}{\max(L_1,L2)}$, b) $\eta=\frac{1}{3\max(L_1,L2)}$, c) $\eta=\frac{1}{L_1+L_2}$, d) $\eta=\frac{1}{3(L_1+L_2)}$. The green straight line represents the position of the global minimum of $\R$ and the blue one the position of the local minimum.}
	\label{sgd_ex2}
\end{figure}

\begin{figure}[h!]
	\centering
	\scalebox{0.45}{\input{chapitre5_img/RRGD_ex2.pgf}}
	\caption{Stationary distributions of RRGD for different time steps on \exTwo: a) $\eta=\frac{1}{\max(L_1,L2)}$, b) $\eta=\frac{1}{3\max(L_1,L2)}$, c) $\eta=\frac{1}{L_1+L_2}$, d) $\eta=\frac{1}{3(L_1+L_2)}$. The green straight line represents the position of the global minimum of $\R$ and the blue one the position of the local minimum.}
	\label{RRGD_ex2}
\end{figure}

\paragraph{Comments}
~~\\
On the graphs a) and c) of figure \ref{sgd_ex1} a Dirac comb appear with a lot of stationary states spaced between the minimum $\theta=0$ of $\R_1$ and $\theta=0.5$ of $\R_2$. In other words, several stationary states have nothing to do with the minimums of the $\R_i$ or $\R$. By decreasing the time step (graphs b) and d)), we get closer to a gaussian but it is not really centered on the global minimum. For RRGD, the behavior is completely different, see figure \ref{RRGD_ex1}. When the time step decreases, the distribution gets closer to a Dirac centered at the global minimum but for intermediary learning rates (graphs a) and b)) the second Dirac is located at the minimum of $\R_2$, that is to say $\theta=0.5$. It is true that the behavior of RRGD is less troubling than the one of SGD but it is still problematic because the convergence (existence of a stationary distribution) does not imply to find a minimum of $\R$. \\
Concerning \exTwo (figure \ref{sgd_ex2}), for any time step, the interpolating local minimum at $\theta=0.5$ is largely favored. By diminishing $\eta$ on the graphs b) and d), the proportion of global minimum increases slightly. Unfortunately, it seems that SGD favors interpolating minimums without taking into account their quality even for time steps sufficiently small. Concerning RRGD (figure \ref{RRGD_ex2}), the global minimum is not neglected even for an intermediary value of the time step (see graph a)). On the graph d), when the learning rate is small enough, the global minimum is favored despite not being interpolating. We will see in what follows that this difference with SGD can be explained by operator splitting. 

About SGD, even by decreasing the learning rate, the stationary distributions are not satisfactory: gaussian not well-centered (\exOne) or interpolating local minimum largely favored.

\paragraph{}
In this section, we have built new benchmarks to analyse the influence of the interpolation condition on the long run behavior of SGD and RRGD. It appears that they have trouble catching non-interpolating critical points. Even worse, SGD creates artificial stationary states in the sense that they are not critical points of the invidual functions $\R_i$ or of the sum function $\R$.   

\section{Splitting schemes}
\label{section_splitting_schemes}

The goal of this section is to explain the previous behaviors of SGD and RRGD and to give numerical schemes that correct their defects. 

Usually, in the litterature, the behaviors of SGD are explained by seeing SGD as a discretization of a brownian differential equation \cite{SDE_comparison,SDE_modified,SDE_edp,malladi_adam,hu2018diffusion,flat_minima_exponential,yang2020fast,sgd_implicit_regularisation,sgd_implicit_regularisation2}. Here, we turn this point of view around by interpreting RRGD (rather its deterministic version) as a particular discretization of the classical gradient flow $\theta'(t)=-\nR(\theta(t))$. This goes through operator splitting. Let us remember the principle on the following Cauchy problem:
\begin{equation}
	\left\{
	\begin{array}{ll}
		y'(t) = F(y(t)), \\
		y(0) = y_0,
	\end{array}
	\right.
	\label{Cauchy_problem}
\end{equation}
which admits the exact solution $\phi_t(y_0)$ at time $t$. Let us consider that $F$ is the sum of $m$ operators $F=\displaystyle{\sum_{i=1}^m}F_i$ in such a way that each system:
\begin{equation*}
	\left\{
	\begin{array}{ll}
		y'(t) = F_i(y(t)), \\
		y(0) = y_0,
	\end{array}
	\right.
\end{equation*}
can be solved exactly (to make it simpler)  with solution $\phi_{\eta}^i(y_0)$ at time $t=\eta$, for all $i\in \{1,\dots,m\}$. By combining these solutions, we get:
\begin{equation}
	\chi_{\eta} = \phi_{\eta}^m \circ \dots \phi_{\eta}^2 \circ \phi_{\eta}^1.
	\label{splitting_lie_trotter}
\end{equation} 
Then the integrator $\chi$ satisfies $\chi_{\eta}(y_0) = \phi_{\eta}(y_0) + \mathcal{O}(\eta^2)$. Then $\chi_{\eta}$ is an order one approximation of \eqref{Cauchy_problem}. A splitting method is then made up of:
\begin{itemize}
	\item the choice of operators $F_i$: in the resolution of an ODE or a PDE, these are selectioned in order to represent the different scales or stiffnesses of a physical phenomenon (for example convection-diffusion). In our case, this choice is imposed by the decomposition of the dataset into $m$ batches. We do not do our best to balance the batches in this paper.
	\item Solve exactly or approximately the ODE $y'(t)=F_i(y(t))$.
	\item Put the different solutions together in order to build a solution for \eqref{Cauchy_problem}. 
\end{itemize}
As a basis for splitting schemes, see \cite{splitting_ode_review,splitting_ode_review2}.

\paragraph{RRGD as a splitting}
~~\\
Let us remember that the deterministic version of RRGD is IG ({\it Incremental Gradient}) in the litterature \cite{IG_proximal}. The latter can be interpreted as a Lie-Trotter splitting with an Euler explicit scheme consisting in applying an Euler explicit scheme to each ODE $\theta'(t)=-\nR_i(\theta(t))$ and then to put them together like \eqref{splitting_lie_trotter}. \\
In the same way as for SGD and RRGD, we plot the stationary distributions of IG on \exOne and \exTwo for the same choices of time steps, on the figures \ref{IG_ex1} and \ref{IG_ex2}. Since there is no stochasticity, we only represent $u$, the stationary distribution corresponding to the initial condition:
\begin{equation*}
	u_0(\theta) = \frac{1}{2} \mathds{1}_{[-1,1]}(\theta).
\end{equation*}
On the figure \ref{IG_ex1}, if the learning rate is not small enough (graphs a) and b)), the Delta Dirac function is centered at the minimum of $\R_1$. On the graph d), $\eta$ is reduced enough in order for the Dirac to be centered at the minimum of $\R$. On the figure \ref{IG_ex2}, the proportion of global minimum prevails over the local one even with an intermediary learning rate (graph a)), which is not the case for RRGD where a reduction of $\eta$ was required. Globally, the asymptotic behavior (when $\eta \to 0$) of IG is the same as RRGD in the long run. As a result, the pre-eminence of RRGD over SGD is not related to a different random sampling but to the splitting.

\begin{figure}[h!]
	\centering
	\scalebox{0.45}{\input{chapitre5_img/IG_ex1.pgf}}
	\caption{Stationary distributions of IG for different time steps on \exOne: a) $\eta=\frac{1}{\max(L_1,L2)}$, b) $\eta=\frac{1}{3\max(L_1,L2)}$, c) $\eta=\frac{1}{L_1+L_2}$, d) $\eta=\frac{1}{3(L_1+L_2)}$. The green straight line represents the position of the global minimum of $\R$.}
	\label{IG_ex1}
\end{figure}

\begin{figure}[h!]
	\centering
	\scalebox{0.45}{\input{chapitre5_img/IG_ex2.pgf}}
	\caption{Stationary distributions of IG for different time steps on \exTwo: a) $\eta=\frac{1}{\max(L_1,L2)}$, b) $\eta=\frac{1}{3\max(L_1,L2)}$, c) $\eta=\frac{1}{L_1+L_2}$, d) $\eta=\frac{1}{3(L_1+L_2)}$. The green straight line represents the position of the global minimum of $\R$ and the blue one the position of the local minimum.}
	\label{IG_ex2}
\end{figure}

The most natural idea to improve IG is then to apply a Strang splitting: in fact, this splitting generates an order two scheme while having a slight supplementary cost for two operators (each indivudal scheme is of order one). This one is commonly written for $m=2$:
\begin{equation*}
	\chi_{\eta} = \phi_{\eta/2}^1 \circ \phi_{\eta}^2 \circ \phi_{\eta/2}^1,
\end{equation*}
where each $\phi_{\eta}^i$ is an Euler explicit scheme with step $\eta$ applied to $\theta'(t) = -\nR_i(\theta(t))$.
The figures \ref{Strang_ex1} and \ref{Strang_ex2} represent respectively the stationary distributions of this scheme for \exOne and \exTwo. On the figure \ref{Strang_ex2}, the rise of the order does not change anything for the values of $\eta$ used on \exTwo. On the figure \ref{Strang_ex1}, we get closer to the stationary state faster when $\eta \to 0$ on \exOne. However, there still exists undesirable stationary states, in other words states, that do not correspond to a minimization of $\R$. 

\begin{figure}[h!]
	\centering
	\scalebox{0.45}{\input{chapitre5_img/Strang_ex1.pgf}}
	\caption{Stationary distributions of the Strang splitting for different learning rates on \exOne: a) $\eta=\frac{1}{\max(L_1,L2)}$, b) $\eta=\frac{1}{3\max(L_1,L2)}$, c) $\eta=\frac{1}{L_1+L_2}$, d) $\eta=\frac{1}{3(L_1+L_2)}$. The green vertical straight line represents the position of the global minimum of $\R$.}
	\label{Strang_ex1}
\end{figure}

\begin{figure}[h!]
	\centering
	\scalebox{0.45}{\input{chapitre5_img/Strang_ex2.pgf}}
	\caption{Stationary distributions of the Strang splitting for different learning rates on \exTwo: a) $\eta=\frac{1}{\max(L_1,L2)}$, b) $\eta=\frac{1}{3\max(L_1,L2)}$, c) $\eta=\frac{1}{L_1+L_2}$, d) $\eta=\frac{1}{3(L_1+L_2)}$. The green vertical line represents the position of the global minimum and the blue one the position of the local minimum of $\R$.}
	\label{Strang_ex2}
\end{figure} 

\paragraph{Non-balanced schemes}
~~\\
Now, the reasonable question is the following: why are there stationary states that have nothing to do with the ones of $y'(t)=F(y(t))$ when this ODE is solved by splitting? This phenomenon was already noticed in the ODE/PDE context \cite{rebalanced_splitting}: arbitrary small time steps may be necessary to recover the stationary states of an ODE, even when the solutions slowly vary. \\
Let us illustrate this phenomenon on an affine ODE where:
\begin{equation}
	F(y) = \left(Ay+a\right) + \left(By+b\right) \coloneqq F_1(y) + F_2(y),
	\label{EDO_linear}
\end{equation}
with $A,B \in \Rb^{N \times N}$ and $a,b \in \Rb^N$. The exact solution of \eqref{EDO_linear} is given by:
\begin{equation*}
	y(t) = y_{\infty} + e^{(A+B)t}\left[y(0)-y_{\infty}\right]
\end{equation*}
where the invariant state $y_{\infty}$ is the solution of the linear system:
\begin{equation*}
	(A+B)y_{\infty} = -(a+b).
\end{equation*}
We assume that the real parts of the eigenvalues of $A+B$ are negative in order for $y_{\infty}$ to be the stationary state of \eqref{EDO_linear} and for all the matrixes at stake to be invertible, to simplify the presentation. The invariant states of each vector field satisfy:
\begin{equation*}
	Ay_{\infty}^1 = -a \text{ et } By_{\infty}^2 = -b.
\end{equation*}
Now, let us apply an exact Lie-Trotter splitting by composing $e^{A\eta}$ with $e^{B\eta}$, that is to say, one step is given by the product $e^{B\eta}e^{A\eta}$. If we have integrated directly $F$, we would use the operator $e^{(A+B)\eta}$. The error for one step $\eta$ is then related to the non-commutativity of $e^{B\eta}e^{A\eta}$ and $e^{(A+B)\eta}$:
\begin{equation}
	e^{B\eta}e^{A\eta} = e^{(A+B)\eta} + \left[B,A\right]\eta^2 + \mathcal{O}(\eta^3),
	\label{Taylor_split}
\end{equation}
where $\left[B,A\right]=BA-AB$ is the commutator of $A$ and $B$.\\
Let us begin by the two cases where this splitting makes it possible to retrieve $y_{\infty}$:
\begin{itemize}
	\item if $A^{-1}a=B^{-1}b$ (interpolation) then $y_{\infty}$ is the stationary sate of each $F_i$.
	\item If $A$ and $B$ commute then for every $\eta>0$, $e^{(A+B)\eta}=e^{B\eta}e^{A\eta}$. Therefore the splitting is exact and the stationary state $y_{\infty}$ is recovered asymptotically. 
\end{itemize}
In the general case, the Taylor developement \eqref{Taylor_split} suggests that the local error introduced by a first order splitting is $\mathcal{O}(\eta^2)$ in the neighborhood of $y_{\infty}$:
\begin{equation*}
	\left[e^{(A+B)\eta}-e^{B\eta}e^{A\eta}\right]y_{\infty} \approx \frac{\eta^2}{2}\left[B,A\right]y_{\infty}.
\end{equation*}
The error above assume that one starts from a point in the neighborhood of $y_{\infty}$ and integrates during one step. In fact, one starts from an arbitrary initial condition and integrate during a time in the order of $1/\eta$, until he approaches the scheme stationary state denoted by $z_{\infty}$. Since then, the error $z_{\infty}-y_{\infty}$ is of order $\mathcal{O}(\eta)$ for a Lie-Trotter splitting and of $\mathcal{O}(\eta^2)$ for the Strang one. Concerning the system \eqref{EDO_linear}, the authors of \cite{rebalanced_splitting} compute explicitly the stationary state for a Strang splitting:
\begin{equation*}
	z_{\infty} = (I-\alpha\beta\alpha)^{-1}\left[\alpha B^*b + (\alpha\beta+I)A^*a\right],
\end{equation*}
where:
\begin{equation*}
	\alpha = e^{A\eta/2} \text{ , } \beta = e^{B\eta},
\end{equation*}
and:
\begin{equation*}
	A^* = (\alpha-I)A^{-1} \text{ , } B^* = (\beta-I) B^{-1}.
\end{equation*}
What we have to retain from this expression is that the stationary state depends on the time step $\eta$ ! In the same way, let us compute the stationary states of IG on \exOne. By expliciting the recurrent equation of IG on \exOne, we get:
\begin{equation*}
	\theta_{n+1} = 4\eta + (1-8\eta)(1-2\eta)\theta_n.
\end{equation*} 
Then, its explicit expression is:
\begin{equation*}
	\theta_n = (1-8\eta)^n(1-2\eta)^n\left(\theta_0-\dfrac{2}{5-8\eta}\right) + \dfrac{2}{5-8\eta}.
\end{equation*}
Therefore for $\eta \in \left]0,\frac{5}{8}\right[$:
\begin{equation*}
	\theta_{\infty} (\eta)= \dfrac{2}{5-8\eta}.
\end{equation*}
For two learning rates tested, the stationary state equals:
\begin{equation*}
	\left\{
	\begin{array}{ll}
		\theta_{\infty}\left(\dfrac{1}{\max(L_1,L_2)}\right) = \frac{1}{2}, \\
		\theta_{\infty}\left(\dfrac{1}{L_1+L_2}\right) = \frac{10}{21}\approx 0.476.
	\end{array}
	\right.
\end{equation*}
We recover the stationary states of the simulation and notice that it is impossible to achieve exactly the minimum of $\R$ at $\theta=\frac{2}{5}$ if $\eta \neq 0$. \\
The splitting approach enables to suggest a plausible explication to the artificial stationary states that we have detected on SGD, RRGD, IG and Strang when $\eta$ does not tend to 0. Such a phenomenon is called an imbalance of the splitting. Therefore, the artificial stationary states (or the predilection towards the interpolating local minimum) are related to the \textbf{non-commutativity} of the vector fields $\nR_1$ and $\nR_2$. 

\paragraph{Construction of balanced schemes}
~~\\
Let us illustrate the construction of a balanced scheme as suggested in \cite{rebalanced_splitting} (adapted to order one) for two operators $F_1$ and $F_2$:
\begin{equation*}
	\text{(Unstationary) } y'(t) = F_1(y(t)) + F_2(y(t)),
\end{equation*}
\begin{equation*}
	\text{(Stationary) } 0 = F_1(y_{\infty}) + F_2(y_{\infty}).
\end{equation*}
The idea of this article is to share a vector $c$ between the two operators $F_1$ and $F_2$ in order for the new operators $F_1^* = F_1+c$ and $F_2^*=F_2-c$ to admit $y_{\infty}$ as a stationary state. At point $y=y_{\infty}$, the vector to add in order to guarantee the equilibrium should be $c_{\infty} = \frac{1}{2} \left[F_2(y_{\infty})-F_1(y_{\infty})\right]$:
\begin{equation*}
	F_1^*(y) =  F_1(y) + \frac{1}{2} \left[F_2(y_{\infty})-F_1(y_{\infty})\right] = 0 \text{ at } y=y_{\infty},
\end{equation*}
\begin{equation*}
	F_2^*(y) =  F_2(y) - \frac{1}{2} \left[F_2(y_{\infty})-F_1(y_{\infty})\right] = 0 \text{ at } y=y_{\infty}.
\end{equation*}
Since $y_{\infty}$ is not known, the balanced splitting methods consist in adding an equilibrium vector $c_n$ at every step in order for the stationary state associated to $F$ to be an approximate stationary state for $F_1^* \coloneqq F_1+c_n$ and $F_2^*\coloneqq F_2-c_n$. The idea often consists in taking this vector of the form $\frac{1}{2}\left[\tilde{F}_1-\tilde{F}_2\right]$ where $\tilde{F}_1$ and $\tilde{F}_2$ are approximations of $F_1$ and $F_2$ computed using previous evaluations. Following this idea, R.L.Speth suggests the following scheme \cite{rebalanced_splitting} (adjustment to order one compared to the original article):
\begin{equation}
	\left\{
	\begin{array}{ll}
		y_{n+1/2} = y_n+\eta F_1(y_n)+\eta c_n, \\
		y_{n+1} = y_{n+1/2}+\eta F_2(y_{n+1/2})-\eta c_n, \\
		c_{n+1} = c_n+\frac{1}{\eta}\left[\frac{y_{n+1}+y_n}{2}-y_{n+1/2}\right].
		\label{splitting_speth}
	\end{array}
	\right.
\end{equation}
This scheme is well-balanced since if $(y_n)_{n\in \mathbb{N}}$ converges then:
\begin{equation*}
	\left\{
	\begin{array}{ll}
		F_1(y_{\infty})=-c_{\infty}, \\
		F_2(y_{\infty})=c_{\infty}.
	\end{array}
	\right.
\end{equation*}
which gives $F(y_{\infty})=0$. The strength of a balanced scheme is the following: either it does not admit a stationary distribution, or it converges to the stationary states of $F$.\\
The figures \ref{speth_ex1} and \ref{speth_ex2} represent the stationary distributions for the scheme \eqref{splitting_speth} on \exOne and \exTwo. First on \exOne, we perfectly retrieve the minimum for the four choices of learning rates contrary to SGD and to unbalanced splittings (RRGD, IG, Strang). On \exTwo, the non-interpolating global minimum is favored for the four learning rates. These first examples show the interest and the efficiency of balanced splittings. 

\begin{figure}[h!]
	\centering
	\scalebox{0.45}{\input{chapitre5_img/Speth_ex1.pgf}}
	\caption{Stationary distributions of the Speth splitting for different learning rates on \exOne: a) $\eta=\frac{1}{\max(L_1,L2)}$, b) $\eta=\frac{1}{3\max(L_1,L2)}$, c) $\eta=\frac{1}{L_1+L_2}$, d) $\eta=\frac{1}{3(L_1+L_2)}$. The green vertical line represents the position of the global minimum of $\R$.}
	\label{speth_ex1}
\end{figure}

\begin{figure}[h!]
	\centering
	\scalebox{0.45}{\input{chapitre5_img/Speth_ex2.pgf}}
	\caption{Stationary distributions of the Speth splitting for different learning rates on \exTwo: a) $\eta=\frac{1}{\max(L_1,L2)}$, b) $\eta=\frac{1}{3\max(L_1,L2)}$, c) $\eta=\frac{1}{L_1+L_2}$, d) $\eta=\frac{1}{3(L_1+L_2)}$. The green vertical line represents the position of the global minimum and the blue one the position of the local minimum of $\R$.}
	\label{speth_ex2}
\end{figure}

\begin{remark}
	To finish this subsection, let us cite some connected works:
	\begin{itemize}
		\item In \cite{splitting_sgd}, the authors interpret SGD (in fact RRGD) as a Lie-Trotter splitting. As an application, the same splitting is conserved by replacing the Euler explicit scheme by a Dormand\&Prince scheme (default integrator of the Scipy library). This process tested on linear and logistic regression problems improves the performances of SGD. However, there is no mention of the essential notion of equilibrium and of stationary distribution. 
		\item For the composite problems where $\R=f+g$ with $f$ a differentiable function and $g$ a non-smooth convex function, there exists a litterature about splitting methods \cite{splitting_proximal}. At first glance, no link was developed (until recently) between these methods and splitting schemes for ODE/PDE. The recent article \cite{splitting_proximal_ode} reinterprets the forward-backward, Douglas-Rachford and ADMM methods as splitting schemes on specific ODE.   
	\end{itemize}
\end{remark}

\subsection{New splitting schemes}
\label{new_splitting}

Despite its benefits, the Speth splitting suffers from two drawbacks in our context:
\begin{itemize}
	\item when $y_{n+1}$ is computed \eqref{splitting_speth}, the evaluation of $F_1$ used in the balancing $c_n$ does not exploit the most recent value, namely $F_1(y_n)$, but rather $F_1(y_{n-1/2})$. But in the ML context, it is possible to employ the most recent evaluation because at every iteration, it is possible to evaluate the batch we want since all the batches have more or less the same computational cost. 
	\item We want to use different time steps for every iteration (adaptive method in the end): $\eta^1$ and $\eta^2$. It is possible to keep the balance property in this context with some modifications, for example by replacing the division by $\eta$ by $\frac{2}{\eta^1+\eta^2}$. Nevertheless, exploiting the most recent evaluation (mentioned above), this issue is settled immediately. 
\end{itemize}

\paragraph{Adjustment of the Speth splitting}
~~\\
Then, we suggest the following scheme where the "balance" vector is slightly modified from one to another step:
\begin{equation}
	\left\{
	\begin{array}{ll}
		y_{n+1/2} = y_n + \eta^1 \left[F_1(y_n)+F_2(y_{n-1/2})\right],\\
		y_{n+1} = y_{n+1/2} + \eta^2 \left[F_1(y_n)+F_2(y_{n+1/2})\right].
	\end{array}
	\right.
	\label{RAG_2batch}
\end{equation}
Asymptotically, $F_1(y_{\infty})+F_2(y_{\infty})=0$ hence $F(y_{\infty})=0$.
Up to a factor $\frac{1}{2}$ on the learning rate , 
\eqref{RAG_2batch} can be written in the same form as the Speth scheme with:
\begin{equation*}
	\left\{
	\begin{array}{ll}
		c_n = F_2(y_{n-1/2})-F_1(y_n),\\
		c_{n+1/2} = F_2(y_{n+1/2})-F_1(y_n).
	\end{array}
	\right.
\end{equation*}
Both on \exOne and \exTwo, the stationary distributions of \eqref{RAG_2batch} are exactly the same as the ones for the Speth splitting, see figures \ref{IAG_ex1} and \ref{IAG_ex2}. This fact confirms empirically the balance property of this scheme.

\begin{figure}[h!]
	\centering
	\scalebox{0.45}{\input{chapitre5_img/IAG_ex1.pgf}}
	\caption{Stationary distributions of \eqref{RAG_2batch} for different learning rates on \exOne: a) $\eta=\frac{1}{\max(L_1,L2)}$, b) $\eta=\frac{1}{3\max(L_1,L2)}$, c) $\eta=\frac{1}{L_1+L_2}$, d) $\eta=\frac{1}{3(L_1+L_2)}$. The green vertical line represents the position of the global minimum of $\R$.}
	\label{IAG_ex1}
\end{figure}

\begin{figure}[h!]
	\centering
	\scalebox{0.45}{\input{chapitre5_img/IAG_ex2.pgf}}
	\caption{Stationary distributions of \eqref{RAG_2batch} for different learning rates on \exTwo: a) $\eta=\frac{1}{\max(L_1,L2)}$, b) $\eta=\frac{1}{3\max(L_1,L2)}$, c) $\eta=\frac{1}{L_1+L_2}$, d) $\eta=\frac{1}{3(L_1+L_2)}$. The green vertical line represents the position of the global minimum and the blue one the position of the local minimum of $\R$.}
	\label{IAG_ex2}
\end{figure}

For $m$ operators \eqref{RAG_2batch} extends to the form:
\begin{equation}
	y_{n+i/m} = y_{n+(i-1)/m}+\eta^i g_n^i,
	\label{RAG_scheme}
\end{equation}
where:
\begin{equation}
	g_n^i = \sum_{j=1}^i F_j\left(y_{n+(j-1)/m}\right)+\sum_{j=i+1}^m F_j\left(y_{n-1+(j-1)/m}\right),
	\label{gni}
\end{equation}
for all $i \in \{1,\dots,m\}$ and $n\geq 1$. It is possible to initialize the scheme ($n=0$) in two ways:
\begin{itemize}
	\item for all $i\in \{1,\dots,m\}$:
	\begin{equation*}
		g_0^i = \sum_{j=1}^i F_j\left(y_{n+(j-1)/m}\right),
	\end{equation*}
	and the iterates update is the same as in the case $n\geq 1$.
	\item We compute the full-batch vector field $F$ sequentially (sum) only at the first epoch:
	\begin{equation*}
		\left\{
		\begin{array}{ll}
			y_{n+i/m}=y_0 \text{ if } 1\leq i<m, \\
			y_{n+i/m} = y_{n+(i-1)/m}+\eta^i F(y_0) \text{ if } i=m.
		\end{array}
		\right.
	\end{equation*}
\end{itemize}
In the case $F_i=-\nR_i$, \eqref{RAG_scheme} is equivalent to IAG (SAG without random permutation). \\
The issue is that the implementation of formula \eqref{RAG_scheme} requires the storage of $m$ gradients hence its memory cost involves the product $mN$. Now, the goal is to obtain a \textbf{close approximation of $g_n^i$ by reducing the memory cost}.

\paragraph{Reduction of the memory cost}
~~\\
Then, we suggest the following balanced splitting for $n\geq 1$:
\begin{multline}
	y_{n+i/m} = y_{n+(i-1)/m} \\
	+\eta^i
	\left\{
	\begin{array}{ll}
		g_n+\displaystyle{\sum_{j=1}^i} \left[F_j\left(y_{n+(j-1)/m}\right)-F_j\left(y_{n+(j-2)/m}\right)\right] \text{ if } 1\leq i<m,\\
		g_{n+1} \text{ if } i=m,
	\end{array}
	\right.
	\label{RAGL_splitting}
\end{multline}
where:
\begin{equation*}
	g_n = \sum_{j=1}^m F_j\left(y_{n-1+(j-1)/m}\right).
\end{equation*}
The scheme is initialized at $n=0$ with:
\begin{equation*}
	y_{n+i/m} = y_0 + \eta \mathds{1}_{i=m} g_0 \text{ with } g_0 = F(y_0),
\end{equation*}
for all $i \in \{1,\dots,m\}$. The full-batch gradient is \textbf{only computed at the first epoch} sequentially (sum). The scheme \eqref{RAGL_splitting} is well-balanced	since $g_n$ asymptotically approximates $F$ while the perturbation (the sum until $i$) tends to 0 in the long run. \\
The formula \eqref{RAGL_splitting} with $F_i=-\nR_i$ and $\eta^i=\eta$ corresponds to the RR-SARAH \cite{RR_SARAH} update for $i\leq m-1$. We are going to see that this formula can be implemented with a storage involving $3N$.

\paragraph{Trade-off between balancing and memory cost}
~~\\
The perturbation added to $g_n$, the estimator of $F$, may grow if the number of batches become large or if one batch is stiffer than the others (every term in the sum is an increase of $F_j$): in fact, the contributions of the previous epoch (hidden in $g_n$) remain since we only substract the vector fields of the epoch $n+1$ \eqref{RAGL_splitting}. \\ 
Here, we suggest an intuitive way to reduce this perturbation and empirically confirm its interest in the following section. For simplicity, let us assume that for $i\in \{1,\dots,m\}$, $F_i$ is $L_i$-lipschitz continuous and let us denote by $i_{max} = \displaystyle{\argmax_{1 \leq i \leq m}} (L_i)$ the index of the stiffest batch. We then define the following splitting:
\begin{multline}
	y_{n+i/m} = y_{n+(i-1)/m} \\
	+\eta^i
	\left\{
	\begin{array}{ll}
		g_n+\displaystyle{\sum_{j=1}^i} \left[F_j\left(y_{n+(j-1)/m}\right)-F_j\left(y_{n-1+(i_{max}-1)/m}\right)\right] \text{ if } 1\leq i<m,\\
		g_{n+1} \text{ if } i=m,
	\end{array}
	\right.
	\label{RAGL2_splitting}
\end{multline}
where $g_n$ and the initialization are defined in the same way as \eqref{RAGL_splitting}. We are going to see that it is possible to implement \eqref{RAGL2_splitting} with a memory cost involving $5N$.
Some comments are necessary:
\begin{itemize}
	\item first the substraction of $F_j$ evaluated at the point of epoch $n$, corresponding to the stiffest batch $F_j\left(y_{n-1+(i_{max}-1)/m}\right)$ enables to supress exactly the part of $g_n$ related to this batch. It is the most problematic part because it has the largest variations.
	\item When the time step is adaptative, we imagine that the batch $i_{max}$ (its number may change adaptively) will constrain (the most) the learning rate. Therefore, the iterates at epoch $n$ are closer to $y_{n-1+(i_{max}-1)/m}$ than the other iterates of the same epoch. As a result, the fact to substract to $g_n$ the $F_j\left(y_{n-1+(i_{max}-1)/m}\right)$ is likely to reduce (at best) the residues of epoch $n$.
\end{itemize} 

\begin{remark}
	Our splitting schemes can be applied to any differential equation $y'(t)=F(y(t))$. For example in the case of the Momentum/Heavy-Ball equation \cite{momentum_sutskever_RNN,Polyak,polyak_momentum_stability,continuous_general} a natural decomposition of the vector field is given by:
	\begin{equation*}
		F_i(\theta,v) = 
		\begin{pmatrix}
			\frac{v}{m} \\
			-\frac{\bbetone}{m} v-\nR_i(\theta)
		\end{pmatrix}.
	\end{equation*}
	The application of one of the splittings above gives a fundamentally different update of STORM (variance reduction on Momentum). Therefore, the balanced splitting point of view might lead to some optimizers like SAG or RR-SARAH but it is really different of the variance reduction technique. The same difference appears for the variance reduction versions related to Adam: see Adam$^+$ \cite{adam_variance1} for example.
\end{remark}

In this section, we have highlighted the necessity of balanced splitting in order to guarantee the restoration of a "correct" stationary distribution (a support related to critical points of $F$). Finally, we have suggested three balanced splitting formulas which differ by their memory complexity and their approximation of $F$.

\section{Adaptive optimizers}
\label{section_optimizers}

In this section, we suggest a way to make the formulas \eqref{RAG_scheme}, \eqref{RAGL_splitting} and \eqref{RAGL2_splitting} adaptative in the case $F_i=-\nR_i$, drawing inspiration from some deterministic Armijo backtrackings \cite{armijo,Rondepierre,Lyap_Theory_Bilel,Bilel_thesis}. To ease the reading, the complete presentation of some algorithms is sent back to appendix \ref{annexe_algo}, especially the backtrackings, the initializations, the RAGL prototype and RAGL.

\subsection{The backtrackings}
Firstly, let us begin by describing the backtrackings necessary to evaluate the bacth stiffness. Let us denote in the following $\epsilon_m$ the precision of the float representation ($\approx 10^{-16}$ for doubles in IEEE754). For simplification, we also denote by $g$ the update vector for one iteration: for example for the splitting \eqref{RAG_scheme}, it is $g_n^i$ (more generally the term in front of $\eta^i$). The gradient of batch $i$, namely $\nR_i$, is stored in the variable $g_i$. 

\paragraph{Backtracking without interaction}
~~\\
The first backtracking described by algorithm \ref{linesearch_batch1} is a dichotomous search similar to the one described in the chapter 2 of \cite{Bilel_thesis} (the memory effect introduced in \cite{Rondepierre,Lyap_Theory_Bilel} discussed later is present but will appear in the whole algorithm). The difference is that the Armijo condition is verified for the function $\R_i$ in the direction $-\nR_i$, in other words the linesearch returns a learning rate $\eta$, satisfying for the point $\theta \in \Rb^N$ (entry of the function):
\begin{equation}
	\R_i(\theta-\eta\nR_i(\theta))-\R_i(\theta) \leq -\lambda \eta \|\nR_i(\theta)\|^2.
	\label{armijo_batch1}
\end{equation}
When $\|\nR_i\|<\epsilon$, we can consider that the batch $i$ contribution is negligible and that it is not reasonable to waste computational time to proceed to a linesearch on this batch. 

\paragraph{Backtracking with interaction}
~~\\
The second linesearch described by algorithm \ref{linesearch_batch2} takes into account the interaction between the batches. The latter is only called on the stiffest batch, if the dot product satisfies $g_i \cdot g>0$ and if $g_i \cdot g \leq \|g\|^2$. Contrary to the first linesearch which evaluates the stiffness of $\R_i$ in the direction $-\nR_i$, this one evaluates it in the update direction, namely $g$: in fact, it may be that the variations of $\R_i$ are high in the direction of $-\nR_i$ whereas they are not in the direction of $g$. The condition of positivity on the dot product is required to ensure that $\R_i$ decreases along $g$. To understand the reasons behind the two other conditions, one has to have in mind that the ideal condition is the Armijo one on the objective function:
\begin{equation*}
	\R(\theta-\eta \nR(\theta)) - \R(\theta) \leq -\lambda \eta \|\nR(\theta)\|^2.
\end{equation*}
We then replace $\nR(\theta)$ by its estimator $g$ (in the long run, the approximation improves), which results in:
\begin{equation}
	\R(\theta-\eta g) - \R(\theta) \leq -\lambda \eta \|g\|^2.
\end{equation}   
The inequality between the dot prodct and $\|g\|^2$ makes it possible to guarantee that $\R$ does not dissipate more than in the full-batch setting, since under this condition, the previous inequality becomes:
\begin{equation*}
	\R(\theta-\eta g) - \R(\theta) \leq -\lambda \eta \|g\|^2 \leq -\lambda \eta g_i\cdot g.
\end{equation*}
Then, we remplace $\R$ by $\R_{i_{max}}$ where $i_{max}$ is the index of the stiffest batch (largest lipschitz constants of the gradients). Indeed, if we proceed to this linesearch on a little steep batch, we would underestimate the stiffness of $\R$: with the "worst" batch this can not happen. The inequality finally becomes:
\begin{equation*}
	\R_{i_{max}}(\theta-\eta g) - \R_{i_{max}}(\theta) \leq -\lambda \eta g_{i_{max}}\cdot g.
\end{equation*}
Finally, the largest time step $\eta$ (between the two linesearches) is used to estimate the stiffness of the batch $i$: $L_i=\frac{2(1-\lambda)}{\eta}$. According to the descent lemma (proposition 1 of \cite{Lyap_Theory_Bilel}), this gives an estimation for the local Lipschitz constant of $\nR_i$ in the considered direction.

\subsection{The RAG optimizer: {\it Rebalanced Aggregated Gradient}}

\paragraph{Presentation of the algorithm}
~~\\
We suggest the algorithm \ref{algo_RAG} called RAG which is an adaptive version of the splitting \eqref{RAG_scheme}. Let us explain below all the steps and sub-procedures:
\begin{enumerate}
	\item let us begin by the initialization procedure Init\_RAG described by algorithm \ref{algo_init}. The latter computes the full-batch gradient in the variable $g$ by accumulation, while evaluating lipschitz constants $L_1, \dots, L_m$ of 
	$\nR_1,$ $\dots, \nR_m$ thanks to the linesearch without interaction. A possible lipschitz constant for $\nR$ is then obtained by $L=L_1+\dots L_m$. The time step is then computed as $2(1-\lambda)L^{-1}$ according to proposition 1 of \cite{Lyap_Theory_Bilel} and a GD step is done. Notice that for the rest of the epochs we define the time step as $2L^{-1}$ (neglect the factor $1-\lambda$) because this choice enables empirically to save time.
	\item The line 10 of RAG corresponds to the computation of $g_n^i$ \eqref{RAG_scheme}. For a neural network, given that a forward pass is necessary before a backpropagation, we have access to $\R_i$. At line 9, an estimation of $\R$ is computed in a similar way as the one for $\nR$:
	\begin{equation}
		\R_n^i = \sum_{j=1}^i \R_j\left(\theta_{n+(j-1)/m}\right)+\sum_{j=i+1}^m \R_j\left(\theta_{n-1+(j-1)/m}\right)
		\label{Rni}
	\end{equation}
	Then the variable $R_0$ at line 6 corresponds to this quantity at the beginning of the epoch. The comparison between $R$ and $R_0$ at the end of the epoch enables to determine if the objective function diminished since these quantities are supposed to approximate it in the long run. 
	\item The lines from 12 to 23 are here to evaluate the lipschitz constants $L_i$ in the direction $g_i$ or $g$, as explained previously. A possible lipschitz constant for $\nR$ is then obtained at line 25 by summing the individual constants. $L_{max}$ corresponds to the largest lipschitz constant among the $m$ last gradients: have in mind that some of them have been evaluated at the running epoch $n+1$ and others at the previous epoch $n$.
	\item This maximum is exploited for the memory effect (see \cite{Lyap_Theory_Bilel,Bilel_ICML} and chapter 2 of \cite{Bilel_thesis}) at line 29. Here, the time step from which we start the next linesearch is taken as the learning rate corresponding to the stiffest batch multiplied by $f_2$. The idea follows the same as the one of the theorem 5.1 of \cite{Bilel_ICML} and theorem 2.5 of \cite{Bilel_thesis}. The idea is to create a recurrent relation between the admissible time steps of two successive epochs for the stiffest batch, in order to get a complexity (number of evaluations of $\R_i$) less dependent on the stiffness. Given that the other batches are less stiff, they require less evaluations. This is satisfied empirically in the section \ref{section_results}.
	\item At line 31, the effective learning rate $\eta$ is computed from the estimation of the lipschitz constant of $\nR$ in the same way as in the initialization (factor $(1-\lambda)$ disregarded). The difference is the division by $2m-1$. Indeed, it is common in splitting operator theory to divide the time step by the number of operators involving in the splitting \cite{splitting_ode_review}. There are $m$ gradient operators $\left(\nR_j\right)_{1 \leq j \leq m}$ and also the $m-1$ delay operators for all $i \in \{1,\dots,m\}$ \eqref{RAG_scheme}:
	\begin{equation*}
		\left(\theta_{n+(j-1)/m}-\theta_{n+(i-1)/m}\right)_{1\leq j <i} \text{ and } \left(\theta_{n-1+(j-1)/m}-\theta_{n+(i-1)/m}\right)_{i+1\leq j \leq m}.
	\end{equation*}
	That is why we have tested SGD, RRGD and the different splittings with learning rates divided by $3=2\times 2-1$ in the previous sections. At line 33, the time step is again divided by a factor $h$ related to the heuristic described later in the paper. The goal is to activate the heuristic when its application conditions are verified at least twice in a row: the only aim of this line is to limit the potential oscillations generated by the heuristic.
	\item Now, let us deal with the Distance function described by algorithm \ref{algo_dist}. The product $\eta \|g\|$ corresponds to the exact distance between two successive iterates:
	\begin{equation*}
		\|\theta_{n+i/m}-\theta_{n+(i-1)/m}\|
	\end{equation*}
	Given that the inverse of $\eta_1$ estimates the lipschitz constant of $\nR_i$ ($\eta_1$ is chosen in place of $\eta$ since we need the contribution of batch $i$ and not their interactions), it is possible to upper bound:
	\begin{equation*}
		\|\nR_i\left(\theta_{n+i/m}\right)-\nR_i\left(\theta_{n+(i-1)/m}\right)\|
	\end{equation*}
	by the product $\frac{2(1-\lambda)}{\eta_1}\eta\|g\|$. Then this quantity gives the variations of the gradient of batch $i$ between two successive iterations. The variable $d$ is the sum of the variations associated to the $m$ last gradients. Smaller it is (little delays) and more accurate the estimator $g$ of $\nR$ is.
	\item This explains the stopping criterion. It guarantees that $g$ is representative of the full-batch gradient under the fixed precision ($d/P<\epsilon$) and that in this case $g$ is small ($\|g\|/P<\epsilon$). This criterion is also exploited at line 38 to stop the algorithm during a epoch.   
	\item It remains to detail the heuristic. Its aim is to accelerate the convergence of the algorithm in practice (it is possible to remove it to not take risks but the execution times are longer). If the estimation of $\R$ decreases during the epoch, we permit ourself to be less restrictive on the "CFL" condition $\frac{2}{op\times L}$: this goes through the reduction of the variable $op$ (default value $2m-1$ the number of operators) by a factor $h$. Even though the variable $\R$ is supposed to diminish in the long run, not taking into account "all the operators" might generate oscillations, since we do not come back if $\R$ increases (it would be necessary to supress an epoch). To restrain this unwelcome effect, the factor $h$ is adapted. Intuitively, we want to "ignore" the delay operators if $d$ (the variations related to the delays) influences a little the estimatation $g$ ($d\leq \|g\|$ even if it is rather $d\ll \|g\|$)): the iterates are "quasi mixed up" from the point of view of $\|g\|$. Then $h$ is increased or decreased linearly if we could ignore or not some operators. In the opposite case, we take into account all the operators without exception, in order to not take any risk if $\R$ increases. Despite being intuitive (heuristic), this part works well in practice. In the case where the number of batch is not important ($m<100$), this heuristic can be removed without major deceleration. 
\end{enumerate}    

\begin{algorithm}[h!]
	\caption{Distance($d$, $dTab$, $g$, $\eta$, $\eta_1$, $\lambda$, $\epsilon$)}
	\label{algo_dist}
	\begin{algorithmic}
		\STATE $d \leftarrow d-dTab[i]$
		\IF{$\|g\|>\epsilon$ and $\eta_1>\epsilon_m$}
		\STATE $dTab[i] \leftarrow \frac{2(1-\lambda)}{\eta_1}\eta \|g\|$
		\ELSE
		\STATE $dTab[i] \leftarrow 0$
		\ENDIF
		\STATE $d \leftarrow d+dTab[i]$
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}[h!]
	\caption{Heuristic($d$, $g$, $h$, $h_{max}$, $R$, $R_0$, $op$, $op_{max}$)}
	\label{algo_heuris}
	\begin{algorithmic}
		\IF{$d<\|g\|$}
		\STATE $h \leftarrow (h+h_{max})/2$	
		\ELSE
		\STATE $h \leftarrow (1+h_{max})/2$	
		\ENDIF
		
		\IF{$R<R_0$}
		\STATE $op \leftarrow op/h$	
		\ELSE
		\STATE $op \leftarrow op_{max}$	
		\ENDIF
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}[h!]
	\caption{RAG}
	\label{algo_RAG}
	\begin{algorithmic}[1]
		\REQUIRE initial values $\theta$, $n_{max}$, $\epsilon>0$, $f_2=1000$, $\eta_{init}=0.1$ 
		\STATE $g \leftarrow 0$, $R \leftarrow 0$
		\STATE $\eta_s \leftarrow \eta_{init}$, $h\leftarrow 1$; $h_{max}\in [1,2]$, $d\leftarrow 0$, $op\leftarrow 2m-1$
		\STATE $gTab$, $RTab$, $dTab$ (initialized at 0) and $LTab$ arrays of size $m$
		\STATE Init\_RAG ($\theta$, $\eta$, $g$, $R$, $gTab$, $RTab$, $LTab$, $\eta_{init}$, $\epsilon$)
		
		\WHILE{($\|g\|/P > \epsilon$ or $d/P>\epsilon$) and $n\leq n_{max}$}
		\STATE $R_0 \leftarrow R$
		
		\FOR{i=1,...,m}
		
		\STATE $V_0 \leftarrow \R_i(\theta)$, $g_i \leftarrow -\nR_i(\theta)$
		\STATE $R \leftarrow R-RTab[i]$, $R \leftarrow R+V_0$, $RTab[i] \leftarrow V_0$
		\STATE $g\leftarrow g-gTab[i]$, $g \leftarrow g+g_i$, $gTab[i] \leftarrow g_i$
		
		\STATE $\eta_1, \eta_2 \leftarrow \eta_s$
		\STATE $\eta_1 \leftarrow$ BFI($\theta$, $\eta_1$, $i$, $V_0$, $g_i$, $\epsilon$)
		\STATE $i_{max} \leftarrow \argmax(LTab)$
		\IF{$g_i \cdot g>\epsilon_m$ and $g_i \cdot g \leq \|g\|^2$ and $i==i_{max}$}
		\STATE $\eta_2 \leftarrow$ BWI($\theta$, $\eta_2$, $i$, $V_0$, $g_i$, $g$), \STATE $\eta \leftarrow \max(\eta_1,\eta_2)$
		\ELSE
		\STATE $\eta \leftarrow \eta_1$
		\ENDIF
		
		\IF{$\|g_i\|<\epsilon$ or $\eta<\epsilon_m$}
		\STATE $LTab[i] \leftarrow 0$
		\ELSE
		\STATE $LTab[i] \leftarrow \frac{2(1-\lambda)}{\eta}$
		\ENDIF
		\STATE $L \leftarrow$ sum(LTab), $L_{max} \leftarrow \max(LTab)$
		
		\IF{$L<\epsilon_m$}
		\STATE $\eta,\eta_s \leftarrow \eta_{init}$
		\ELSE
		\STATE $\eta_s \leftarrow f_2\dfrac{2(1-\lambda)}{L_{max}}$
		\IF{$op==2m-1$}
		\STATE $\eta \leftarrow \frac{2}{op \times L}$
		\ELSE
		\STATE $\eta \leftarrow \frac{2}{h \times op \times L}$
		\ENDIF
		\ENDIF
		
		\STATE $\theta \leftarrow \theta - \eta g$
		\STATE Distance($d$, $dTab$, $g$, $\eta$, $\eta_1$, $\lambda$, $\epsilon$)
		
		\STATE Stopping if $\|g\|/P<\epsilon$ and $d/P<\epsilon$
		
		\ENDFOR
		\STATE Heuristic ($d$, $g$, $h$, $h_{max}$, $R$, $R_0$, $op$, $2m-1$)
		\ENDWHILE
		\RETURN $\theta$
	\end{algorithmic}
\end{algorithm} 

\subsection{The RAGL optimizer: {\it Rebalanced Aggregated Gradient Light}}

\paragraph{Présentation of the algorithm}
~~\\
Firstly, we present the adaptive implementation of the splitting formula \eqref{RAGL_splitting} described by algorithm \ref{algo_RAGL_proto} in order to reduce the memory cost of RAG. There are a lot of similarities with RAG and we only comment the differences:
\begin{enumerate}
	\item The initialization \ref{algo_init_RAGL} is the same as RAG with the exception that it is not necessary to store the values $\nR_i$ in an array $gTab$.
	\item The lines 8-9 and 13-17 correspond to the scheme \eqref{RAGL_splitting}.
	\item By default, the number of operators is $4m-1$ (line 2 or 35). This is in fact an upper bound since at batch $i$, $2m+2i-1$ operators are involved: the $m$ operators $\left(\nR_j\right)_{1\leq j \leq m}$ and the delays:
	\begin{multline*}
		\left(\theta_{n+(j-1)/m}-\theta_{n+(i-1)/m}\right)_{1\leq j<i} \text{ , } \left(\theta_{n-1+(j-1)/m}-\theta_{n+(i-1)/m}\right)_{1\leq j \leq m} \\ 
		\text{ and } \left(\theta_{n+(j-1)/m}-\theta_{n+(j-2)/m}\right)_{1\leq j \leq i}.
	\end{multline*}
	This is clearer by writing explicitly the difference between the GD update and the splitting update:
	\begin{multline*}
		\sum_{j=1}^m \nR_j\left(\theta_{n+(i-1)/m}\right) - \sum_{j=1}^m \nR_j\left(\theta_{n-1+(j-1)/m}\right) \\
		 + \displaystyle{\sum_{j=1}^i} \left[\nR_j\left(\theta_{n+(j-1)/m}\right)-\nR_j\left(\theta_{n+(j-2)/m}\right)\right]. 
	\end{multline*}
	\item The line 43 evaluates $d$ (as RAG). The difference is that on batch $i$, $d$ corresponds to the variations of the $i$ last gradients: here we wait for $d$ to accumulate the variations of the last $m$ gradients (no "early" stopping). In fact, in the meantime, $g$ is a perturbed approximation of $\nR$, see formula \eqref{RAGL_splitting}.
\end{enumerate}
The RAGL algorithm \ref{algo_RAGL} implements the formula \eqref{RAGL2_splitting}. It is similar with the difference that the function \eqref{algo_comparaison} called at line 44 enables to store the point $\theta_2$ associated to the stiffest batch of the running epoch: $L_{max}$ is not necessary equal to $\tilde{L}_{max}$. Therefore at the end of an epoch, $\theta_2$ assigned to $\theta_1$, is the point associated to the stiffest batch of this epoch. \\

\begin{remark}
The goal of this article is to develop free-hyperparameters mini-batch algorithm. But aside from the heuristic, the algorithms RAG, RAGL, RAGL-prototype introduce three new hyperparameters $\lambda, f_1$ and $f_2$. As explained in \cite{Lyap_Theory_Bilel,Bilel_ICML} and chapter 2 of \cite{Bilel_thesis}, they do not influence the convergence of the algorithm (contrary to $\eta$ for GD) but only the nature of the critical points found and the time complexity. It is also possible to estimate the number of evaluations of $\R$ (in the full-batch case). Theorem 2.5 of \cite{Bilel_thesis} and 5.1 of \cite{Bilel_ICML} stipulate that the mean number of evaluations is upper bounded by ($C_n$ the number of evaluations until the $n$-th epoch):
\begin{equation}
	\displaystyle{\limsup_{n\to \infty}}\dfrac{C_n}{n} \leq 1+\frac{\log(f_2)}{\log(f_1)}+p,
	\label{complexity_law}
\end{equation}
where $p$ denotes the number of dichotomous steps in the linesearches. 
We will see later that this estimation is also verified empirically in the mini-batch case.
\end{remark}

\begin{remark}
	When one sees the formula \eqref{RAGL_splitting}, is seemed appropriate to go all over the batches in decreasing order of the lipschitz constants of $\nR_i$ in order to limit the perturbation added to $g_n$, since the perturbation is a sum of variations of $\nR_i$. This can be done by permuting the batches (at the beginning of an epoch) using the array $LTab$ obtained at the last epoch. In practice, this trick does not improve the performances. We think that it is due to the fact that we have not really access to the decreasing order of the lipschitz constants of the running epoch since the permutation used gives the order for the previous epoch. This problem of "optimal" scheduling is intrinsically hard since it would be necessary to have a way of predicting how the updates will influence the future lipschitz constants.   
\end{remark}

\paragraph{A first test to skim off the best}
~~\\
As explained in subsection \ref{new_splitting}, RAGL-prototype is not adapted to deal with stiff problems. We are going to confirm this fact on the stiffest analytical example that we have, namely \polyFive: $\R$ is a two variables polynomial, of degree total 10, with different degree of degeneracy. Although $m=2$ and the benchmark satisfies the interpolation condition, it is very difficult since $\R_1$ and $\R_2$ do not admit the same order of degeneracy around some critical points (for example $(-4,3)$): $\nnR_1$ is non-zero whereas the second and third order derivatives of $\R_2$ are zero. This may generate some important slow downs since around these points, two very different scales should be taken into account: it is the main reason behind the linesearch with interaction \ref{linesearch_batch2}. \\
The table \ref{polyFive_ecreme} presents the number of divergent trajectories on \polyFive as well as the execution time for $28000$ trainings initialized uniformly on $[-5,5]^2$. RAGL-prototype diverges $25 \%$ of the time with the heuristic contrary to RAGL, with similar execution times. It is possible to completely supress the divergent trajectories by turning off the heuristic ($h_{max}=1$) but the execution time is $2.6$ times larger than RAGL. In fact on the ML benchmarks, the difference is more distinct (factor from 4 to 6). Therefore, we will only test RAG and RAGL in the following.


\begin{table}[h!]
	\centering
	\caption{Proportion of non-convergent trajectories (nonConv) and execution time on \polyFive in order to compare RAG, RAGL-prototype and RAGL with $f_1=30$, $f_2=1000$, $p=2$, $\lambda=0.5$ et $h_{max}=2$, unless otherwise specified.}
	\begin{tabular}{lll}
		\toprule
		\begin{bf} \diagbox{Algos}{} \end{bf} & \begin{bf}nonConv\end{bf} & \begin{bf}Time(s)\end{bf} \\
		\midrule
		\begin{bf}RAG\end{bf} & 6e-4 & 3600 \\ \midrule
		\begin{bf}RAGL-prototype\end{bf} & 0.251 & 4902 \\ \midrule
		\begin{bf}RAGL-prototype ($h_{max}=1$)\end{bf} & 3.6e-5 & 13661 \\ \midrule
		\begin{bf}RAGL\end{bf} & 1e-3 & 5300 \\ \bottomrule
	\end{tabular}
	\label{polyFive_ecreme}
\end{table} 

In this section, we have introduced new adaptive mini-batch optimizers based on the balanced splitting schemes of section \ref{section_splitting_schemes}.

\section{Numerical results}
\label{section_results}

The goal of this section is to evaluate RAG and RAGL in many different configurations and then to compare them to full-batch algorithms and to the most famous stochastic optimizer in the Deep Learning litterature, namely RRAdam (called also Adam in the litterature \cite{Adam}). It is exactly described in appendix \ref{annexe_adam}. Another slight modification of RRAdam without the debiased steps (that we call RRAWB: Random Reshuffle Adam without bias steps) is sometimes tested to illustrate some instability phenomenons. They are also compared to new Armijo-like full-batch optimizers introduced in chapter 2 of \cite{Bilel_thesis} (see \cite{Lyap_Theory_Bilel} for an analysis): LC (Lyapunov control) and LCD (Lyapunov Control Dichotomy) since they have many good properties (stability for example).

\subsection{Analytic benchmarks}
\label{test_analytic}

First of all, we begin by the analytic benchmarks where all the information is available about the $\R_i$ and $\R$, in order to test the behaviors of the new optimizers in several configurations:
\begin{itemize}
	\item the examples in dimension one, \exOne to \exHeight, are necessary to know if our algorithms can handle the interpolation deficiency \eqref{interpolation}. Let us remember that the stochastic optimizers used by the ML community, namely SGD and RRGD (the same for RRAdam) are not able of this, even in the simple setups of \exOne and \exTwo. 
	\item Are the examples \polyTwo and \polyThree stable as it is the case for Armijo-like algorithms in chapter 2 of \cite{Bilel_thesis}. In fact, in \cite{Bilel}, the authors show that it is crucial to develop Lyapunov stable optimizers for ML. Let us recall what a Lyapunov stable dynamical system means: if the initialization $\theta_0$ is close to a critical point, all the trajectory remains close to this critical point. In \cite{Bilel}, the authors empirically evaluate the stability of a given deterministic optimizer, for $N=2$, by a map (called sensitivity map) drawn as follows:
	\begin{itemize}
		\item the localization of a point on the plane is the value of $\theta_0$.
		\item The color of the point (see the legend of figure \ref{RAG_stabilite} which is generated with 10000 initializations) corresponds to the convergent point of a given algorithm. More precisely, for a minimum $\theta^*$ a color covers the set:
		\begin{equation*}
			E_{\theta^*} = \{ \theta_0, \|\theta_n(\theta_0)-\theta^*\| < 10^{-3} \},
		\end{equation*}
		where the value $10^{-3}$ is an arbitrary threshold to decide on which critical point is the closest of $\theta_n$.
		\item The red points refers to values of $\theta_0$ such that the algorithm does not converge.
		\item Some isolines are drawn in green to easily identify the minimums.
	\end{itemize}
	We also extend this map to the stochastic setting. In this case, there as many maps as minimums and each point is an initialization $\theta_0$. For each minimum $\theta^*$, the gradation of colors indicates the approximative probability to converge to $\theta^*$ in the knowledge that the initial point is $\theta_0$. An example of such a map is given by the figure \ref{RRAWB_polyThree} which is generated with $10000$ initial points and for each $\theta_0$, $100$ trajectories are simulated.
	\item The management of the stiffness (\polyFive), especially when the functions $\R_i$ tackle distinct order of degeneracy. Informally, for \polyFive, $\R_1$ "sees" a quadratic function (possibly degenerated) while $\R_2$ "sees" a quadric one in the neighborhood of some minimums. Moreover, the estimation of the lipschitz constant of $\nR$ by $L_1+L_2$ is rough around some critical points of \polyFive, like $(-4,3)$ and $(4,-1)$ if we only employed the linesearch free of interaction. 
	\item The behavior in the neighborhood of saddle points especially the ones that are degenerated (comparison between \polyThree and \polyFive). Do RAG and RAGL achieve to escape them? This is relevant since some saddle points of $\R$ are global minimums for one of the $\R_i$ (one may see this as a "strong" interpolation property).
\end{itemize}

\paragraph{Hyperparameters}
~~\\
In the following, the hyperparameters are the following:
\begin{itemize}
	\item for RAG and RAGL, $f_1=30$, $f_2=1000$, $\lambda=0.5$ and $p=2$. 
	\item Concerning RRAdam, the default values of Keras/Tensorflow and Pytorch are chosen: $\eta=10^{-3}$, $\betone=0.9$, $\bettwo=0.999$ and $\epsilon_a=10^{-7}$, unless otherwise specified. These are the most used values by the practitioners.
\end{itemize}

\paragraph{One dimension benchmark}
~~\\
In the same way as in the section \ref{section_splitting_schemes}, we represent the final distribution of the points $\theta$ on the eight benchmarks using $10000$ points in the Monte-Carlo simulation. Here, we do not stop the algorithm for a time $t_f$ large enough, but we use our stopping criterion with $\epsilon=10^{-4}$ and $n_{max}=200000$: this makes it possible to evaluate its relevancy. 
\\
The figures \ref{RAG_exs} and \ref{RAGL_exs} present the eight distributions , both for RAG and RAGL. The below comments concern the two algorithms given that their distributions are exactly the same.\\
In the eight cases, the distributions are made up of Dirac delta functions at the minimums, regardless of the distance between the minimums of $\R_i$ and $\R$ (zero distance as soon as there is interpolation, "small" for \exThree and "large" for \exFour), which suggests that the interpolation assumption is not necessary anymore. This also points out that our stopping criterion seems sufficient. In fact, computing a posteriori the full-batch gradient, it satisfies $\|\nR(\theta)\|/P<\epsilon$. \\
The global minimum is favored even facing interpolating minimums, except on \exHeight. Even not-mild setups like \exSix ("semi-interpolation": minimum of two functions among three) or \exThree (a local minimum is the intersection of the graphs of $\R_1$ and $\R_2$) do not promote local minimums. The singularity of \exHeight can not be explained by the interpolation condition since any of its minimums is interpolating. We think that this is due to the fact that in the neighborhood of the local minimum, the stiffnesses $L_i$ are uniform whereas the variations of the gradients around the global minimum are largely scattered. \\
As a result, it is relevant to think that a critical point is not favored depending on their degree of interpolation but rather in accordance with a complex structure between its nature (global or local) and the covariance matrix around this point. In our eyes, it is critical that a minimum is not favored because of its interpolation condition since this property has no meaning from the point of view of the objective function. \\
It is important to notice that in the litterature, the interpolation hypothesis is always considered on the whole set of critical points and there is no intermediary situation: critical points that are interpolating and others that are not, critical points that only vanish some of the $\nR_i$ but not the whole like in \exSix.     

\begin{figure}[h!]
	\centering
	\scalebox{0.45}{\input{chapitre5_img/RAG_exs.pgf}}
	\caption{Final distributions of RAG on: a) \exOne, b) \exTwo, c) \exThree, d) \exFour, e) \exFive, f) \exSix, g) \exSeven, h) \exHeight. The green vertical straight lines represent the positions of the global minimums of $\R$ (sometimes mix with the black peak) and the blue ones refer to the local minimums.}
	\label{RAG_exs}
\end{figure}

\begin{figure}[h!]
	\centering
	\scalebox{0.45}{\input{chapitre5_img/RAGL_exs.pgf}}
	\caption{Final distributions of RAGL on: a) \exOne, b) \exTwo, c) \exThree, d) \exFour, e) \exFive, f) \exSix, g) \exSeven, h) \exHeight. The green vertical straight lines represent the positions of the global minimums of $\R$ (sometimes mix with the black peak) and the blue ones refer to the local minimums.}
	\label{RAGL_exs}
\end{figure}

\paragraph{}
Now, we focus on the benchmarks \polyTwo, \polyThree and \polyFive in dimension 2 introduced in \cite{Bilel} ($m=2$) to analyse the stability, the behavior of the saddle points and the speed of convergence. The threshold is fixed to $\epsilon=10^{-4}$ and $n_{max}=200000$. Let us remember that these benchmarks satisfy the interpolation condition for all the critical points. \\
In order to be able to compare the optimizers, we have no choice but to compute the full-batch gradient at the end of every epoch for RRGD, RRAWB and RRAdam in the rest of the paper, since there are no stopping criteria for the stochastic optimizers. Of course, this choice increases the running time of these algorithms, but it is the only manner to be fairly comparable.

\paragraph{Stability}
~~\\
In \cite{Bilel}, the authors argue that some full-batch algorithms like AWB (with $\eta=0.1$) on \polyThree, even if it converges suffers from Lyapunov instabilities. This issue even remains in the stochastic case. The figure \ref{RRAWB_polyThree} is a sensitivity map for RRAWB with $\eta=0.1$ on \polyThree. \\
For instance, around $(0,-1)$ (the global minimum), RRAWB may converge to any of the four minimums (probability between 0.2 and 0.4 depending on $\theta_0$). Starting close to $(-2,1)$, the process converges "almost surely" to $(0,-1)$ (the worst local minimum). In the neighborhood of $(2,-1)$, RRAWB is likely to converge to $(2,-1)$ or $(0,1)$ (sometimes to $(0,-1)$). Therefore we find the \textbf{same instabilities as in the deterministic setting} (starting close to a global minimum, the optimizer may converge to a "bad" local minimum) with the additional difficulty that the probability distribution is shared between different far away minimums. \\
The figure \ref{RAG_stabilite} is the sensitivity map for RAG and RAGL on \polyTwo and \polyThree. \textbf{They are Lyapunov stable in all the cases}. Concerning \polyTwo, the stability regions $E_{\theta^*}$ are connected. On \polyThree, the latter is not satisfied by the stability region of $(0,1)$ (purple area in the green one). 

\begin{figure}[h!]
	\centering
	\includegraphics[width=\textwidth, height=13cm]{chapitre5_img/RRAWB_polyThree.png}
	\caption{Stochastic sensitivity map of RRAWB ($\eta=0.1$) indicating for every critical point the probability to converge to the latter, conditioned to the initial condition.}
	\label{RRAWB_polyThree}
\end{figure}

\begin{figure}[h!]
	\centering
	\scalebox{0.45}{\input{chapitre5_img/RAG_stabilite.pgf}}
	\caption{Sensitivity map of: a) RAG on \polyTwo, b) RAGL on \polyTwo, c) RAG on \polyThree and d) RAGL on \polyThree.
	For every initial point, the color of its limit critical point is assigned.}
	\label{RAG_stabilite}
\end{figure}

\paragraph{Saddle points}
~~\\
Now, we study the saddle points of \polyFive on the figures \ref{RRAdam_polyFive} and \ref{RAG_polyFive}. They look like the sensitivity maps but they only indicate the nature of the critical points and not the exact critical point to which the trajectories converge. They are produced with $28000$ initial points and concerning the stochastic case, for every $\theta_0$, $100$ trajectories are simulated.\\
Let us first notice (figures \ref{RRAWB_polyThree} and \ref{RAG_stabilite}) that RRAWB (the same thing for RRAdam) RAG and RAGL do not converge to a saddle point of \polyTwo and \polyThree and let us recall that these saddle points are non-degenerated. It was already the case for the full-batch optimizers in chapter 2 of \cite{Bilel_thesis}. Therefore, with or without stochasticity, it is relatively easy to escape non-degenerate saddle points in two dimensions. \\
In figure \ref{RRAdam_polyFive}, all the points initialized in the two brown stanting connected components converge almost surely to non-strict saddle points ($\approx$ 11\% of the trajectories). Let us recall that a saddle point is strict if there exists one strictly negative eigenvalue of the hessian at this point. In particular a non-strict saddle point is degenerated. Compared with \cite{Bilel}, the behavior of RRAdam is the same as in the deterministic setting: therefore in two dimensions, \textbf{the stochasticity is not a key element to escape saddle points}. Indeed, the deterministic process (Adam) already escapes strict saddle points and remains stuck around non-strict saddle points according to \cite{Bilel}. \\
However, in figure \ref{RAG_polyFive}, the proportion of non-strict saddle points is very low (0.4\% of the trajectories) contrary to RRAdam. As a result, it seems that our new mini-batch optimizers (RAG and RAGL) escape more efficiently from saddle points than the classical stochastic optimizers.    

\begin{figure}[h!]
	\centering
	\includegraphics[width=\textwidth]{chapitre5_img/RRAdam_polyFive.png}
	\caption{Stochastic convergence map of RRAdam on \polyFive. For every initial point, the probability to converge to a global minimum, a local one, a saddle point or to diverge, is assigned.}
	\label{RRAdam_polyFive}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=\textwidth]{chapitre5_img/RAG_polyFive.png}
	\caption{For every initial point, the color indicating the nature of the critical point attained by the algorithm, is assigned on \polyFive for: a) RAG and b) RAGL.}
	\label{RAG_polyFive}
\end{figure}

\paragraph{Training speed}
~~\\
The table \ref{poly_exec_sto} displays the necessary time to train \polyTwo, \polyThree and \polyFive  (for all the initializations) as well as the proportion of non-convergent trajectories. Given that for all stochastic optimizers, 100 trajectories are simulated per initialization, the execution times are divided automatically by 100 to make the comparison easier. \\
On \polyTwo, RAG and RAGL are $\left[1500/250\right]=6$ to $\left[13500/250\right]=54$ times faster than RRGD, Adam or RRAdam although 5 times slower than LCD. On \polyThree, RRGD despite being divergent $70\%$ of the time (similar as GD) is less efficient than RRAdam and RRAWB for any choice of learning rates tested. With the default configuration, RRAdam is $\left[18371/(0.5\times(190+297)) \right] \approx 75$ times slower than RAG or RAGL. By increasing the learning rate to 0.1, it is possible to diminish this factor by 9. However, the proportion of global minimum decreases (not written here) from 8 to 4\%. About \polyFive, the differences are more distinct: RAG and RAGL are $\left[237700/(0.5\times(3600+5300))\right] \approx 53$ times faster than the stochastic version of Adam and even $\left[39000/(0.5\times (3600+5300))\right]\approx8.7$ times faster than its deterministic version. LCD is even more efficient. \\
Finally, it is noteworthy that the factor of difference between RAG and RAGL is at most of 1.5, even though with constant learning rate, we would expect a factor 2, since at each iteration RAGL requires two evaluations of mini-batch gradient. One should also know that without the linesearch with interaction, the costs of RAG and RAGL on \polyFive would be prohibitive (the same order than RRAdam). In fact, in this case the necessary number of epochs to converge to $(-4,3)$ or $(4,-1)$ is too high (100000 compared to the 500 epochs of our actual versions). This can be explained by the fact that in the neighborhood of these points, the stiffnesses $L_1$ and $L_2$ differ by a factor $10^4$, if they are evaluated with the linesearch free of interaction. This difference is related to the fact that $\R_1$ and $\R_2$ have distinct degree of degeneracy around these points. 

\begin{table}[h!]
	\centering
	\caption{Proportion of non-convergent trainings (nonConv) and total (all the trajectories) execution time of the optimizers on \polyTwo, \polyThree and \polyFive respectively. The time of stochastic algorithms are divided by 100 since 100 trajectories are simulated for each initial point, to make the comparison easier.}
	\begin{tabular}{lll}
		\toprule
		\begin{bf} Algos \end{bf} & \begin{bf} nonConv \end{bf} & \begin{bf} Time (s) \end{bf} \\ \midrule
		\begin{bf}RRGD ($\eta=10^{-2}$) \end{bf} & 0 & 1695 \\ \midrule
		\begin{bf}Adam\end{bf} & 0 & 1500  \\ \midrule
		\begin{bf}RRAdam\end{bf} & 0 & 13500  \\ \midrule
		\begin{bf}LCD\end{bf} & 0 & 49  \\ \midrule
		\begin{bf}\textcolor{red}{RAG}\end{bf} & 0 & 230  \\ \midrule
		\begin{bf}\textcolor{red}{RAGL}\end{bf} & 0 & 264  \\ \bottomrule
	\end{tabular}
	
	\vspace{0.2cm}
	
	\begin{tabular}{lll}
		\toprule
		\begin{bf} Algos \end{bf} & \begin{bf} nonConv\end{bf} & \begin{bf} Time (s) \end{bf} \\ \midrule
		\begin{bf}RRGD ($\eta=10^{-2}$)\end{bf} & 0.7 & 70650 \\ \midrule
		\begin{bf}Adam\end{bf} & 0 & 4140 \\ 
		\midrule
		\begin{bf}RRAdam($\eta=0.1$)\end{bf} & 0 & 2450 \\ \midrule
		\begin{bf}RRAdam\end{bf} & 0 & 18371 \\ \midrule
		\begin{bf}RRAWB($\eta=0.1$)\end{bf} &  0 & 1800 \\ \midrule
		\begin{bf}LCD\end{bf} & 0 & 92 \\ \midrule
		\begin{bf}\textcolor{red}{RAG}\end{bf} & 0 & 190 \\ \midrule
		\begin{bf}\textcolor{red}{RAGL}\end{bf} &  0 & 297 \\ \bottomrule
	\end{tabular}
	
	\vspace{0.2cm}
	
	\begin{tabular}{lll}
		\toprule
		\begin{bf} Algos \end{bf} & \begin{bf}nonConv\end{bf} & \begin{bf}Time (s)\end{bf} \\ \midrule
		\begin{bf}Adam\end{bf} & 0 & 39000 \\ \midrule
		\begin{bf}RRAdam\end{bf} & 0 & 237700 \\ \midrule
		\begin{bf}LCD\end{bf} & 0 & 1200 \\ \midrule
		\begin{bf}\textcolor{red}{RAG}\end{bf} & 6e-4 & 3600 \\ \midrule
		\begin{bf}\textcolor{red}{RAGL}\end{bf} & 1e-3 & 5300 \\ \bottomrule
	\end{tabular}
	\label{poly_exec_sto}
\end{table}

\paragraph{}
In a nutshell, the analytic examples enable us to check the following aspects for RAG and RAGL:
\begin{itemize}
	\item the interpolation assumption is not necessary anymore to guarantee that all the mass of the stationary distribution is concentrated on the critical points of $\R$.
	\item The stopping criterion is sufficient to be close to critical points of $\R$ and to guarantee $\|\nR(\theta)\|\leq \epsilon$.
	\item The time step adaptivity seems to ensure the local Lyapunov stability as in \cite{Bilel,Bilel_thesis}.
	\item It seems that RAG and RAGL escape the saddle points more efficiently than classical methods disregarded of their level of degeneracy.
	\item On the benchmarks in two dimensions, the stochastic optimizers are very expensive compared to RAG/RAGL. 
\end{itemize}

\subsection{ML benchmarks}
\label{ML_benchs}

From now on, we evaluate the practical performances of RAG and RAGL on ML tasks. These algorithms are mainly executed on three problems with several batch sizes. These problems are meticulously selected to discuss the interpolation condition. The latter is at the heart of mini-batch optimization, in the ML litterature. It is not possible to know analytically if this condition is satisfied. More the neural network is overparametrized and more likely this condition is satisfied. As a result, we mainly test three configurations: $N\gg P$, $N\approx P$ et $N\ll P$. \\
These tests were also selected so that all the data fit into memory, in such a way the full-batch optimizers can be used as a standard. Although the aim to develop mini-batch optimizers is to overcome the RAM restriction, this choice does not change the relevance of the numerical results (since the values of $m$ are very scattered) and make the conclusions more thorough. Additional benchmarks will also be briefly tested.\\
The stopping criterion for stochastic methods is the same as for the analytical examples. In the following, RRAdam is often shortened by RRA in the figures. Adam denotes the full-batch version in what follows. \\
For fully-connected networks, let us describe our convention for describing an architecture on an example: 2-15(tanh)-20(gelu)-3(linear). This means the following. The dimension of the input data is two and the dimension of the output data are three. Moreover, the network begins with a layer of $15$ neurons with tanh activation, followed by a layer of $20$ neurons with gelu activation \cite{gelu} and finishes with a layer of $3$. The number of parameters is therefore $N=15\times 2 + 15 + 20\times 15 + 20 + 3\times 20 + 3 = 428$.
Among the different approximations of gelu proposed in the original paper \cite{gelu}, we choose the approximation using the sigmoid function $\sigma$:
\begin{equation*}
	gelu(x) \approx x\sigma(1.702x).
\end{equation*}
Concerning the initializers, uniform law as well as Xavier and Bengio procedures \cite{Xavier} are implemented. These procedures are very popular, although there are many variants of Xavier and Bengio initializations. Let us briefly describe the version we use. The common point is that the bias are initialized to 0 anyway. Concerning the weights, the Xavier initializer for the layer number $l$ leads to:
\begin{equation*}
	W_l \sim \dfrac{1}{\sqrt{n_{in}}}\mathcal{N}(0,1),
\end{equation*}
where $W_l$ is the weight matrix, $n_{in}$ the input dimension, $n_{out}$ the ouput dimension of this layer and $\mathcal{N}$ is the normal law.\\
With the same notations, the Bengio initializer leads to:
\begin{equation*}
	W_l \sim \mathcal{U}\left(-\sqrt{\dfrac{6}{n_{in}+n_{out}}},\sqrt{\dfrac{6}{n_{in}+n_{out}}}\right)
\end{equation*}
where $\mathcal{U}$ denotes the uniform law. 

\paragraph{Sonar description} 
~~\\
This task consists in distinguishing if a sonar signal has been reflected on a metal or rock structure with 104/104 training/testing points: the original database is \href{http://archive.ics.uci.edu/ml/datasets/connectionist+bench+(sonar,+mines+vs.+rocks)}{\textcolor{cyan}{UCI ML}} \footnote{http://archive.ics.uci.edu/ml/datasets/connectionist+bench+(sonar,+mines+vs.+rocks)}. On this problem, the original file contains 208 data but there is no usual repartition between train/test. We then used the train\_test\_split function of scikit-learn with test\_size$=0.5$ and \\ random\_state$=7$ to guarantee the balancing of the labels between the two databases. The pre-treatment merely consists in a standard normalization for the inputs and a label-encoding for the outputs. All these treatments are listed in a jupyter lab and the data files obtained are available in a git deposit.  \\
The architecture is given by 60-30(gelu)-1(sigmoid). We choose $\epsilon=10^{-7}$ and $n_{max}=200000$. The 1000 initialization points are sampled from the Xavier law and in the stochastic setting, $10$ trajectories are simulated for each $\theta_0$. This model is clearly overparametrized since $P=104$ and $N=1861$. 

\paragraph{Sonar results}
~~\\
Before discussing the performances of the optimizers, it is valuable to check if the complexity law \eqref{complexity_law} proved in \cite{Bilel_thesis,Bilel_ICML} in the full-batch setting (LC and LCD) is also verified by RAG and RAGL.
In the table \ref{Sonar_epochs}, one can find the median of $\frac{C_n}{mn}$ (the mean number of evaluations of functions $\R_i$) where $n$ denotes here the number of epochs necessary to converge. The aim of our trick $f_2 \frac{2(1-\lambda)}{L_{max}}$ was to try to satisfy the estimation \eqref{complexity_law}. With the hyperparameter values, we get:
\begin{equation*}
	1+\frac{\log(f_2)}{\log(f_1)}+p \approx 5.
\end{equation*}
Also have in mind that it is possible to proceed to two linesearches in the same iteration if the gradient of the running batch is considered stiffer than the $m$ last gradients and if the stated conditions on the dot product are verified. Then, we expect an estimation of $\frac{C_n}{mn}$ close to 5 and inferior to 10. This is satisfied by the table \ref{Sonar_epochs}. 

\begin{table}[h!]
	\centering
	\caption{Median of $\frac{C_n}{mn}$ on Sonar where $n$ denotes, for each trajectory, the number of epochs necessary to converge.}
	\begin{tabular}{ll}
		\toprule
		\begin{bf} Algos \end{bf} & \begin{bf}$\frac{C_n}{mn}$\end{bf} \\ \midrule
		\begin{bf}LCD\end{bf} & 7 \\ \midrule
		\begin{bf}\textcolor{red}{RAG}\end{bf} ($b=8$) & 7 \\ \midrule
		\begin{bf}\textcolor{red}{RAG} ($b=4$)\end{bf} & 6 \\ \midrule
		\begin{bf}\textcolor{red}{RAGL} ($b=8$)\end{bf} & 7 \\ \midrule
		\begin{bf}\textcolor{red}{RAGL} ($b=4$)\end{bf} & 5 \\ \bottomrule
	\end{tabular}
	\label{Sonar_epochs}
\end{table} 

Now, let us deal with the performances. First, all the trajectories of all the optimizers converge. To analyse the performances, we based on the box plots \ref{Sonar_train}, \ref{Sonar_test}, \ref{Sonar_time}. The plots \ref{Sonar_train} and \ref{Sonar_test} represent the distribution of well-classified points both in training and testing on the 1000 trajectories. The figure \ref{Sonar_time} presents the distribution of execution times. \\
Let us begin by the training results on figure \ref{Sonar_train}. In median, all the optimizers classify correctly the whole set of signals. On rare occasions, RAG (b=4) and RAGL (b=8) could have poor performances ($\approx$ 50\%) although all the quartiles are concentrated around 100 \%. Nevertheless for RAGL (b=4), these singularities are more frequent since the first quartile is equal to 60\% even though the mediane and the third quartile remain concentrated around 100\%. When we see that RRAdam do not find any minimum that generates less than 100\% of recognition, it is valid to assume that RAG and RAGL have found non-interpolating minimums and that in this problem this kind of minimum is of lowest quality. It is sensitive to believe that the number of such minimums increases with $m$ ($m=13$ and $m=26$ for this test).\\
Regarding the test performances, the conclusions are similar on figure \ref{Sonar_test}. This similarity between training and test should be highlighted in a ML context, where the practitioners use extensively early stopping to avoid overfitting. Here, it seems that the good training performances are equivalent to correct testing performances: since then early stopping would be a bad practice for this benchmark. \\
Let us finish by the execution times displayed on figure \ref{Sonar_time}. LCD is 2 to 5 times faster than the mini-batch versions. RAG and RAGL are 3 to 10 times more efficient than RRAdam and 13 to 34 times more effective than Adam. These results are valid in distribution and not only for a particular trajectory, what it is commonly the case in ML.

\begin{figure}[h!]
	\centering
	\scalebox{0.9}{\input{chapitre5_img/Sonar_train.pgf}}
	\caption{Distribution of the proportion of well-classified training points on Sonar.}
	\label{Sonar_train}
\end{figure}

\begin{figure}[h!]
	\centering
	\scalebox{0.9}{\input{chapitre5_img/Sonar_test.pgf}}
	\caption{Distribution of the proportion of well-classified testing points on Sonar.}
	\label{Sonar_test}
\end{figure}

\begin{figure}[h!]
	\centering
	\scalebox{0.9}{\input{chapitre5_img/Sonar_time.pgf}}
	\caption{Distribution of execution times on Sonar.}
	\label{Sonar_time}
\end{figure} 

\paragraph{Boston description}
~~\\
This task concerns the Boston Housing Dataset \footnote{\url{https://keras.io/api/datasets/boston_housing/}{data source}} after a standard normalization of the entries (mean $0$ and standard deviation $1$). The goal is to predict median value of owner-occupied homes in Boston from 13 variables like average number of rooms per dwelling, distances to five Boston employment centres,... We have 404/102 training/testing points with the architecture 13-15(tanh)-15(tanh)-1(linear). Here, $\epsilon=10^{-4}$ and $n_{max}=40000$. The $300$ initial points are sampled from the Xavier initializer and in the stochastic setting, 10 trajectories are simulated for each point $\theta_0$. For this model, $P=404$ and $N=466$.

\paragraph{Boston results}
~~\\
In the table \ref{Boston_epochs} is written the median of the number of epochs ($n$) and of $\frac{C_n}{mn}$. RRAdam with $b=4$ ($m=101$) and $b=101$ ($m=4$) do not converge. The number of epochs is given on an indicative basis in order to convince oneself that $n_{max}$ is sufficient, because $n_{max}$ is 6 to 13 times larger than the number of epochs necessary for RAG or RAGL to converge. Actually even by taking $n_{max}=200000$, the results seem similar: simulation launched during 5 days (not finished) contrary to one day for RAG/RAGL. Just like Sonar, the number of evaluations $C_n$ seem to confirm the rule \eqref{complexity_law}. \\
Although RRAdam does not converge for $n_{max}$ epochs, it is logical to wonder if the gradient after $n_{max}$ epochs is close to the threshold $\epsilon$. The histogram \ref{Boston_grads} indicates the distribution of the full-batch gradient after $n_{max}$ epochs in logarithmic scale. In average and in median, the gradient is equal to $10^{-2.25}\approx 5.6e-3$ and globally varies between $10^{-3}$ and $10^{-1.5}\approx3.16e-2$. Therefore the gradient is far away from $10^{-4}$. 

\begin{table}[h!]
	\centering
	\caption{Median of the number of epochs ($n$) necessary to converge and median of $\frac{C_n}{mn}$.}
	\begin{tabular}{lcc}
		\toprule
		\begin{bf} Algos \end{bf} & \begin{bf}Epochs \end{bf} & \begin{bf}$\frac{C_n}{mn}$\end{bf} \\ \midrule
		\begin{bf}Adam\end{bf} & 5900 & 0 \\ \midrule
		\begin{bf}RRAdam\end{bf} ($b=101$) & $>n_{max}$ & 0 \\ \midrule
		\begin{bf}RRAdam\end{bf} ($b=4$) & $>n_{max}$ & 0 \\ \midrule
		\begin{bf}LCD\end{bf} & 3114 & 7 \\ \midrule
		\begin{bf}\textcolor{red}{RAG} ($b=101$) \end{bf} & 6791 & 7.5 \\ \midrule
		\begin{bf}\textcolor{red}{RAG} ($b=4$)\end{bf} & 2634 & 7  \\ \midrule
		\begin{bf}\textcolor{red}{RAGL} ($b=101$)\end{bf} & 7279 & 7.5 \\ \midrule
		\begin{bf}\textcolor{red}{RAGL} ($b=4$) \end{bf} & 3750 & 7 \\ \bottomrule
	\end{tabular}
	\label{Boston_epochs}
\end{table}

\begin{figure}[h!]
	\centering
	\scalebox{0.9}{\input{chapitre5_img/Boston_grads.pgf}}
	\caption{Distribution of $\log\left(\|\nR(\theta_{n_{max}})\|\right)$ for RRAdam with $b=4$ on Boston.}
	\label{Boston_grads}
\end{figure}

Let us now discuss the quality of the regression. Firstly, let us tackle the training performances displayed on the figure \ref{Boston_train}. The distributions of LCD and RAG/RAGL for the two choices of $b$ are very close and the one for Adam is slightly better. In all the cases, the model reaches a training error of $2 \times 10^{-4}$ for the best weights. On the real problem, this leads to an error on the house prices between 1 and 4\%. \\
The testing distributions are even more identical, see figure \ref{Boston_test}. The best performances guarantee an error between $1.5 \times 10^{-3}$ and $2 \times 10^{-3}$ for all the optimizers. On the real problem, this gives errors between $4$ and $10\%$ on the house prices. As a result, our splitting schemes manage to guarantee similar performances as full-batch optimizers for very different batch sizes and so for different noise structures. This fact goes against some ML statements \cite{sgd_escape1,sgd_escape2} saying that small $b$ increases stochasticity and so that the likelihood to escape saddle points or poor local minimums is increased (lead to better performances as a result).\\
The figure \ref{Boston_time} provides information on execution times. In median, RAG and RAGL are 2 to 3.4 times slower than LCD (same for the quartiles) which is itself slower than Adam. There exists some minority points that are more costly (>1000s). We notice that optimization is more expensive with 4 batches ($b=101$) than with 101 batches ($b=4$). This may seem counter-intuitive because at first sight, the problem is more complicated when the number of batches rises. However, it is rather the distribution of $(L_i)_{1\leq i \leq m}$ along trajectories which seem to play a role in the convergence speed of RAG/RAGL.    

\begin{figure}[h!]
	\centering
	\scalebox{0.9}{\input{chapitre5_img/Boston_train.pgf}}
	\caption{Distribution of the training costs on Boston.}
	\label{Boston_train}
\end{figure}

\begin{figure}[h!]
	\centering
	\scalebox{0.9}{\input{chapitre5_img/Boston_test.pgf}}
	\caption{Distribution of the testing costs on Boston.}
	\label{Boston_test}
\end{figure}

\begin{figure}[h!]
	\centering
	\scalebox{0.9}{\input{chapitre5_img/Boston_time.pgf}}
	\caption{Distribution of the execution time on Boston.}
	\label{Boston_time}
\end{figure}

\paragraph{Diamonds task}
~~\\
The problem consists in predicting the price of diamonds from 9 caracteristics: number of carat, the color, the clarity, etc. The original file is available at the git deposit and it is uploadable at the \href{https://www.kaggle.com/datasets/shivam2503/diamonds}{\textcolor{cyan}{following adress}} \footnote{https://www.kaggle.com/datasets/shivam2503/diamonds}. The pre-treatments, presented in a jupyter lab are wthe following:
\begin{itemize}
	\item the train/test splitting is done with the train\_test\_split function of scikit-learn with test\_size$=0.3$ and random\_state$=7$, which results in \\ 37760/16183 training/testing points. 
	\item All the categorical inputs are transformed via label-encoding. The inputs are standardized (mean 0 and variance 1). On the price, the transformation $x \mapsto \log_2(1+x)$ is performed followed by a min-max normalization. 
\end{itemize}
The architecture is 9-30(gelu)-30(gelu)-1(linear). Since $P=37760$ and $N=1261$, the network is under-parametrized and so the critical points are likely to be non-interpolating.
The 100 initial points $\theta_0$ are sampled from the Xavier initializer and for each $\theta_0$, 10 trajectories are simulated. $\epsilon$ is set to $10^{-4}$ and $n_{max}=15000$.

\paragraph{Diamonds results}
~~\\
In the table \ref{Diamonds_epochs}, we observe that the maximal number of epochs is clearly sufficient because $n_{max}$ is at least $15$ times larger than the number of epochs necessary to guarantee the convergence of RAG/RAGL. Once more time, RRAdam never converges , which leads us to believe that the interpolation assumption is never satisfied (the distribution of full-batch gradients is not drawn but it is similar as Boston). The law \eqref{complexity_law} is again satisfied. 

\begin{table}[h!]
	\centering
	\begin{tabular}{lcc}
		\toprule
		\begin{bf} Algos \end{bf} & \begin{bf}Epochs \end{bf} & \begin{bf}$\frac{C_n}{mn}$\end{bf} \\ \midrule
		\begin{bf}Adam\end{bf} & 4820 & 0 \\ \midrule
		\begin{bf}RRAdam ($b=10$)\end{bf} & $>n_{max}$ & 0 \\ \midrule
		\begin{bf}LCD\end{bf} & 2592 & 7 \\ \midrule
		\begin{bf}\textcolor{red}{RAG} ($b=10$)\end{bf} & 397 & 6 \\ \midrule
		\begin{bf}\textcolor{red}{RAGL} ($b=10$)\end{bf} & 1108 & 6 \\ \bottomrule  
	\end{tabular}
	\caption{Median of the number of epochs ($n$) to converge and of $\frac{C_n}{mn}$ on Diamonds.}
	\label{Diamonds_epochs}
\end{table}

Now, let us comment the training costs on figure \ref{Diamonds_train}. The distributions of LCD and RAG/RAGL are concentrated around $10^{-3.5}$ which leads to an error of $4\%$ on the diamond prices (best performance of $3.4\%$). The Adam performances are slightly better and not scattered. However, there is a value of RAG that is far away from the center of the distribution without generating poor performances. The conclusions are globally the same for the test set on figure \ref{Diamonds_test}, except for the Adam distribution that is as scattered as LCD or RAGL. \\
Let us finish with the execution time on figure \ref{Diamonds_time}. Here, LCD is 2.5 times slower than Adam in median and is even more expensive in distribution. RAGL is globally 1.5 times more costly than Adam. In distribution, RAG is two times less expensive than Adam, except for some singular prohitive points (eight times more expensive). On this benchmark, the mini-batch optimizers are more efficient than their full-batch version (LCD) and RAG is more efficient than Adam.

\begin{figure}[h!]
	\centering
	\scalebox{0.9}{\input{chapitre5_img/Diamonds_train.pgf}}
	\caption{Distribution of training costs on Diamonds, in logarithmic scale.}
	\label{Diamonds_train}
\end{figure}

\begin{figure}[h!]
	\centering
	\scalebox{0.9}{\input{chapitre5_img/Diamonds_test.pgf}}
	\caption{Distribution of testing costs on Diamonds, in logarithmic scale.}
	\label{Diamonds_test}
\end{figure}

\begin{figure}[h!]
	\centering
	\scalebox{0.9}{\input{chapitre5_img/Diamonds_time.pgf}}
	\caption{Distribution of execution times on Diamonds.}
	\label{Diamonds_time}
\end{figure}

\paragraph{MNIST-like datasets}
~~\\
The table \ref{MNIST_perf} indicates the best testing performance on MNIST-like datasets obtained with RAG/RAGL (same results for both). The MNIST dataset consists in identifying digits between 0 and 9 with 60000 $28\times 28$ images in training and 10000 in testing. %KMNIST looks like MNIST but with 10 Hiragana (japanese) characters.
EMNIST balanced (available at \href{https://www.kaggle.com/datasets/crawford/emnist}{\textcolor{blue}{this adress}} \footnote{https://www.kaggle.com/datasets/crawford/emnist}) is made up of 47 classes with digits and letters. There are 112800/18800 training/testing images. Finally, Kuzushiji-49 (K49) has 49 classes consisting in Hiragana (japanase) characters: the difficulty comes from the fact that the classes are unbalanced. The data is available on \href{https://github.com/rois-codh/kmnist}{\textcolor{cyan}{this deposit}}. They are all trained with the Lenet1 architecture \cite{LeNet1} (convolutional network) with 25 initializations sampled with the Bengio procedure. The only preprocessing consists in dividing by 255 every pixel. For information, EMNIST and K49 exceed the RAM storage. \\
In table \ref{MNIST_perf}, RAG and RAGL achieve state of the art classification results on the four tasks: see \href{https://yann.lecun.com/exdb/mnist/index.html}{\textcolor{cyan}{for MNIST performances}} \footnote{https://yann.lecun.com/exdb/mnist/index.html}, \href{https://github.com/rois-codh/kmnist}{\textcolor{cyan}{for K49 performances}} \footnote{https://github.com/rois-codh/kmnist} and \href{https://paperswithcode.com/sota/image-classification-on-emnist-balanced}{\textcolor{cyan}{for EMNIST results}} \footnote{https://paperswithcode.com/sota/image-classification-on-emnist-balanced}. \\
The best performances on MNIST with Lenet1 is 98.3\%. RAG and RAGL achieve the same performances as LeNet5 but with LeNet1. The best recognition rate for EMNIST on convolutional network is 83.21\% with a network much larger than LeNet1. For K49 the comparison is more difficult because we use the classical accuracy and some authors used some "balanced" accuracy measure (there are different definitions of this measure). 

\begin{table}[h!]
	\centering
	\begin{tabular}{ccc}
		\toprule
		\begin{bf} MNIST \end{bf} & \begin{bf}EMNIST\end{bf} & \begin{bf}K49\end{bf} \\ \midrule
		 99 \% & 87 \% & 83 \% \\ \bottomrule
	\end{tabular}
	\caption{Best testing performances of RAG/RAGL on MNIST-like benchmarks.}
	\label{MNIST_perf}
\end{table}


\section{Conclusion}
\label{section_conclusion}

The goal of this paper is to develop adaptive mini-batch optimizers in the event that full-batch optimizers (like Adam or LCD) exceeds the RAM memory. We have tackled the following points:
\begin{itemize}
	\item the interpolation hypothesis was largely discussed through different configurations in dimension one. It appears that two of the most popular algorithms used in practice, namely SGD and RRGD, admit (in case of convergence) stationary distributions with undesirable supports: they may not contain minimums of the objective function and create new stationary states, when this assumption is not satisfied. This is very concerning if one wants to reduce the size of a model (possibly under-parametrized networks).
	\item To understand this undesirable behavior, we suggest a new point of view on RRGD reinterpreted as a splitting scheme applied on the gradient flow. This turns the classical approaches upside down since they largely consists in suggesting a new continuous system (a stochastic ODE) and then to discretize it with an Euler explicit scheme. This perspective makes it possible to explain the appearance of "artificial" stationary states that are related to the unbalancing of the splitting: the stationary states may depend on the time step. With the classical approach based on the brownian motion, it is impossible to explain the source of such a singular behavior (if the diffusion is constant, the stationary distribution is the Gibbs distribution).
	\item The notion of \textbf{balancing is then crucial} to get an appropriate behavior with constant learning rate and a fortiori with adaptive time step. We call appropriate behavior the following situation: either the learning rate is too large and then there is no stationary distribution, or it is small enough and then the support is only made up of critical points of $\R$. An intermediary situation is unwanted.
	\item In a first time, we build the optimizer RAG that can be seen as the adaptive version of IAG. Given that its memory cost involves the product $mN$, we suggest RAGL that only involves $5N$. This algorithm is the first, at the best of our knowledge, \textbf{to satisfy the six requirements} claimed in the introduction. From the point of view of numerical analysis, the scheme associated to this optimizer is original since the splitting formula is itself adaptative.
	\item These new optimizers tackle the non-interpolating cases, seem Lyapunov stable and escape from non-strict saddle points (in two dimensions anyway). They present good performances on the classification and regression ML tasks studied, for very different levels of parametrization ($N$ large or smaller than $P$), without any extensive tuning of hyperparameters. In many of these benchmarks, \textbf{Adam-like optimizers do not converge to an arbitrary precision contrary to well-balanced splitting algorithms}. 
\end{itemize} 
In a nutshell, we have suggested a \textbf{serious fourth path} (the first three ones are: stochastic gradients, reshuffle versions and variance reduction methods) \textbf{to develop mini-batch optimizers}. In future works, the following points should be handled:
\begin{itemize}
	\item prove the convergence of $\left(\nR(\theta_n)\right)_{n\in \mathbb{N}}$ for RAG/RAGL under the assumption that $\nR_i$ is locally lipschitz continuous. This will be particularly difficult regarded what it is commonly done in the litterature, given that the lipschitz constants are evaluated themselves with delays. Since in our simulations, the estimation of $\R$ decreases after some epochs, it is reasonable to think that it is judicious to introduce a Lyapunov function \cite{Bilel_ICML,polyak_momentum_stability,Lyapunov_Nesterov,variational_perspective} involving the terms:
	\begin{equation*}
		\R_n^i \coloneqq \sum_{j=1}^i \R_j\left(\theta_{n+(j-1)/m}\right)+\sum_{j=i+1}^m \R_j\left(\theta_{n-1+(j-1)/m}\right)
	\end{equation*}
	and:
	\begin{equation*}
		\|\theta_{n+(i-1)/m}-\theta_{n+(i-2)/m}\|,
	\end{equation*}
	for $i\in \{1,\dots,m\}$.
	\item Find a way to accelerate RAG/RAGL without the heuristic thay may generate oscillations. Knowing when taking into account all the operators (or not) is a complex question, depending both on the local geometry of the critical points and the covariance matrix (of gradients) in their neighbordhood. 
	\item Investigate the "optimal" scheduling question of the batch in the non-convex case. We have tried orders based on gradient norms and lipschitz estimations $L_i$ but without effective enhancement.   
\end{itemize}  

\section*{Data availibility and harware}
The two dimension benchmarks (runned on CPU i7-1165G7@2.80GHz) and the Sonar, Boston and Diamonds datasets (runned on CPU Intel®Xeon®Gold SKL-6130) are coded in C++: this code is available at \href{https://github.com/bbensaid30/COptimizers.git}{\textcolor{cyan}{this adress}}.	\footnote{https://github.com/bbensaid30/COptimizers.git}. The one dimension examples and the MNIST-like tasks are implemented in Python and Tensorflow: the code is available at \href {https://github.com/bbensaid30/TOptimizers.git}{\textcolor{cyan}{this adress}}\footnote{https://github.com/bbensaid30/TOptimizers.git}. The data and their preprocessing can be found on \href{https://github.com/bbensaid30/ML_data.git}{\textcolor{cyan}{this deposit}}\footnote{https://github.com/bbensaid30/ML\_data.git}

\section*{Conflict of interest}
The authors declare that they have no conflict of interest.

\newpage

\appendix

\section{Interpolating benchmarks}
\label{annexe_polys}

Here we recall the benchmarks of \cite{Bilel}. They are made up of one neuron with a polynomial activation function $g$ (architecture $1(g)$). For $\theta=(w,b)$, the functions are of the form:
\begin{equation*}
	\R(w,b)=\frac{1}{4}\left[g(b)^2+g(w+b)^2\right].
\end{equation*}
Each activation function makes the training more complex.

\paragraph{Table description}
~~\\
Each caracteristic of the benchmarks is described in tables \ref{polyTwo_example}, \ref{polyThree_example}, \ref{polyFive_example1} and \ref{polyFive_example2}. 
The term "order of dominating terms" refers to the order of the first non-zero term in the asymptotic development of $\mathcal{R}(\theta)-\mathcal{R}(\theta^*)$ in the neighborhood of a critical point $\theta^*$. 
The values of the critical points $\theta^*$, in other words $\R(\theta^*)$, appear in the same order than the minimums.
The critical points can be global minimum (\mg), local one (\ml) or saddle point (\ps).

\paragraph{{\it Benchmark} \polyTwo}
~~\\

The activation is simply defined by:
\begin{equation}
	g(z)=z^2-1.
	\label{def_polyTwo}
\end{equation}

\begin{table}[h!]
	\centering
	\caption{Description of benchmark \polyTwo}
	\begin{tabular}{lll}
		
		\toprule
		\begin{bf} \diagbox{Properties}{Behaviors} \end{bf} & \begin{bf}1\end{bf} & \begin{bf}2\end{bf} \\ \midrule
		
		\begin{bf}Nature \end{bf} & \mg & \ps \\ \midrule
		\begin{bf}$\nnR(\theta^*)$ invertible/zero ?\end{bf} & invertible & invertible\\ \midrule
		\begin{bf}Order of dominating terms \end{bf} & 2 & 2 \\ \midrule
		\begin{bf}Set of points\end{bf} & (-2,1); (2,-1) & (-1,0); (1,0)\\ 
		& (0,-1); (0,1) & (1,-1); (-1,1)\\ \midrule
		\begin{bf}Values\end{bf} & 0; 0 & 0.5; 0.5 \\ 
		& 0; 0 & 0.5; 0.5 \\ \bottomrule
	\end{tabular}
	\label{polyTwo_example}
\end{table}

\paragraph{{\it Benchmark} \polyThree}
~~\\
In this case:
\begin{equation}
	g(z)=2z^3-3z^2+5.
	\label{def_polyThree}
\end{equation}

\begin{table}[h!]
	\centering
	\caption{Description of benchmark \polyThree}
	\begin{tabular}{llll}
		\toprule
		
		\begin{bf} \diagbox{Properties}{Behaviors} \end{bf} & \begin{bf}1\end{bf} & \begin{bf}2\end{bf} & \begin{bf}3\end{bf} \\ \midrule
		
		\begin{bf}Nature \end{bf} & \ml & \mg & \ps \\ \midrule
		\begin{bf}$\nnR(\theta^*)$ invertible/zero ?\end{bf} & invertible & invertible & invertible \\ \midrule 
		\begin{bf}Order of dominating terms \end{bf} & 2 & 2 & 2 \\ \midrule 
		\begin{bf}Set of points\end{bf} & (-2,1); (2,-1) & (0,-1) & (-1,0); (1,0) \\ 
		& (0,1) & & (-1,1); (1,-1) \\ \midrule
		\begin{bf}Values \end{bf} & 4; 4 & 0 & 6.25; 10.25 \\ 
		& 8 & & 10.25; 6.25 \\ \bottomrule 
	\end{tabular}
	\label{polyThree_example}
\end{table}

\paragraph{{\it Benchmark} \polyFive}
~~\\
In this case:
\begin{equation}
	g(z)=z^5-4z^4+2z^3+8z^2-11z-12.
	\label{def_polyFive}
\end{equation}.

\begin{table}[h!]
	\centering
	\caption{Description of benchmark \polyFive (part 1)}
	\begin{tabular}{lllll}
		
		\toprule
		\begin{bf} \diagbox{Properties}{Behaviors} \end{bf} & \begin{bf}1\end{bf} & \begin{bf}2\end{bf} & \begin{bf}3\end{bf} & \begin{bf}4\end{bf}  \\
		\midrule
		
		\begin{bf}Nature\end{bf} & \mg & \mg & \ml & \mg \\ \midrule
		\begin{bf}$\nnR(\theta^*)$ invertible/zero ?\end{bf} & invertible & singular & singular & zero \\ \midrule
		\begin{bf}Order of dominating terms\end{bf} & 2 & 2 & 2 & 4 \\ \midrule
		\begin{bf}Set of points\end{bf} & (0,3) & (-4,3); (4,-1) & (2,1); (-2,3) & (0,-1) \\ \midrule
		\begin{bf}Values\end{bf} & 0 & 0; 0 & 64; 64 & 0 \\ \bottomrule
	\end{tabular}
	\label{polyFive_example1}
\end{table}

\begin{table}[h!]
	\centering
	\caption{Description of benchmark \polyFive (part 2)}
	\begin{tabular}{lll}
		
		\toprule
		\begin{bf} \diagbox{Properties}{Behaviors} \end{bf} & \begin{bf}5\end{bf} & \begin{bf}6\end{bf} \\
		\midrule
		
		\begin{bf}Nature\end{bf} & \ps & \ps \\ \midrule
		\begin{bf}$\nnR(\theta^*)$ invertible/zero ?\end{bf} & invertible & zero \\ \midrule
		\begin{bf}Order of dominating terms\end{bf} & 2 & 3 \\ \midrule
		\begin{bf}Set of points\end{bf} & (4/5,11/5); (-4/5,3) & (-2,1); (2,-1); (0,1) \\ \midrule
		\begin{bf}Values\end{bf} & 84.2; 84.2 & 64; 64; 128 \\ \bottomrule
	\end{tabular}
	\label{polyFive_example2}
\end{table}


\section{Non-interpolating benchmarks}
\label{annexe_non_interpolated}

For each example, we present the functions $\R_i$ as well as the position $\theta$ of their global minimums (\mg) and local ones (\ml). For \exOne and \exTwo, the lipschitz constants of $\nR$ on $[-1,1]$ are computed.

\paragraph{Example \exOne}
~~\\

The functions are the following ($m=2$):
\begin{equation*}
	\R_1(\theta) = \theta^2+1,
\end{equation*}
\begin{equation*}
	\R_2(\theta) = (2\theta-1)^2+1.
\end{equation*}
The lipschitz constants of the gradients on $[-1,1]$ are: $L_1=2$ and $L_2=8$.

\begin{table}[h!]
	\centering
	\caption{Description of benchmark \exOne}
	\begin{tabular}{ll}
		
		\toprule
		\begin{bf} \diagbox{Functions}{Minimums} \end{bf} & \begin{bf}\mg\end{bf} \\ \midrule
		
		\begin{bf}$\R_1$\end{bf} &  0   \\ \midrule
		\begin{bf}$\R_2$\end{bf} &  0.5 \\ \midrule
		\begin{bf}$\R$\end{bf}   &  0.4 \\ \bottomrule
	\end{tabular}
	\label{ex1_example}
\end{table}

\paragraph{Example \exTwo}
~~\\
The functions $\R_i$ are the following:
\begin{equation*}
	\R_1(\theta) = 2\theta^4-\theta^2+1,
\end{equation*}
\begin{equation*}
	\R_2(\theta) = 12\theta^4+4\theta^3-9\theta^2+4.
\end{equation*}
The lipschitz constants of the gradients on $[-1,1]$ are respectively: $L_1=22$ and $L_2=150$.

\begin{table}[h!]
	\centering
	\caption{Description of benchmark \exTwo}
	\begin{tabular}{lll}
		
		\toprule
		\begin{bf} \diagbox{Functions}{Minimums} \end{bf} & \begin{bf}\mg\end{bf} & \begin{bf}\ml\end{bf} \\
		\midrule
		
		\begin{bf}$\R_1$\end{bf} & 0.5 & -0.5  \\ \midrule
		\begin{bf}$\R_2$\end{bf} & 0.5 & -0.75 \\ \midrule
		\begin{bf}$\R$\end{bf}   & -0.71 & 0.5 \\ \bottomrule
	\end{tabular}
	\label{ex2_example}
\end{table}

\paragraph{Example \exThree}
~~\\
The benchmark is described by the functions ($m=2$):
\begin{equation*}
	\R_1(\theta) = 12\theta^{10}-4\theta^9+5\theta^8+\theta^6-3\theta^5-2\theta^4-\theta^3+\theta^2-\theta+1,
\end{equation*}
\begin{equation*}
	\R_2(\theta) = \theta^6-\theta^5-\theta^3+\theta+1.
\end{equation*}
Let us notice that at the minimum $\theta=0$: $\R_1(0)=\R_2(0)$. The function $\R$ is quasi-constant between $\theta=0$ and $\theta=0.229$ where it varies between $1$ and $1.01$. The global minimums of $\R_1$ and $\R$ are close.

\begin{table}[h!]
	\centering
	\caption{Description of benchmark \exThree}
	\begin{tabular}{lll}
		
		\toprule
		\begin{bf} \diagbox{Functions}{Minimums} \end{bf} & \begin{bf}\mg\end{bf} & \begin{bf}\ml\end{bf} \\
		\midrule
		
		\begin{bf}$\R_1$\end{bf} & 0.707 &   \\ \midrule
		\begin{bf}$\R_2$\end{bf} & -0.463 & 1.124 \\ \midrule
		\begin{bf}$\R$\end{bf}   & 0.721 &  0 \\ \bottomrule
	\end{tabular}
	\label{ex3_example}
\end{table}

\paragraph{Example \exFour}
~~\\
The functions are of the form ($m=2$):
\begin{equation*}
	\R_1(\theta) = \theta^6+\theta^5+\theta^3-2\theta^2+9,
\end{equation*}
\begin{equation*}
	\R_2(\theta) = \theta^6-\theta^5-\theta^3+\theta+1.
\end{equation*}

\begin{table}[h!]
	\centering
	\caption{Description of benchmark \exFour}
	\begin{tabular}{lll}
		
		\toprule
		\begin{bf} \diagbox{Functions}{Minimums} \end{bf} & \begin{bf}\mg\end{bf} & \begin{bf}\ml\end{bf} \\
		\midrule
		
		\begin{bf}$\R_1$\end{bf} & -1.364 & 0.624  \\ \midrule
		\begin{bf}$\R_2$\end{bf} & -0.463 & 1.124 \\ \midrule
		\begin{bf}$\R$\end{bf}   & -0.812 &  0.677 \\ \bottomrule
	\end{tabular}
	\label{ex4_example}
\end{table}

\paragraph{Example \exFive}
~~\\
The three functions are:
\begin{equation*}
	\R_1(\theta) = \theta^2+1,
\end{equation*}
\begin{equation*}
	\R_2(\theta) = (2\theta-1)^2+1,
\end{equation*}
and
\begin{equation*}
	\R_3(\theta) = (2\theta+3)^2+1.
\end{equation*}

\begin{table}[h!]
	\centering
	\caption{Description of benchmark \exFive}
	\begin{tabular}{ll}
		
		\toprule
		\begin{bf} \diagbox{Functions}{Minimums} \end{bf} & \begin{bf}\mg\end{bf} \\ \midrule
		
		\begin{bf}$\R_1$\end{bf} & 0 \\ \midrule
		\begin{bf}$\R_2$\end{bf} & 0.5  \\ \midrule
		\begin{bf}$\R_3$\end{bf} & -1.5  \\ \midrule
		\begin{bf}$\R$\end{bf}   & -0.444  \\ \bottomrule
	\end{tabular}
	\label{ex5_example}
\end{table}

\paragraph{Example \exSix}
~~\\
The three functions are described by the following expressions:
\begin{equation*}
	\R_1(\theta) = 2\theta^4-\theta^2+1,
\end{equation*}
\begin{equation*}
	\R_2(\theta) = 12\theta^4+4\theta^3-9\theta^2+4,
\end{equation*}
and
\begin{equation*}
	\R_3(\theta) = 4\theta^6+\frac{14}{5}\theta^5-\frac{7}{4}\theta^4-\theta^3+1.
\end{equation*}

\begin{table}[h!]
	\centering
	\caption{Description of benchmark \exSix}
	\begin{tabular}{lll}
		
		\toprule
		\begin{bf} \diagbox{Functions}{Minimums} \end{bf} & \begin{bf}\mg\end{bf} & \begin{bf}\ml\end{bf} \\
		\midrule
		
		\begin{bf}$\R_1$\end{bf} & 0.5 & -0.5  \\ \midrule
		\begin{bf}$\R_2$\end{bf} & 0.5 & -0.75 \\ \midrule
		\begin{bf}$\R_3$\end{bf} & -0.75; 0.5  &  \\ \midrule
		\begin{bf}$\R$\end{bf}   & -0.718 &  0.5 \\ \bottomrule
	\end{tabular}
	\label{ex6_example}
\end{table}

\paragraph{Example \exSeven}
~~\\
The three functions are described by:
\begin{equation*}
	\R_1(\theta) = \theta^6+\theta^5+\theta^3-2\theta^2+9,
\end{equation*}
\begin{equation*}
	\R_2(\theta) = \theta^6-\theta^5-\theta^3+\theta+1,
\end{equation*}
and
\begin{equation*}
	\R_3(\theta) = \theta^6+\frac{\theta^5}{2}+2\theta^3+2.
\end{equation*}

\begin{table}[h!]
	\centering
	\caption{Description of benchmark \exSeven}
	\begin{tabular}{lll}
		
		\toprule
		\begin{bf} \diagbox{Functions}{Minimums} \end{bf} & \begin{bf}\mg\end{bf} & \begin{bf}\ml\end{bf} \\
		\midrule
		
		\begin{bf}$\R_1$\end{bf} & -1.364 & 0.624  \\ \midrule
		\begin{bf}$\R_2$\end{bf} & -0.462 & 1.125 \\ \midrule
		\begin{bf}$\R_3$\end{bf} & -1.159 &  \\ \midrule
		\begin{bf}$\R$\end{bf}   & -0.912 &   \\ \bottomrule
	\end{tabular}
	\label{ex7_example}
\end{table}

\paragraph{Example \exHeight}
~~\\

Here, there are $m=4$ batches with functions:
\begin{equation*}
	\R_1(\theta) = \theta^6+\theta^5+\theta^3-2\theta^2+9,
\end{equation*}
\begin{equation*}
	\R_2(\theta) = \theta^6-\theta^5-\theta^3+\theta+1,
\end{equation*}
\begin{equation*}
	\R_3(\theta) = \theta^6+\frac{\theta^5}{2}+2\theta^3+2,
\end{equation*}
and
\begin{equation*}
	\R_4(\theta) = \theta^3+\theta^2-2\theta+1.
\end{equation*}

\begin{table}[h!]
	\centering
	\caption{Description of benchmark \exHeight}
	\begin{tabular}{lll}
		
		\toprule
		\begin{bf} \diagbox{Functions}{Minimums} \end{bf} & \begin{bf}\mg\end{bf} & \begin{bf}\ml\end{bf} \\
		\midrule
		
		\begin{bf}$\R_1$\end{bf} & -1.364 & 0.624  \\ \midrule
		\begin{bf}$\R_2$\end{bf} & -0.463 & 1.124 \\ \midrule
		\begin{bf}$\R_3$\end{bf} & -1.159 &  \\ \midrule
		\begin{bf}$\R_4$\end{bf} & 0.548 &   \\ \midrule
		\begin{bf}$\R$\end{bf} & -0.870 & 0.413 \\ \bottomrule
	\end{tabular}
	\label{ex8_example}
\end{table}

\section{Classical algorithms}
\label{annexe_adam}

In this section, we recall the stochastic optimizers used in this paper, since there exists many versions of a same algorithm. 

\begin{algorithm}[h!]
	\caption{{\it Stochastic Gradient Descent}: SGD}
	\begin{algorithmic}
		\REQUIRE initial values $\theta$ and $\eta$.
		
		\FOR{$n=0, \dots, mn_{max}-1$}
		\STATE $i \leftarrow$ uniformly sampled in $\{1,\dots,m\}$
		\STATE $\theta \leftarrow \theta-\eta \nR_i(\theta)$
		\ENDFOR
		
		\RETURN $\theta$
	\end{algorithmic}
	\label{algo_sgd}
\end{algorithm} 

\begin{algorithm}[h!]
	\caption{{\it Random Reshuffle Gradient Descent}: RRGD}
	\begin{algorithmic}
		\REQUIRE initial values $\theta$ and $\eta$.
		
		\FOR{$n=0, \dots, n_{max}-1$}
		\STATE $\Pi \leftarrow$ random permutation of $\{1,\dots,m\}$
		\FOR{$i=1, \dots, m$}
		\STATE $\theta \leftarrow \theta-\eta \nR_{\Pi(i)}(\theta)$
		\ENDFOR
		\ENDFOR
		\RETURN $\theta$
	\end{algorithmic}
	\label{algo_RRGD}
\end{algorithm} 

\begin{algorithm}[h!]
	\caption{{\it Random Reshuffle AWB}: RRAWB}
	\begin{algorithmic}
		\REQUIRE initial values $\theta$, $\eta$, $\betone$, $\bettwo$ and $\epsilon_a$.
		\STATE $v\leftarrow 0$, $s\leftarrow 0$
		\FOR{$n=0, \dots, n_{max}-1$}
		\STATE $\Pi \leftarrow$ random permutation of $\{1,\dots,m\}$
		\FOR{$i=1, \dots, m$}
		\STATE $v \leftarrow(1-\betone)v + \betone \nR_{\Pi(i)}(\theta)$
		\STATE $s \leftarrow (1-\bettwo)s + \bettwo \nR_{\Pi(i)}(\theta)^2$
		\STATE $\theta \leftarrow \theta -\eta \dfrac{v}{\epsilon_a+\sqrt{s}}$
		\ENDFOR
		\ENDFOR
		\RETURN $\theta$
	\end{algorithmic}
	\label{algo_RRAWB}
\end{algorithm} 

\begin{algorithm}[h!]
	\caption{{\it Random Reshuffle Adam}: RRAdam}
	\begin{algorithmic}
		\REQUIRE initial values $\theta$, $\eta$, $\betone$, $\bettwo$ and $\epsilon_a$.
		\STATE $v\leftarrow 0$, $s\leftarrow 0$
		\FOR{$n=0, \dots, n_{max}-1$}
		\STATE $\Pi \leftarrow$ random permutation of $\{1,\dots,m\}$
		\FOR{$i=1, \dots, m$}
		\STATE $v \leftarrow(1-\betone)v + \betone \nR_{\Pi(i)}(\theta)$
		\STATE $s \leftarrow (1-\bettwo)s + \bettwo \nR_{\Pi(i)}(\theta)^2$
		\STATE $\theta \leftarrow \theta -\eta \dfrac{\sqrt{1-\bettwo^{n+1}}}{1-\betone^{n+1}} \dfrac{v}{\epsilon_a+\sqrt{s}}$
		\ENDFOR
		\ENDFOR
		\RETURN $\theta$
	\end{algorithmic}
	\label{algo_RRAdam}
\end{algorithm} 

\begin{remark}
	We have written above the stochastic algorithms with the classical stopping criterion, namely the maximal number of epochs. All through the sections \ref{section_stationary}, \ref{section_splitting_schemes}, \ref{section_results}, they are implemented with different criteria: maximal time $t_f$ and full-batch gradient $\|\nR(\theta)\|/P \leq \epsilon$.
\end{remark}

\section{Splitting algorithms}
\label{annexe_algo}

Here, we present some of the algorithms (except RAG) involving in the section \ref{section_optimizers}, namely the backtrackings, the initializations, the record of the point associated to the stiffest batch, RAG and RAGL. 

\begin{algorithm}[h!]
	\caption{Backtracking free of interaction: BFI ($\theta$, $\eta$, $i$,$V_0$, $g_i$, $\epsilon$)}
	\label{linesearch_batch1}
	\begin{algorithmic}
		\REQUIRE initial values $\lambda=0.5$, $f_1=30$, $p=2$ 
		
		\STATE $\theta_0 \leftarrow \theta$
		\STATE condition=($\|g_i\|>\epsilon$)
		\STATE $j=0$
		\WHILE{condition}
		\STATE $\theta \leftarrow \theta-\eta g_i$
		\STATE $V \leftarrow \R_i(\theta)$
		\STATE condition = ($V-V_0>-\lambda \eta \|g_i\|^2$)
		\IF{condition}
		\STATE $\eta \leftarrow \eta/f_1$
		\ENDIF
		\STATE $\theta \leftarrow \theta_0$
		\STATE $j \leftarrow j+1$
		\ENDWHILE
		
		\IF{$j>1$ and $\eta>\epsilon_m$}
		\STATE $ga \leftarrow \log{\eta}$; $d \leftarrow \log{f_1\eta}$
		\FOR{$k<p$}
		\STATE $m \leftarrow \frac{ga+d}{2}$; $\eta \leftarrow 10^m$
		\STATE $\theta \leftarrow \theta - \eta g_i$; $V \leftarrow \R_i(\theta)$
		\IF{$V-V_0>-\lambda \eta \|g_i\|^2$}
		\STATE $m^* \leftarrow ga$; $d \leftarrow m$; last\_pass = false
		\ELSE
		\STATE $ga \leftarrow m$; last\_pass=true
		\ENDIF
		\STATE $\theta \leftarrow \theta_0$, $j\leftarrow j+1$
		\ENDFOR
		\IF{last\_pass==false}
		\STATE $\eta \leftarrow 10^{m^*}$
		\ENDIF
		\ENDIF
		
		\RETURN $\eta$
	\end{algorithmic}
\end{algorithm} 

\begin{algorithm}[h!]
	\caption{Backtracking with interaction: BWI ($\theta$, $\eta$, $i$, $V_0$, $g_i$, $g$)}
	\label{linesearch_batch2}
	\begin{algorithmic}
		\REQUIRE initial values $\lambda=0.5$, $f_1=30$, $p=2$ 
		
		\STATE $\theta_0 \leftarrow \theta$
		\STATE condition=true
		\STATE $j=0$
		\WHILE{condition}
		\STATE $\theta \leftarrow \theta-\eta g$
		\STATE $V \leftarrow \R_i(\theta)$
		\STATE condition = ($V-V_0>-\lambda \eta g_i \cdot g $)
		\IF{condition}
		\STATE $\eta \leftarrow \eta/f_1$
		\ENDIF
		\STATE $\theta \leftarrow \theta_0$
		\STATE $j \leftarrow j+1$
		\ENDWHILE
		
		\IF{$j>1$ and $\eta>\epsilon_m$}
		\STATE $ga \leftarrow \log{\eta}$; $d \leftarrow \log{f_1\eta}$
		\FOR{$k<p$}
		\STATE $m \leftarrow \frac{ga+d}{2}$; $\eta \leftarrow 10^m$
		\STATE $\theta \leftarrow \theta - \eta g$; $V \leftarrow \R_i(\theta)$
		\IF{$V-V_0>-\lambda \eta g_i \cdot g$}
		\STATE $m^* \leftarrow ga$; $d \leftarrow m$; last\_pass = false
		\ELSE
		\STATE $ga \leftarrow m$; last\_pass=true
		\ENDIF
		\STATE $\theta \leftarrow \theta_0$, $j\leftarrow j+1$
		\ENDFOR
		\IF{last\_pass==false}
		\STATE $\eta \leftarrow 10^{m^*}$
		\ENDIF
		\ENDIF
		
		\RETURN $\eta$
	\end{algorithmic}
\end{algorithm} 

\begin{algorithm}[h!]
	\caption{Init\_RAG ($\theta$, $\eta$, $g$, $R$, $gTab$, $RTab$, $LTab$, $\eta_{init}$, $\epsilon$)}
	\label{algo_init}
	\begin{algorithmic}
		\FOR{i=1,..., m}
		\STATE $\theta_0 \leftarrow \theta$
		\STATE $V_0 \leftarrow \R_i(\theta)$, $RTab[i] \leftarrow V_0$, $R \leftarrow R+V_0$
		\STATE $g_i \leftarrow \nR_i(\theta)$, $gTab[i] \leftarrow g_i$, $g\leftarrow g+g_i$
		\STATE $\eta \leftarrow$ BFI($\theta$, $\eta$, $i$, $V_0$, $g_i$, $\epsilon$)
		\IF{$\|g_i\|>\epsilon$ or $\eta<\epsilon_m$}
		\STATE $LTab[i] \leftarrow 0$
		\ELSE
		\STATE $LTab[i] \leftarrow \frac{2(1-\lambda)}{\eta}$
		\ENDIF
		\ENDFOR
		
		\IF{$\|g\|/P>\epsilon$}
		\STATE $L \leftarrow sum(LTab)$
		\IF{$L<\epsilon_m$}
		\STATE $\eta \leftarrow \eta_{init}$
		\ELSE 
		\STATE $\eta \leftarrow \frac{2(1-\lambda)}{L}$
		\ENDIF
		\STATE $\theta \leftarrow \theta -\eta g$
		\ENDIF
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}[h!]
	\caption{Init\_RAGL ($\theta$, $\eta$, $g$, $R$, $RTab$, $LTab$, $\eta_{init}$, $\epsilon$)}
	\label{algo_init_RAGL}
	\begin{algorithmic}
		\FOR{i=1,..., m}
		\STATE $\theta_0 \leftarrow \theta$
		\STATE $V_0 \leftarrow \R_i(\theta)$, $RTab[i] \leftarrow V_0$, $R \leftarrow R+V_0$
		\STATE $g_i \leftarrow \nR_i(\theta)$, $g\leftarrow g+g_i$
		\STATE $\eta \leftarrow$ BFI($\theta$, $\eta$, $i$, $V_0$, $g_i$, $\epsilon$)
		\IF{$\|g_i\|>\epsilon$ or $\eta<\epsilon_m$}
		\STATE $LTab[i] \leftarrow 0$
		\ELSE
		\STATE $LTab[i] \leftarrow \frac{2(1-\lambda)}{\eta}$
		\ENDIF
		\ENDFOR
		
		\IF{$\|g\|/P>\epsilon$}
		\STATE $L \leftarrow sum(LTab)$
		\IF{$L<\epsilon$}
		\STATE $\eta \leftarrow \eta_{init}$
		\ELSE 
		\STATE $\eta \leftarrow \frac{2(1-\lambda)}{L}$
		\ENDIF
		\STATE $\theta \leftarrow \theta -\eta g$
		\ENDIF
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}[h!]
	\caption{Comparison($i$, $\theta$, $\theta_2$ $\tilde{L}_{max}$, $LTab$)}
	\label{algo_comparaison}
	\begin{algorithmic}
		\IF{$i==1$}
		\STATE $\tilde{L}_{max}\leftarrow LTab[1]$, $\theta_2 \leftarrow \theta$
		\ELSE
		\IF{$LTab[i]>\tilde{L}_{max}$}
		\STATE $\tilde{L}_{max}\leftarrow$ $LTab[i]$, $\theta_2 \leftarrow \theta$
		\ENDIF
		\ENDIF 
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}[h!]
	\caption{RAGL-prototype}
	\label{algo_RAGL_proto}
	\begin{algorithmic}[1]
		\REQUIRE initial values $\theta$, $n_{max}$, $\epsilon>0$, $f_1=30$, $f_2=1000$, $\eta_{init}=0.1$ 
		\STATE $\theta_0 \leftarrow \theta$, $g \leftarrow 0$, $R \leftarrow 0$
		\STATE $\eta_s \leftarrow \eta_{init}$, $h\leftarrow 1$; $h_{max}\in [1,2]$, $d\leftarrow 0$, $op\leftarrow 4m-1$
		\STATE $RTab$ and $LTab$ arrays of size $m$
		\STATE Init\_RAGL ($\theta$, $\eta$, $g$, $R$, $RTab$, $LTab$)
		
		\WHILE{($\|g\|/P > \epsilon$ or $d/P>\epsilon$) and $n\leq n_{max}$}
		\STATE $gSum \leftarrow 0$, $d\leftarrow 0$, $R_0 \leftarrow R$
		
		\FOR{i=1,...,m}
		
		\IF{$i\leq m-1$}
		\STATE $gs \leftarrow -\nR_i(\theta_0)$
		\ENDIF
		\STATE $\theta_0 \leftarrow \theta$, $V_0 \leftarrow \R_i(\theta)$, $g_i \leftarrow -\nR_i(\theta)$
		\STATE $R \leftarrow R-RTab[i]$, $R \leftarrow R+V_0$, $RTab[i] \leftarrow V_0$
		\STATE $gSum \leftarrow g+g_i$
		\IF{$i\leq m-1$}
		\STATE $g \leftarrow g-gs$, $g \leftarrow g+g_i$
		\ELSE
		\STATE $g \leftarrow gSum$
		\ENDIF
		
		\STATE $\eta_1, \eta_2 \leftarrow \eta_s$, $\eta_1 \leftarrow$ BFI($\theta$, $\eta_1$, $i$, $V_0$, $g_i$, $\epsilon$), $i_{max} \leftarrow \argmax(LTab)$
		\IF{$g_i \cdot g>\epsilon_m$ and $g_i \cdot g \leq \|g\|^2$ and $i==i_{max}$}
		\STATE $\eta_2 \leftarrow$ BWI($\theta$, $\eta_2$, $i$, $V_0$, $g_i$, $g$), $\eta \leftarrow \max(\eta_1,\eta_2)$
		\ELSE
		\STATE $\eta \leftarrow \eta_1$
		\ENDIF
		
		\IF{$\|g_i\|<\epsilon$ or $\eta<\epsilon_m$}
		\STATE $LTab[i] \leftarrow 0$
		\ELSE
		\STATE $LTab[i] \leftarrow \frac{2(1-\lambda)}{\eta}$
		\ENDIF
		\STATE $L \leftarrow$ sum(LTab), $L_{max} \leftarrow \max(LTab)$
		
		\IF{$L<\epsilon_m$}
		\STATE $\eta,\eta_s \leftarrow \eta_{init}$
		\ELSE
		\STATE $\eta_s \leftarrow f_2\dfrac{2(1-\lambda)}{L_{max}}$
		\IF{$op==4m-1$}
		\STATE $\eta \leftarrow \frac{2}{op \times L}$
		\ELSE
		\STATE $\eta \leftarrow \frac{2}{h \times op \times L}$
		\ENDIF
		\ENDIF
		
		\STATE $\theta \leftarrow \theta -\eta g$
		\IF{$\|g\|>\epsilon$ and $\eta_1>\epsilon_m$}
		\STATE $d \leftarrow d+\frac{2(1-\lambda)}{\eta_1}\eta \|g\|$
		\ENDIF
		
		\ENDFOR
		\STATE Heuristic ($d$, $g$, $h$, $h_{max}$, $R$, $R_0$, $op$, $4m-1$)
		\ENDWHILE
		\RETURN $\theta$
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}[h!]
	\caption{RAGL}
	\label{algo_RAGL}
	\begin{algorithmic}[1]
		\REQUIRE initial values $\theta$, $n_{max}$, $\epsilon>0$, $f_1=30$, $f_2=1000$, $\eta_{init}=0.1$ 
		\STATE $\theta_0, \theta_1, \theta_2 \leftarrow \theta$, $g \leftarrow 0$, $R \leftarrow 0$, $\tilde{L}_{max} \leftarrow 0$
		\STATE $\eta_s \leftarrow \eta_{init}$, $h\leftarrow 1$; $h_{max}\in [1,2]$, $d\leftarrow 0$, $op\leftarrow 4m-1$
		\STATE $RTab$ and $LTab$ arrays of size $m$
		\STATE Init\_RAGL ($\theta$, $\eta$, $g$, $R$, $RTab$, $LTab$)
		
		\WHILE{($\|g\|/P > \epsilon$ or $d/P>\epsilon$) and $n\leq n_{max}$}
		\STATE $gSum \leftarrow 0$, $d\leftarrow 0$, $R_0 \leftarrow R$
		
		\FOR{i=1,...,m}
		
		\IF{$i\leq m-1$}
		\STATE $gs \leftarrow -\nR_i(\theta_1)$
		\ENDIF
		\STATE $\theta_0 \leftarrow \theta$, $V_0 \leftarrow \R_i(\theta)$, $g_i \leftarrow -\nR_i(\theta)$
		\STATE $R \leftarrow R-RTab[i]$, $R \leftarrow R+V_0$, $RTab[i] \leftarrow V_0$
		\STATE $gSum \leftarrow g+g_i$
		\IF{$i\leq m-1$}
		\STATE $g \leftarrow g-gs$, $g \leftarrow g+g_i$
		\ELSE
		\STATE $g \leftarrow gSum$
		\ENDIF
		
		\STATE $\eta_1, \eta_2 \leftarrow \eta_s$, $\eta_1 \leftarrow$ BFI($\theta$, $\eta_1$, $i$, $V_0$, $g_i$, $\epsilon$), $i_{max} \leftarrow \argmax(LTab)$
		\IF{$g_i \cdot g>\epsilon_m$ and $g_i \cdot g \leq \|g\|^2$ and $i==i_{max}$}
		\STATE $\eta_2 \leftarrow$ BWI($\theta$, $\eta_2$, $i$, $V_0$, $g_i$, $g$), $\eta \leftarrow \max(\eta_1,\eta_2)$
		\ELSE
		\STATE $\eta \leftarrow \eta_1$
		\ENDIF
		
		\IF{$\|g_i\|<\epsilon$ or $\eta<\epsilon_m$}
		\STATE $LTab[i] \leftarrow 0$
		\ELSE
		\STATE $LTab[i] \leftarrow \frac{2(1-\lambda)}{\eta}$
		\ENDIF
		\STATE $L \leftarrow$ sum(LTab), $L_{max} \leftarrow \max(LTab)$
		
		\IF{$L<\epsilon_m$}
		\STATE $\eta,\eta_s \leftarrow \eta_{init}$
		\ELSE
		\STATE $\eta_s \leftarrow f_2\dfrac{2(1-\lambda)}{L_{max}}$
		\IF{$op==4m-1$}
		\STATE $\eta \leftarrow \frac{2}{op \times L}$
		\ELSE
		\STATE $\eta \leftarrow \frac{2}{h \times op \times L}$
		\ENDIF
		\ENDIF
		
		\IF{$\|g\|>\epsilon$ and $\eta_1>\epsilon_m$}
		\STATE $d \leftarrow d+\frac{2(1-\lambda)}{\eta_1}\eta \|g\|$
		\ENDIF
		
		\STATE Comparison($i$, $\theta$, $\theta_2$, $\tilde{L}_{max}$, $LTab$) (update of $\theta_2$)
		
		\ENDFOR
		\STATE $\theta_1 \leftarrow \theta_2$
		\STATE Heuristic ($d$, $g$, $h$, $h_{max}$, $R$, $R_0$, $op$, $4m-1$)
		\ENDWHILE
		\RETURN $\theta$
	\end{algorithmic}
\end{algorithm}

\clearpage

% BibTeX users please use one of
%\bibliographystyle{spbasic}      % basic style, author-year citations
\bibliographystyle{spmpsci}      % mathematics and physical sciences
%\bibliographystyle{spphys}       % APS-like style for physics
\bibliography{biblio}   % name your BibTeX data base

\end{document}
% end of file template.tex

